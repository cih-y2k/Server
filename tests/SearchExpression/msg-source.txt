Now in its seventh year, the two-day Coachella Valley Music and Arts Festival will overrun the Empire Polo Fields in Indio, California, this weekend with an eclectic mix of music, art installations--and, oh, a certain leotard-loving pop icon named...Madonna. 

While much as been said about the multiplatinum artist's scheduled Sunday performance in the dance music-friendly Sahara tent, Coachella's forward-thinking, wide-ranging line-up is still the main draw. The 80-something bands include '80s synth giants Depeche Mode, Iceland's Sigur Ros, heavy art-rockers Tool, Orthodox Jewish reggae singer Matisyahu and new hotshots like Infadels, Gnarls Barkley and Youth Group. 

A poll on the Coachella.com message boards has mysterious French dance duo Daft Punk tagged as the performer to see this year, beating out Madge with nearly three times the votes. (Of course, some band named "other" is also racking 'em in. They must rule!) 

"It's different. We've never had an artist who's sold 100 million records play Coachella," says festival founder Paul Tollett, who made the offer to Madonna after floating the idea by a couple of close friends. "To me it makes so much sense to have her in the deejay tent." 

The Madonna booking came when Tollett was looking for a marquee name to anchor the Sahara tent's Sunday lineup, which has been modified to handle a larger crowd for the Confessions on the Dancefloor performer. Offers went out to Madonna, Daft Punk and Massive Attack, the latter two who haven't appeared on a stage in years. 

"I was hoping for one or two of those. Instead all three came in. Any one of them would have been great on their own," he says. 

With Madonna, Tool and Massive Attack. leading the lineup, Sunday's installment has sold out. Organizers say tickets are still available for Saturday's show, which is headlined by the aforementioned Depeche Mode, Daft Punk and Sigur Ros, as well as Kanye West, Franz Ferdinand, Common, My Morning Jacket and Damien Marley. 

Scottish deejay Mylo, performing on Sunday, seems to take the attitude of most attendees. "I'll be looking forward to Madonna," he says, "And I'll definitely be there to witness the very rare occasion that is a live set from my beloved Daft Punk. I just completely love festivals. It's the madness and unpredictability of people in large numbers that attracts me." 

So it's not all about Madonna. No matter what she may have to say about it. 

Tollett is equally thrilled about other aspects of this year's fest: a Myspace-esque Website that allows ticketholders to communicate (you get access only after purchasing a ticket); how Tool, one of the headliners from the festival's 1998 debut is using this year's Coachella to end a lengthy hibernation; and how booking It Band of the moment Gnarls Barkley was a good (and kinda lucky) idea. The collaboration between soulful rapper Cee-Lo and producer Danger Mouse made U.K. chart history this year when their "Crazy" topped traditional singles charts based on digital download sales only. Gnarls Barkley's debut album St. Elsewhere comes out May 9. They are on the bill for Sunday. 

"I've got some of this booked in October or November," says Tollett. "And someone like Gnarls Barkley isn't really happening yet. But come April, look at them now. And some of these smaller bands have started happening too. That's when it gets exciting, when we can bring it all together." 
Marlee Matlin Joining 'The L Word'
Friday April 28 12:16 PM ET


Marlee Matlin is joining the cast of "The L Word," Showtime's drama about a circle of gay friends and lovers who move in the trendiest of L.A. circles. 

The 40-year-old actress will play "a fiery artist who catches the attention of Jennifer Beals' character, Bette Porter," the cable network announced Friday. 

Matlin won an Oscar for her portrayal of a deaf student in 1986's "Children of a Lesser God." Her TV credits include a recurring role on NBC's "The West Wing." 

ADVERTISEMENT
 
 
Showtime said the fourth season of the series will premiere early next year. 

The ensemble cast also includes Pam Grier, Laurel Holloman, Mia Kirshner, Katherine Moennig, Leisha Hailey, Rachel Shelley and Daniela Sea. 

  
Ellen DeGeneres wins Daytime Emmys
April 29, 12:45 AM ET, Reuters
Comedian Ellen DeGeneres swept the talk show awards at the Daytime Emmys for a second consecutive year while soap opera "General Hospital" won a record ninth award for best daytime drama.
 
NBC renews 'Medium, 'Jordan, 'Vegas'
April 28, 5:43 PM ET, Reuters
NBC will return "Medium," "Crossing Jordan" and "Las Vegas" to its primetime schedule in 2006-07, the network said Friday.
 
First, they need to learn to act standing up...
April 28, 8:53 AM ET, Reuters
A new television reality show invites porn stars to test their serious acting abilities in London's theater district, raising the question: Debbie can do Dallas, but can she take on Chekhov's "The Cherry Orchard?"
 
New show lets porn stars test acting skills
April 28, 8:00 AM ET, Reuters
A new television reality show invites porn stars to test their serious acting abilities in London's theater district, raising the question: Debbie can do Dallas, but can she take on Chekhov's "The Cherry Orchard?"
 
   
ANOTHER DAY ANOTHER LAWSUIT FOR CHARLIE SHEEN
Friday April 28, 2006, Star Magazine
Just days after Denise Richards filed a restraining order against her soon to be ex husband, Charlie Sheen, another woman has come forward to add to his legal woes. Ursula Auburn, who claims to be an ex-girlfriend of the actor, has filed a bizarre lawsuit against Sheen for invasion of privacy and intentional infliction of emotional distress.
 
REPORT: ROSIE O'DONNELL IS THE VIEW'S NEW CO-HOST!
Friday April 28, 2006, Star Magazine
Rosie O'Donnell, 44, reportedly has a new day job: As The View's new co-host! Extra reports that O'Donnell will replace Meredith Vieira   who is joining NBC's Today show later this year, when she replaces Katie Couric, who's heading to CBS. ABC, The View's network, is expected to make the official announcement on Friday, reports Extra. The short list of candidates for the gig reportedly included Connie Chung, Patricia Heaton and Soledad O'Brien. Tell Us What You Think
 
ANGELINA FINALLY BREAKS HER SILENCE ON BRAD: THE ROMANCE JUST HAPPENED!
Wednesday April 26, 2006, Star Magazine
Angelina Jolie, 30, has finally spoken out about her relationship with her Mr. & Mrs. Smith co-star Brad Pitt, 42, who divorced former Friends star Jennifer Aniston last year. "I don't talk about our   my relationship in public," Jolie told NBC's Ann Curry in an interview shot in Namibia   where Jolie, Pitt and their children Maddox, 4, and Zahara, 1, are holed up in a luxury resort   airing Thursday on Today (7 a.m. ET/PT) and Dateline on Sunday ( 7 p.m. ET/PT). "But we also don't...
 
STAR'S ALL OVER BRITNEY!
Tuesday April 25, 2006, Star Magazine
Once again, Star told you breaking celebrity news first   only to have another publication follow suit! Case in point? Britney Spears' pregnancy. In Star's Feb. 13 issue   which landed on newsstands on Feb. 1   Star broke the news that Britney was pregnant with her second child. Now, a competitor is finally following suit, months later, in its issue hitting newsstands this week, saying that Britney is expecting. Well, it's about time! Plus, in Star's issue hitting newsstands...
 
 Distribution
GNU Emacs is free software; this means that everyone is free to use it and free to redistribute it on certain conditions. GNU Emacs is not in the public domain; it is copyrighted and there are restrictions on its distribution, but these restrictions are designed to permit everything that a good cooperating citizen would want to do. What is not allowed is to try to prevent others from further sharing any version of GNU Emacs that they might get from you. The precise conditions are found in the GNU General Public License that comes with Emacs and also appears in this manual1. See Copying. 

One way to get a copy of GNU Emacs is from someone else who has it. You need not ask for our permission to do so, or tell any one else; just copy it. If you have access to the Internet, you can get the latest distribution version of GNU Emacs by anonymous FTP; see http://www.gnu.org/software/emacs on our website for more information. 

You may also receive GNU Emacs when you buy a computer. Computer manufacturers are free to distribute copies on the same terms that apply to everyone else. These terms require them to give you the full sources, including whatever changes they may have made, and to permit you to redistribute the GNU Emacs received from them under the usual terms of the General Public License. In other words, the program must be free for you when you get it, not just free for the manufacturer. 

You can also order copies of GNU Emacs from the Free Software Foundation. This is a convenient and reliable way to get a copy; it is also a good way to help fund our work. We also sell hardcopy versions of this manual and An Introduction to Programming in Emacs Lisp, by Robert J. Chassell. You can find an order form on our web site at http://www.gnu.org/order/order.html. For further information, write to 

     Free Software Foundation
     51 Franklin Street, Fifth Floor
     Boston, MA 02110-1301
     USA

The income from distribution fees goes to support the foundation's purpose: the development of new free software, and improvements to our existing programs including GNU Emacs. 

If you find GNU Emacs useful, please send a donation to the Free Software Foundation to support our work. Donations to the Free Software Foundation are tax deductible in the US. If you use GNU Emacs at your workplace, please suggest that the company make a donation. If company policy is unsympathetic to the idea of donating to charity, you might instead suggest ordering a CD-ROM from the Foundation occasionally, or subscribing to periodic updates. 



--------------------------------------------------------------------------------
Next: Glossary, Previous: Distrib, Up: Top 
Introduction
You are reading about GNU Emacs, the GNU incarnation of the advanced, self-documenting, customizable, extensible real-time display editor Emacs. (The `G' in `GNU' is not silent.) 

We say that Emacs is a display editor because normally the text being edited is visible on the screen and is updated automatically as you type your commands. See Display. 

We call it a real-time editor because the display is updated very frequently, usually after each character or pair of characters you type. This minimizes the amount of information you must keep in your head as you edit. See Real-time. 

We call Emacs advanced because it provides facilities that go beyond simple insertion and deletion: controlling subprocesses; automatic indentation of programs; viewing two or more files at once; editing formatted text; and dealing in terms of characters, words, lines, sentences, paragraphs, and pages, as well as expressions and comments in several different programming languages. 

Self-documenting means that at any time you can type a special character, Control-h, to find out what your options are. You can also use it to find out what any command does, or to find all the commands that pertain to a topic. See Help. 

Customizable means that you can change the definitions of Emacs commands in little ways. For example, if you use a programming language in which comments start with  <**  and end with  **> , you can tell the Emacs comment manipulation commands to use those strings (see Comments). Another sort of customization is rearrangement of the command set. For example, if you prefer the four basic cursor motion commands (up, down, left and right) on keys in a diamond pattern on the keyboard, you can rebind the keys that way. See Customization. 

Extensible means that you can go beyond simple customization and write entirely new commands, programs in the Lisp language to be run by Emacs's own Lisp interpreter. Emacs is an  on-line extensible  system, which means that it is divided into many functions that call each other, any of which can be redefined in the middle of an editing session. Almost any part of Emacs can be replaced without making a separate copy of all of Emacs. Most of the editing commands of Emacs are written in Lisp; the few exceptions could have been written in Lisp but are written in C for efficiency. Although only a programmer can write an extension, anybody can use it afterward. See Emacs Lisp Intro, if you want to learn Emacs Lisp programming. 

When run under the X Window System, Emacs provides its own menus and convenient bindings to mouse buttons. But Emacs can provide many of the benefits of a window system on a text-only terminal. For instance, you can look at or edit several files at once, move text between files, and edit files while running shell commands. 



--------------------------------------------------------------------------------
Next: User Input, Previous: Acknowledgments, Up: Top 
1 The Organization of the Screen
On a text-only terminal, the Emacs display occupies the whole screen. On the X Window System, Emacs creates its own X windows to use. We use the term frame to mean an entire text-only screen or an entire X window used by Emacs. Emacs uses both kinds of frames in the same way to display your editing. Emacs normally starts out with just one frame, but you can create additional frames if you wish. See Frames. 

When you start Emacs, the entire frame except for the top and bottom is devoted to the text you are editing. This area is called the window. At the top there is normally a menu bar where you can access a series of menus; then there may be a tool bar, a row of icons that perform editing commands if you click on them. Below this, the window begins. The last line is a special echo area or minibuffer window, where prompts appear and where you enter information when Emacs asks for it. See below for more information about these special lines. 

You can subdivide the large text window horizontally or vertically into multiple text windows, each of which can be used for a different file (see Windows). In this manual, the word  window  always refers to the subdivisions of a frame within Emacs. 

At any time, one window is the selected window. On graphical terminals, the selected window normally shows a more prominent cursor (solid and blinking) while other windows show a weaker cursor (such as a hollow box). On text terminals, which have just one cursor, that cursor appears in the selected window. 

Most Emacs commands implicitly apply to the text in the selected window (though mouse commands generally operate on whatever window you click them in, whether selected or not). The other windows display text for reference only, unless/until you select them. If you use multiple frames under the X Window System, then giving the input focus to a particular frame selects a window in that frame. 

Each window's last line is a mode line, which describes what is going on in that window. It appears in different color and/or a  3D  box, if the terminal supports that; its contents normally begin with  --:--  *scratch*  when Emacs starts. The mode line displays status information such as what buffer is being displayed above it in the window, what major and minor modes are in use, and whether the buffer contains unsaved changes. 

Point: The place in the text where editing commands operate. 
Echo Area: Short messages appear at the bottom of the screen. 
Mode Line: Interpreting the mode line. 
Menu Bar: How to use the menu bar. 


--------------------------------------------------------------------------------
Next: Echo Area, Up: Screen 
1.1 Point
Within Emacs, the active cursor shows the location at which editing commands will take effect. This location is called point. Many Emacs commands move point through the text, so that you can edit at different places in it. You can also place point by clicking mouse button 1. 

While the cursor appears to point at a character, you should think of point as between two characters; it points before the character that appears under the cursor. For example, if your text looks like  frob  with the cursor over the  b , then point is between the  o  and the  b . If you insert the character  !  at that position, the result is  fro!b , with point between the  !  and the  b . Thus, the cursor remains over the  b , as before. 

Sometimes people speak of  the cursor  when they mean  point,  or speak of commands that move point as  cursor motion  commands. 

If you are editing several files in Emacs, each in its own buffer, each buffer has its own point location. A buffer that is not currently displayed remembers its point location in case you display it again later. When Emacs displays multiple windows, each window has its own point location. If the same buffer appears in more than one window, each window has its own position for point in that buffer, and (when possible) its own cursor. 

A text-only terminal has just one cursor, so Emacs puts it in the selected window. The other windows do not show a cursor, even though they do have a location of point. When Emacs updates the screen on a text-only terminal, it has to put the cursor temporarily at the place the output goes. This doesn't mean point is there, though. Once display updating finishes, Emacs puts the cursor where point is. 

On graphical terminals, Emacs shows a cursor in each window; the selected window's cursor is solid and blinking, and the other cursors are just hollow. Thus, the most prominent cursor always shows you the selected window, on all kinds of terminals. 

See Cursor Display, for customizable variables that control display of the cursor or cursors. 

The term  point  comes from the character  . , which was the command in TECO (the language in which the original Emacs was written) for accessing the value now called  point.  



--------------------------------------------------------------------------------
Next: Mode Line, Previous: Point, Up: Screen 
1.2 The Echo Area
The line at the bottom of the frame (below the mode line) is the echo area. It is used to display small amounts of text for various purposes. 

Echoing means displaying the characters that you type. At the command line, the operating system normally echoes all your input. Emacs handles echoing differently. 

Single-character commands do not echo in Emacs, and multi-character commands echo only if you pause while typing them. As soon as you pause for more than a second in the middle of a command, Emacs echoes all the characters of the command so far. This is to prompt you for the rest of the command. Once echoing has started, the rest of the command echoes immediately as you type it. This behavior is designed to give confident users fast response, while giving hesitant users maximum feedback. You can change this behavior by setting a variable (see Display Custom). 

If a command cannot be executed, it may display an error message in the echo area. Error messages are accompanied by beeping or by flashing the screen. The error also discards any input you have typed ahead. 

Some commands display informative messages in the echo area. These messages look much like error messages, but they are not announced with a beep and do not throw away input. Sometimes the message tells you what the command has done, when this is not obvious from looking at the text being edited. Sometimes the sole purpose of a command is to show you a message giving you specific information for example, C-x = (hold down <CTRL> and type x, then let go of <CTRL> and type =) displays a message describing the character position of point in the text and its current column in the window. Commands that take a long time often display messages ending in  ...  while they are working, and add  done  at the end when they are finished. 

Echo-area informative messages are saved in an editor buffer named  *Messages* . (We have not explained buffers yet; see Buffers, for more information about them.) If you miss a message that appears briefly on the screen, you can switch to the  *Messages*  buffer to see it again. (Successive progress messages are often collapsed into one in that buffer.) 

The size of  *Messages*  is limited to a certain number of lines. The variable message-log-max specifies how many lines. Once the buffer has that many lines, each line added at the end deletes one line from the beginning. See Variables, for how to set variables such as message-log-max. 

The echo area is also used to display the minibuffer, a window that is used for reading arguments to commands, such as the name of a file to be edited. When the minibuffer is in use, the echo area begins with a prompt string that usually ends with a colon; also, the cursor appears in that line because it is the selected window. You can always get out of the minibuffer by typing C-g. See Minibuffer. 



--------------------------------------------------------------------------------
Next: Menu Bar, Previous: Echo Area, Up: Screen 
1.3 The Mode Line
Each text window's last line is a mode line, which describes what is going on in that window. When there is only one text window, the mode line appears right above the echo area; it is the next-to-last line in the frame. The mode line starts and ends with dashes. On a text-mode display, the mode line is in inverse video if the terminal supports that; on a graphics display, the mode line has a 3D box appearance to help it stand out. The mode line of the selected window has a slightly different appearance than those of other windows; see Optional Mode Line, for more about this. 

Normally, the mode line looks like this: 

     -cs:ch  buf      pos line   (major minor)------

This gives information about the buffer being displayed in the window: the buffer's name, what major and minor modes are in use, whether the buffer's text has been changed, and how far down the buffer you are currently looking. 

ch contains two stars  **  if the text in the buffer has been edited (the buffer is  modified ), or  --  if the buffer has not been edited. For a read-only buffer, it is  %*  if the buffer is modified, and  %%  otherwise. 

buf is the name of the window's buffer. In most cases this is the same as the name of a file you are editing. See Buffers. 

The buffer displayed in the selected window (the window that the cursor is in) is also Emacs's current buffer, the one that editing takes place in. When we speak of what some command does to  the buffer,  we are talking about the current buffer. 

pos tells you whether there is additional text above the top of the window, or below the bottom. If your buffer is small and it is all visible in the window, pos is  All . Otherwise, it is  Top  if you are looking at the beginning of the buffer,  Bot  if you are looking at the end of the buffer, or  nn% , where nn is the percentage of the buffer above the top of the window. With Size Indication mode, you can display the size of the buffer as well. See Optional Mode Line. 

line is  L  followed by the current line number of point. This is present when Line Number mode is enabled (which it normally is). You can optionally display the current column number too, by turning on Column Number mode (which is not enabled by default because it is somewhat slower). See Optional Mode Line. 

major is the name of the major mode in effect in the buffer. At any time, each buffer is in one and only one of the possible major modes. The major modes available include Fundamental mode (the least specialized), Text mode, Lisp mode, C mode, Texinfo mode, and many others. See Major Modes, for details of how the modes differ and how to select one. 

Some major modes display additional information after the major mode name. For example, Rmail buffers display the current message number and the total number of messages. Compilation buffers and Shell buffers display the status of the subprocess. 

minor is a list of some of the minor modes that are turned on at the moment in the window's chosen buffer. For example,  Fill  means that Auto Fill mode is on.  Abbrev  means that Word Abbrev mode is on.  Ovwrt  means that Overwrite mode is on. See Minor Modes, for more information.  Narrow  means that the buffer being displayed has editing restricted to only a portion of its text. This is not really a minor mode, but is like one. See Narrowing.  Def  means that a keyboard macro is being defined. See Keyboard Macros. 

In addition, if Emacs is currently inside a recursive editing level, square brackets ( [...] ) appear around the parentheses that surround the modes. If Emacs is in one recursive editing level within another, double square brackets appear, and so on. Since recursive editing levels affect Emacs globally, not just one buffer, the square brackets appear in every window's mode line or not in any of them. See Recursive Edit. 

Non-windowing terminals can only show a single Emacs frame at a time (see Frames). On such terminals, the mode line displays the name of the selected frame, after ch. The initial frame's name is  F1 . 

cs states the coding system used for the file you are editing. A dash indicates the default state of affairs: no code conversion, except for end-of-line translation if the file contents call for that.  =  means no conversion whatsoever. Nontrivial code conversions are represented by various letters for example,  1  refers to ISO Latin-1. See Coding Systems, for more information. If you are using an input method, a string of the form  i>  is added to the beginning of cs; i identifies the input method. (Some input methods show  +  or  @  instead of  > .) See Input Methods. 

When you are using a character-only terminal (not a window system), cs uses three characters to describe, respectively, the coding system for keyboard input, the coding system for terminal output, and the coding system used for the file you are editing. 

When multibyte characters are not enabled, cs does not appear at all. See Enabling Multibyte. 

The colon after cs can change to another string in certain circumstances. Emacs uses newline characters to separate lines in the buffer. Some files use different conventions for separating lines: either carriage-return linefeed (the MS-DOS convention) or just carriage-return (the Macintosh convention). If the buffer's file uses carriage-return linefeed, the colon changes to either a backslash ( s) or  (DOS) , depending on the operating system. If the file uses just carriage-return, the colon indicator changes to either a forward slash ( / ) or  (Mac) . On some systems, Emacs displays  (Unix)  instead of the colon even for files that use newline to separate lines. 

You can customize the mode line display for each of the end-of-line formats by setting each of the variables eol-mnemonic-unix, eol-mnemonic-dos, eol-mnemonic-mac, and eol-mnemonic-undecided to any string you find appropriate. See Variables, for an explanation of how to set variables. 

See Optional Mode Line, for features that add other handy information to the mode line, such as the size of the buffer, the current column number of point, the current time, and whether new mail for you has arrived. 

The mode line is mouse-sensitive; when you move the mouse across various parts of it, Emacs displays help text to say what a click in that place will do. See Mode Line Mouse. 



--------------------------------------------------------------------------------
Previous: Mode Line, Up: Screen 
1.4 The Menu Bar
Each Emacs frame normally has a menu bar at the top which you can use to perform certain common operations. There's no need to list them here, as you can more easily see for yourself. 

When you are using a window system, you can use the mouse to choose a command from the menu bar. An arrow pointing right, after the menu item, indicates that the item leads to a subsidiary menu;  ...  at the end means that the command will read arguments (further input from you) before it actually does anything. 

To view the full command name and documentation for a menu item, type C-h k, and then select the menu bar with the mouse in the usual way (see Key Help). 

On text-only terminals with no mouse, you can use the menu bar by typing M-` or <F10> (these run the command tmm-menubar). This command enters a mode in which you can select a menu item from the keyboard. A provisional choice appears in the echo area. You can use the up and down arrow keys to move through the menu to different choices. When you have found the choice you want, type <RET> to select it. 

Each menu item also has an assigned letter or digit which designates that item; it is usually the initial of some word in the item's name. This letter or digit is separated from the item name by  => . You can type the item's letter or digit to select the item. 

Some of the commands in the menu bar have ordinary key bindings as well; if so, the menu lists one equivalent key binding in parentheses after the item itself. 



--------------------------------------------------------------------------------
Next: Keys, Previous: Screen, Up: Top 
2 Kinds of User Input
GNU Emacs uses an extension of the ASCII character set for keyboard input; it also accepts non-character input events including function keys and mouse button actions. 

ASCII consists of 128 character codes. Some of these codes are assigned graphic symbols such as  a  and  = ; the rest are control characters, such as Control-a (usually written C-a for short). C-a gets its name from the fact that you type it by holding down the <CTRL> key while pressing a. 

Some ASCII control characters have special names, and most terminals have special keys you can type them with: for example, <RET>, <TAB>, <DEL> and <ESC>. The space character is usually referred to below as <SPC>, even though strictly speaking it is a graphic character whose graphic happens to be blank. Some keyboards have a key labeled  linefeed  which is an alias for C-j. 

Emacs extends the ASCII character set with thousands more printing characters (see International), additional control characters, and a few more modifiers that can be combined with any character. 

On ASCII terminals, there are only 32 possible control characters. These are the control variants of letters and  @[]\^_ . In addition, the shift key is meaningless with control characters: C-a and C-A are the same character, and Emacs cannot distinguish them. 

But the Emacs character set has room for control variants of all printing characters, and for distinguishing between C-a and C-A. The X Window System makes it possible to enter all these characters. For example, C-- (that's Control-Minus) and C-5 are meaningful Emacs commands under X. 

Another Emacs character-set extension is additional modifier bits. Only one modifier bit is commonly used; it is called Meta. Every character has a Meta variant; examples include Meta-a (normally written M-a, for short), M-A (not the same character as M-a, but those two characters normally have the same meaning in Emacs), M-<RET>, and M-C-a. For reasons of tradition, we usually write C-M-a rather than M-C-a; logically speaking, the order in which the modifier keys <CTRL> and <META> are mentioned does not matter. 

Some terminals have a <META> key, and allow you to type Meta characters by holding this key down. Thus, Meta-a is typed by holding down <META> and pressing a. The <META> key works much like the <SHIFT> key. Such a key is not always labeled <META>, however, as this function is often a special option for a key with some other primary purpose. Sometimes it is labeled <ALT> or <EDIT>; on a Sun keyboard, it may have a diamond on it. 

If there is no <META> key, you can still type Meta characters using two-character sequences starting with <ESC>. Thus, you can enter M-a by typing <ESC> a. You can enter C-M-a by typing <ESC> C-a. Unlike <META>, which modifies other characters, <ESC> is a separate character. You don't hold down <ESC> while typing the next character; instead, you press it and release it, then you enter the next character. <ESC> is allowed on terminals with <META> keys, too, in case you have formed a habit of using it. 

The X Window System provides several other modifier keys that can be applied to any input character. These are called <SUPER>, <HYPER> and <ALT>. We write  s- ,  H-  and  A-  to say that a character uses these modifiers. Thus, s-H-C-x is short for Super-Hyper-Control-x. Not all X terminals actually provide keys for these modifier flags in fact, many terminals have a key labeled <ALT> which is really a <META> key. The standard key bindings of Emacs do not include any characters with these modifiers. But you can assign them meanings of your own by customizing Emacs. 

If your keyboard lacks one of these modifier keys, you can enter it using C-x @: C-x @ h adds the  hyper  flag to the next character, C-x @ s adds the  super  flag, and C-x @ a adds the  alt  flag. For instance, C-x @ h C-a is a way to enter Hyper-Control-a. (Unfortunately there is no way to add two modifiers by using C-x @ twice for the same character, because the first one goes to work on the C-x.) 

Keyboard input includes keyboard keys that are not characters at all: for example function keys and arrow keys. Mouse buttons are also outside the gamut of characters. You can modify these events with the modifier keys <CTRL>, <META>, <SUPER>, <HYPER> and <ALT>, just like keyboard characters. 

Input characters and non-character inputs are collectively called input events. See Input Events, for more information. If you are not doing Lisp programming, but simply want to redefine the meaning of some characters or non-character events, see Customization. 

ASCII terminals cannot really send anything to the computer except ASCII characters. These terminals use a sequence of characters to represent each function key. But that is invisible to the Emacs user, because the keyboard input routines recognize these special sequences and convert them to function key events before any other part of Emacs gets to see them. 



--------------------------------------------------------------------------------
Next: Commands, Previous: User Input, Up: Top 
3 Keys
A key sequence (key, for short) is a sequence of input events that are meaningful as a unit as  a single command.  Some Emacs command sequences are just one character or one event; for example, just C-f is enough to move forward one character in the buffer. But Emacs also has commands that take two or more events to invoke. 

If a sequence of events is enough to invoke a command, it is a complete key. Examples of complete keys include C-a, X, <RET>, <NEXT> (a function key), <DOWN> (an arrow key), C-x C-f, and C-x 4 C-f. If it isn't long enough to be complete, we call it a prefix key. The above examples show that C-x and C-x 4 are prefix keys. Every key sequence is either a complete key or a prefix key. 

Most single characters constitute complete keys in the standard Emacs command bindings. A few of them are prefix keys. A prefix key combines with the following input event to make a longer key sequence, which may itself be complete or a prefix. For example, C-x is a prefix key, so C-x and the next input event combine to make a two-event key sequence. Most of these key sequences are complete keys, including C-x C-f and C-x b. A few, such as C-x 4 and C-x r, are themselves prefix keys that lead to three-event key sequences. There's no limit to the length of a key sequence, but in practice people rarely use sequences longer than four events. 

By contrast, you can't add more events onto a complete key. For example, the two-event sequence C-f C-k is not a key, because the C-f is a complete key in itself. It's impossible to give C-f C-k an independent meaning as a command. C-f C-k is two key sequences, not one. 

All told, the prefix keys in Emacs are C-c, C-h, C-x, C-x <RET>, C-x @, C-x a, C-x n, C-x r, C-x v, C-x 4, C-x 5, C-x 6, <ESC>, M-o and M-g. (<F1> and <F2> are aliases for C-h and C-x 6.) But this list is not cast in concrete; it is just a matter of Emacs's standard key bindings. If you customize Emacs, you can make new prefix keys, or eliminate these. See Key Bindings. 

If you do make or eliminate prefix keys, that changes the set of possible key sequences. For example, if you redefine C-f as a prefix, C-f C-k automatically becomes a key (complete, unless you define that too as a prefix). Conversely, if you remove the prefix definition of C-x 4, then C-x 4 f (or C-x 4 anything) is no longer a key. 

Typing the help character (C-h or <F1>) after a prefix key displays a list of the commands starting with that prefix. There are a few prefix keys for which C-h does not work for historical reasons, they have other meanings for C-h which are not easy to change. But <F1> should work for all prefix keys. 



--------------------------------------------------------------------------------
Next: Text Characters, Previous: Keys, Up: Top 
4 Keys and Commands
This manual is full of passages that tell you what particular keys do. But Emacs does not assign meanings to keys directly. Instead, Emacs assigns meanings to named commands, and then gives keys their meanings by binding them to commands. 

Every command has a name chosen by a programmer. The name is usually made of a few English words separated by dashes; for example, next-line or forward-word. A command also has a function definition which is a Lisp program; this is what makes the command do what it does. In Emacs Lisp, a command is actually a special kind of Lisp function; one which specifies how to read arguments for it and call it interactively. For more information on commands and functions, see What Is a Function. (The definition we use in this manual is simplified slightly.) 

The bindings between keys and commands are recorded in various tables called keymaps. See Keymaps. 

When we say that  C-n moves down vertically one line  we are glossing over a distinction that is irrelevant in ordinary use but is vital in understanding how to customize Emacs. It is the command next-line that is programmed to move down vertically. C-n has this effect because it is bound to that command. If you rebind C-n to the command forward-word then C-n will move forward by words instead. Rebinding keys is a common method of customization. 

In the rest of this manual, we usually ignore this distinction to keep things simple. We will often speak of keys like C-n as commands, even though strictly speaking a key is bound to some command. To give the information needed for customization, we state the name of the command which really does the work in parentheses after mentioning the key that runs it. For example, we will say that  The command C-n (next-line) moves point vertically down,  meaning that next-line is a command that moves vertically down, and C-n is a key that is normally bound to it. 

While we are on the subject of information for customization only, it's a good time to tell you about variables. Often the description of a command will say,  To change this, set the variable mumble-foo.  A variable is a name used to remember a value. Most of the variables documented in this manual exist just to facilitate customization: some command or other part of Emacs examines the variable and behaves differently according to the value that you set. Until you are interested in customizing, you can ignore the information about variables. When you are ready to be interested, read the basic information on variables, and then the information on individual variables will make sense. See Variables. 



--------------------------------------------------------------------------------
Next: Entering Emacs, Previous: Commands, Up: Top 
5 Character Set for Text
Text in Emacs buffers is a sequence of 8-bit bytes. Each byte can hold a single ASCII character. Both ASCII control characters (octal codes 000 through 037, and 0177) and ASCII printing characters (codes 040 through 0176) are allowed; however, non-ASCII control characters cannot appear in a buffer. The other modifier flags used in keyboard input, such as Meta, are not allowed in buffers either. 

Some ASCII control characters serve special purposes in text, and have special names. For example, the newline character (octal code 012) is used in the buffer to end a line, and the tab character (octal code 011) is used for indenting to the next tab stop column (normally every 8 columns). See Text Display. 

Non-ASCII printing characters can also appear in buffers. When multibyte characters are enabled, you can use any of the non-ASCII printing characters that Emacs supports. They have character codes starting at 256, octal 0400, and each one is represented as a sequence of two or more bytes. See International. Single-byte characters with codes 128 through 255 can also appear in multibyte buffers. 

If you disable multibyte characters, then you can use only one alphabet of non-ASCII characters, but they all fit in one byte. They use codes 0200 through 0377. See Single-Byte Character Support. 



--------------------------------------------------------------------------------
Next: Exiting, Previous: Text Characters, Up: Top 
6 Entering and Exiting Emacs
The usual way to invoke Emacs is with the shell command emacs. Emacs clears the screen and then displays an initial help message and copyright notice. Some operating systems discard all type-ahead when Emacs starts up; they give Emacs no way to prevent this. Therefore, it is advisable to wait until Emacs clears the screen before typing your first editing command. 

If you run Emacs from a shell window under the X Window System, run it in the background with emacs&. This way, Emacs does not tie up the shell window, so you can use that to run other shell commands while Emacs operates its own X windows. You can begin typing Emacs commands as soon as you direct your keyboard input to the Emacs frame. 

When Emacs starts up, it creates a buffer named  *scratch* . That's the buffer you start out in. The  *scratch*  buffer uses Lisp Interaction mode; you can use it to type Lisp expressions and evaluate them, or you can ignore that capability and simply doodle. (You can specify a different major mode for this buffer by setting the variable initial-major-mode in your init file. See Init File.) 

It is possible to specify files to be visited, Lisp files to be loaded, and functions to be called, by giving Emacs arguments in the shell command line. See Emacs Invocation. But we don't recommend doing this. The feature exists mainly for compatibility with other editors. 

Many other editors are designed to be started afresh each time you want to edit. You edit one file and then exit the editor. The next time you want to edit either another file or the same one, you must run the editor again. With these editors, it makes sense to use a command-line argument to say which file to edit. 

But starting a new Emacs each time you want to edit a different file does not make sense. This would fail to take advantage of Emacs's ability to visit more than one file in a single editing session, and it would lose the other accumulated context, such as the kill ring, registers, undo history, and mark ring, that are useful for operating on multiple files. 

The recommended way to use GNU Emacs is to start it only once, just after you log in, and do all your editing in the same Emacs session. Each time you want to edit a different file, you visit it with the existing Emacs, which eventually comes to have many files in it ready for editing. Usually you do not kill the Emacs until you are about to log out. See Files, for more information on visiting more than one file. 

If you want to edit a file from another program and already have Emacs running, you can use the emacsclient program to open a file in the already running Emacs. See Emacs Server, for more information on editing files with Emacs from other programs. 



--------------------------------------------------------------------------------
Next: Basic, Previous: Entering Emacs, Up: Top 
7 Exiting Emacs
There are two commands for exiting Emacs because there are three kinds of exiting: suspending Emacs, Iconifying Emacs, and killing Emacs. 

Suspending means stopping Emacs temporarily and returning control to its parent process (usually a shell), allowing you to resume editing later in the same Emacs job, with the same buffers, same kill ring, same undo history, and so on. This is the usual way to exit Emacs when running on a text terminal. 

Iconifying means replacing the Emacs frame with a small box somewhere on the screen. This is the usual way to exit Emacs when you're using a graphics terminal. 

Killing Emacs means destroying the Emacs job. You can run Emacs again later, but you will get a fresh Emacs; there is no way to resume the same editing session after it has been killed. 

C-z
Suspend Emacs (suspend-emacs) or iconify a frame (iconify-or-deiconify-frame). 

C-x C-c
Kill Emacs (save-buffers-kill-emacs). 
To suspend or iconify Emacs, type C-z (suspend-emacs). On text terminals, this suspends Emacs. On graphics terminals, it iconifies the Emacs frame. 

Suspending Emacs takes you back to the shell from which you invoked Emacs. You can resume Emacs with the shell command %emacs in most common shells. On systems that don't support suspending programs, C-z starts an inferior shell that communicates directly with the terminal. Emacs waits until you exit the subshell. (The way to do that is probably with C-d or exit, but it depends on which shell you use.) The only way on these systems to get back to the shell from which Emacs was run (to log out, for example) is to kill Emacs. 

Suspending can fail if you run Emacs under a shell that doesn't support suspending programs, even if the system itself does support it. In such a case, you can set the variable cannot-suspend to a non-nil value to force C-z to start an inferior shell. (One might also describe Emacs's parent shell as  inferior  for failing to support job control properly, but that is a matter of taste.) 

On graphics terminals, C-z has a different meaning: it runs the command iconify-or-deiconify-frame, which temporarily iconifies (or  minimizes ) the selected Emacs frame (see Frames). Then you can use the window manager to get back to a shell window. 

To exit and kill Emacs, type C-x C-c (save-buffers-kill-emacs). A two-character key is used for this to make it harder to type by accident. This command first offers to save any modified file-visiting buffers. If you do not save them all, it asks for reconfirmation with yes before killing Emacs, since any changes not saved will be lost forever. Also, if any subprocesses are still running, C-x C-c asks for confirmation about them, since killing Emacs will also kill the subprocesses. 

If the value of the variable confirm-kill-emacs is non-nil, C-x C-c assumes that its value is a predicate function, and calls that function. If the result is non-nil, the session is killed, otherwise Emacs continues to run. One convenient function to use as the value of confirm-kill-emacs is the function yes-or-no-p. The default value of confirm-kill-emacs is nil. 

There is no way to resume an Emacs session once you have killed it. You can, however, arrange for Emacs to record certain session information when you kill it, such as which files are visited, so that the next time you start Emacs it will try to visit the same files and so on. See Saving Emacs Sessions. 

The operating system usually listens for certain special characters whose meaning is to kill or suspend the program you are running. This operating system feature is turned off while you are in Emacs. The meanings of C-z and C-x C-c as keys in Emacs were inspired by the use of C-z and C-c on several operating systems as the characters for stopping or killing a program, but that is their only relationship with the operating system. You can customize these keys to run any commands of your choice (see Keymaps). 



--------------------------------------------------------------------------------
Next: Minibuffer, Previous: Exiting, Up: Top 
8 Basic Editing Commands
We now give the basics of how to enter text, make corrections, and save the text in a file. If this material is new to you, you might learn it more easily by running the Emacs learn-by-doing tutorial. To use the tutorial, run Emacs and type Control-h t (help-with-tutorial). 

To clear the screen and redisplay, type C-l (recenter). 

Inserting Text: Inserting text by simply typing it. 
Moving Point: How to move the cursor to the place where you want to change something. 
Erasing: Deleting and killing text. 
Undo: Undoing recent changes in the text. 
Files: Visiting, creating, and saving files. 
Help: Asking what a character does. 
Blank Lines: Commands to make or delete blank lines. 
Continuation Lines: Lines too wide for the screen. 
Position Info: What page, line, row, or column is point on? 
Arguments: Numeric arguments for repeating a command. 
Repeating: A short-cut for repeating the previous command. 


--------------------------------------------------------------------------------
Next: Moving Point, Up: Basic 
8.1 Inserting Text
To insert printing characters into the text you are editing, just type them. This inserts the characters you type into the buffer at the cursor (that is, at point; see Point). The cursor moves forward, and any text after the cursor moves forward too. If the text in the buffer is  FOOBAR , with the cursor before the  B , then if you type XX, you get  FOOXXBAR , with the cursor still before the  B . 

To delete text you have just inserted, use the large key labeled <DEL>, <BACKSPACE> or <DELETE> which is a short distance above the <RET> or <ENTER> key. This is the key you normally use, outside Emacs, for erasing the last character that you typed. Regardless of the label on that key, Emacs thinks of it as <DEL>, and that's what we call it in this manual. 

The <DEL> key deletes the character before the cursor. As a consequence, the cursor and all the characters after it move backwards. If you type a printing character and then type <DEL>, they cancel out. 

On most computers, Emacs recognizes automatically which key ought to be <DEL>, and sets it up that way. But in some cases, especially with text-only terminals, you will need to tell Emacs which key to use for that purpose. If the large key not far above the <RET> or <ENTER> key doesn't delete backwards, you need to do this. See DEL Does Not Delete, for an explanation of how. 

Most PC keyboards have both a <BACKSPACE> key a short ways above <RET> or <ENTER>, and a <DELETE> key elsewhere. On these keyboards, Emacs supports when possible the usual convention that the <BACKSPACE> key deletes backwards (it is <DEL>), while the <DELETE> key deletes  forwards,  deleting the character after point, the one underneath the cursor, like C-d (see below). 

To end a line and start typing a new one, type <RET>. This inserts a newline character in the buffer. If point is in the middle of a line, the effect is to split the line. Typing <DEL> when the cursor is at the beginning of a line deletes the preceding newline, thus joining the line with the preceding line. 

Emacs can split lines automatically when they become too long, if you turn on a special minor mode called Auto Fill mode. See Filling, for how to use Auto Fill mode. 

If you prefer to have text characters replace (overwrite) existing text rather than shove it to the right, you can enable Overwrite mode, a minor mode. See Minor Modes. 

Direct insertion works for printing characters and <SPC>, but other characters act as editing commands and do not insert themselves. If you need to insert a control character or a character whose code is above 200 octal, you must quote it by typing the character Control-q (quoted-insert) first. (This character's name is normally written C-q for short.) There are two ways to use C-q: 

C-q followed by any non-graphic character (even C-g) inserts that character. 
C-q followed by a sequence of octal digits inserts the character with the specified octal character code. You can use any number of octal digits; any non-digit terminates the sequence. If the terminating character is <RET>, it serves only to terminate the sequence. Any other non-digit terminates the sequence and then acts as normal input thus, C-q 1 0 1 B inserts  AB . 
The use of octal sequences is disabled in ordinary non-binary Overwrite mode, to give you a convenient way to insert a digit instead of overwriting with it. 

When multibyte characters are enabled, if you specify a code in the range 0200 through 0377 octal, C-q assumes that you intend to use some ISO 8859-n character set, and converts the specified code to the corresponding Emacs character code. See Enabling Multibyte. You select which of the ISO 8859 character sets to use through your choice of language environment (see Language Environments). 

To use decimal or hexadecimal instead of octal, set the variable read-quoted-char-radix to 10 or 16. If the radix is greater than 10, some letters starting with a serve as part of a character code, just like digits. 

A numeric argument to C-q specifies how many copies of the quoted character should be inserted (see Arguments). 

Customization information: <DEL> in most modes runs the command delete-backward-char; <RET> runs the command newline, and self-inserting printing characters run the command self-insert, which inserts whatever character was typed to invoke it. Some major modes rebind <DEL> to other commands. 



--------------------------------------------------------------------------------
Next: Erasing, Previous: Inserting Text, Up: Basic 
8.2 Changing the Location of Point
To do more than insert characters, you have to know how to move point (see Point). The simplest way to do this is with arrow keys, or by clicking the left mouse button where you want to move to. 

There are also control and meta characters for cursor motion. Some are equivalent to the arrow keys (these date back to the days before terminals had arrow keys, and are usable on terminals which don't have them). Others do more sophisticated things. 


C-a
Move to the beginning of the line (move-beginning-of-line). 

C-e
Move to the end of the line (move-end-of-line). 

C-f
Move forward one character (forward-char). The right-arrow key does the same thing. 

C-b
Move backward one character (backward-char). The left-arrow key has the same effect. 

M-f
Move forward one word (forward-word). 

M-b
Move backward one word (backward-word). 

C-n
Move down one line, vertically (next-line). This command attempts to keep the horizontal position unchanged, so if you start in the middle of one line, you end in the middle of the next. The down-arrow key does the same thing. 

C-p
Move up one line, vertically (previous-line). The up-arrow key has the same effect. 

M-r
Move point to left margin, vertically centered in the window (move-to-window-line). Text does not move on the screen. 
A numeric argument says which screen line to place point on. It counts screen lines down from the top of the window (zero for the top line). A negative argument counts lines from the bottom (?1 for the bottom line). 


M-<
Move to the top of the buffer (beginning-of-buffer). With numeric argument n, move to n/10 of the way from the top. See Arguments, for more information on numeric arguments. 

M->
Move to the end of the buffer (end-of-buffer). 

C-v
<PAGEDOWN>
<PRIOR>
Scroll the display one screen forward, and move point if necessary to put it on the screen (scroll-up). This doesn't always move point, but it is commonly used to do so. If your keyboard has a <PAGEDOWN> or <PRIOR> key, it does the same thing. 
Scrolling commands are further described in Scrolling. 


M-v
<PAGEUP>
<NEXT>
Scroll one screen backward, and move point if necessary to put it on the screen (scroll-down). This doesn't always move point, but it is commonly used to do so. If your keyboard has a <PAGEUP> or <NEXT> key, it does the same thing. 

M-x goto-char
Read a number n and move point to buffer position n. Position 1 is the beginning of the buffer. 

M-g M-g
M-g g
M-x goto-line
Read a number n and move point to the beginning of line number n. Line 1 is the beginning of the buffer. If point is on or just after a number, then that is the default for n, if you just press <RET> with an empty minibuffer. 

C-x C-n
Use the current column of point as the semipermanent goal column for C-n and C-p (set-goal-column). Henceforth, those commands always move to this column in each line moved into, or as close as possible given the contents of the line. This goal column remains in effect until canceled. 

C-u C-x C-n
Cancel the goal column. Henceforth, C-n and C-p once again try to stick to a fixed horizontal position, as usual. 
If you set the variable track-eol to a non-nil value, then C-n and C-p, when starting at the end of the line, move to the end of another line. Normally, track-eol is nil. See Variables, for how to set variables such as track-eol. 

C-n normally stops at the end of the buffer when you use it on the last line of the buffer. But if you set the variable next-line-add-newlines to a non-nil value, C-n on the last line of a buffer creates an additional line at the end and moves down onto it. 



--------------------------------------------------------------------------------
Next: Undo, Previous: Moving Point, Up: Basic 
8.3 Erasing Text
<DEL>
Delete the character before point (delete-backward-char). 

C-d
Delete the character after point (delete-char). 

<DELETE>
<BACKSPACE>
One of these keys, whichever is the large key above the <RET> or <ENTER> key, deletes the character before point, like <DEL>. If that is <BACKSPACE>, and your keyboard also has <DELETE>, then <DELETE> deletes forwards, like C-d. 

C-k
Kill to the end of the line (kill-line). 

M-d
Kill forward to the end of the next word (kill-word). 

M-<DEL>
Kill back to the beginning of the previous word (backward-kill-word). 
You already know about the <DEL> key which deletes the character before point (that is, before the cursor). Another key, Control-d (C-d for short), deletes the character after point (that is, the character that the cursor is on). This shifts the rest of the text on the line to the left. If you type C-d at the end of a line, it joins together that line and the next line. 

To erase a larger amount of text, use the C-k key, which kills a line at a time. If you type C-k at the beginning or middle of a line, it kills all the text up to the end of the line. If you type C-k at the end of a line, it joins that line and the next line. 

See Killing, for more flexible ways of killing text. 



--------------------------------------------------------------------------------
Next: Basic Files, Previous: Erasing, Up: Basic 
8.4 Undoing Changes
You can undo all the recent changes in the buffer text, up to a certain point. Each buffer records changes individually, and the undo command always applies to the current buffer. Usually each editing command makes a separate entry in the undo records, but some commands such as query-replace make many entries, and very simple commands such as self-inserting characters are often grouped to make undoing less tedious. 

C-x u
Undo one batch of changes usually, one command worth (undo). 

C-_
C-/
The same. 

C-u C-x u
Undo one batch of changes in the region. 
The command C-x u (or C-_ or C-/) is how you undo. The first time you give this command, it undoes the last change. Point moves back to where it was before the command that made the change. 

Consecutive repetitions of C-_ or C-x u undo earlier and earlier changes, back to the limit of the undo information available. If all recorded changes have already been undone, the undo command displays an error message and does nothing. 

Any command other than an undo command breaks the sequence of undo commands. Starting from that moment, the previous undo commands become ordinary changes that you can undo. Thus, to redo changes you have undone, type C-f or any other command that will harmlessly break the sequence of undoing, then type more undo commands. On the other hand, if you want to ignore previous undo commands, use M-x undo-only. This is like undo, but will not redo changes you have just undone. 

Ordinary undo applies to all changes made in the current buffer. You can also perform selective undo, limited to the current region (see Mark). To do this, specify the region you want, then run the undo command with a prefix argument (the value does not matter): C-u C-x u or C-u C-_. This undoes the most recent change in the region. To undo further changes in the same region, repeat the undo command (no prefix argument is needed). In Transient Mark mode (see Transient Mark), any use of undo when there is an active region performs selective undo; you do not need a prefix argument. 

If you notice that a buffer has been modified accidentally, the easiest way to recover is to type C-_ repeatedly until the stars disappear from the front of the mode line. At this time, all the modifications you made have been canceled. Whenever an undo command makes the stars disappear from the mode line, it means that the buffer contents are the same as they were when the file was last read in or saved. 

If you do not remember whether you changed the buffer deliberately, type C-_ once. When you see the last change you made undone, you will see whether it was an intentional change. If it was an accident, leave it undone. If it was deliberate, redo the change as described above. 

Not all buffers record undo information. Buffers whose names start with spaces don't; these buffers are used internally by Emacs and its extensions to hold text that users don't normally look at or edit. 

You cannot undo mere cursor motion; only changes in the buffer contents save undo information. However, some cursor motion commands set the mark, so if you use these commands from time to time, you can move back to the neighborhoods you have moved through by popping the mark ring (see Mark Ring). 

When the undo information for a buffer becomes too large, Emacs discards the oldest undo information from time to time (during garbage collection). You can specify how much undo information to keep by setting three variables: undo-limit, undo-strong-limit, and undo-outer-limit. Their values are expressed in units of bytes of space. 

The variable undo-limit sets a soft limit: Emacs keeps undo data for enough commands to reach this size, and perhaps exceed it, but does not keep data for any earlier commands beyond that. Its default value is 20000. The variable undo-strong-limit sets a stricter limit: a previous command (not the most recent one) which pushes the size past this amount is itself forgotten. The default value of undo-strong-limit is 30000. 

Regardless of the values of those variables, the most recent change is never discarded unless it gets bigger than undo-outer-limit (normally 3,000,000). At that point, Emacs discards the undo data and warns you about it. This is the only situation in which you cannot undo the last command. If this happens, you can increase the value of undo-outer-limit to make it even less likely to happen in the future. But if you didn't expect the command to create such large undo data, then it is probably a bug and you should report it. See Reporting Bugs. 

The reason the undo command has three key bindings, C-x u, C-_ and C-/, is that it is worthy of a single-character key, but C-x u is more straightforward for beginners to type. 



--------------------------------------------------------------------------------
Next: Basic Help, Previous: Undo, Up: Basic 
8.5 Files
The commands described above are sufficient for creating and altering text in an Emacs buffer; the more advanced Emacs commands just make things easier. But to keep any text permanently you must put it in a file. Files are named units of text which are stored by the operating system for you to retrieve later by name. To look at or use the contents of a file in any way, including editing the file with Emacs, you must specify the file name. 

Consider a file named /usr/rms/foo.c. In Emacs, to begin editing this file, type 

     C-x C-f /usr/rms/foo.c <RET>

Here the file name is given as an argument to the command C-x C-f (find-file). That command uses the minibuffer to read the argument, and you type <RET> to terminate the argument (see Minibuffer). 

Emacs obeys the command by visiting the file: creating a buffer, copying the contents of the file into the buffer, and then displaying the buffer for you to edit. If you alter the text, you can save the new text in the file by typing C-x C-s (save-buffer). This makes the changes permanent by copying the altered buffer contents back into the file /usr/rms/foo.c. Until you save, the changes exist only inside Emacs, and the file foo.c is unaltered. 

To create a file, just visit the file with C-x C-f as if it already existed. This creates an empty buffer in which you can insert the text you want to put in the file. The file is actually created when you save this buffer with C-x C-s. 

Of course, there is a lot more to learn about using files. See Files. 



--------------------------------------------------------------------------------
Next: Blank Lines, Previous: Basic Files, Up: Basic 
8.6 Help
If you forget what a key does, you can find out with the Help character, which is C-h (or <F1>, which is an alias for C-h). Type C-h k followed by the key you want to know about; for example, C-h k C-n tells you all about what C-n does. C-h is a prefix key; C-h k is just one of its subcommands (the command describe-key). The other subcommands of C-h provide different kinds of help. Type C-h twice to get a description of all the help facilities. See Help. 



--------------------------------------------------------------------------------
Next: Continuation Lines, Previous: Basic Help, Up: Basic 
8.7 Blank Lines
Here are special commands and techniques for putting in and taking out blank lines. 

C-o
Insert one or more blank lines after the cursor (open-line). 

C-x C-o
Delete all but one of many consecutive blank lines (delete-blank-lines). 
When you want to insert a new line of text before an existing line, you can do it by typing the new line of text, followed by <RET>. However, it may be easier to see what you are doing if you first make a blank line and then insert the desired text into it. This is easy to do using the key C-o (open-line), which inserts a newline after point but leaves point in front of the newline. After C-o, type the text for the new line. C-o F O O has the same effect as F O O <RET>, except for the final location of point. 

You can make several blank lines by typing C-o several times, or by giving it a numeric argument to tell it how many blank lines to make. See Arguments, for how. If you have a fill prefix, the C-o command inserts the fill prefix on the new line, when you use it at the beginning of a line. See Fill Prefix. 

The easy way to get rid of extra blank lines is with the command C-x C-o (delete-blank-lines). C-x C-o in a run of several blank lines deletes all but one of them. C-x C-o on a solitary blank line deletes that blank line. When point is on a nonblank line, C-x C-o deletes any blank lines following that nonblank line. 



--------------------------------------------------------------------------------
Next: Position Info, Previous: Blank Lines, Up: Basic 
8.8 Continuation Lines
If you add too many characters to one line without breaking it with <RET>, the line grows to occupy two (or more) lines on the screen. On graphical displays, Emacs indicates line wrapping with small bent arrows in the fringes to the left and right of the window. On text-only terminals, Emacs displays a  \  character at the right margin of a screen line if it is not the last in its text line. This  \  character says that the following screen line is not really a distinct line in the text, just a continuation of a line too long to fit the screen. Continuation is also called line wrapping. 

When line wrapping occurs before a character that is wider than one column, some columns at the end of the previous screen line may be  empty.  In this case, Emacs displays additional  \  characters in the  empty  columns, just before the  \  character that indicates continuation. 

Sometimes it is nice to have Emacs insert newlines automatically when a line gets too long. Continuation on the screen does not do that. Use Auto Fill mode (see Filling) if that's what you want. 

As an alternative to continuation, Emacs can display long lines by truncation. This means that all the characters that do not fit in the width of the screen or window do not appear at all.  $  in the last column or a small straight arrow in the fringe to the right of the window indicates a truncated line. 

See Display Custom, for more information about line truncation, and other variables that affect how text is displayed. 



--------------------------------------------------------------------------------
Next: Arguments, Previous: Continuation Lines, Up: Basic 
8.9 Cursor Position Information
Here are commands to get information about the size and position of parts of the buffer, and to count lines. 

M-x what-page
Display the page number of point, and the line number within the page. 

M-x what-line
Display the line number of point in the buffer. 

M-x line-number-mode
M-x column-number-mode
Toggle automatic display of current line number or column number. See Optional Mode Line. 

M-=
Display the number of lines in the current region (count-lines-region). See Mark, for information about the region. 

C-x =
Display the character code of character after point, character position of point, and column of point (what-cursor-position). 

M-x hl-line-mode
Enable or disable highlighting of the current line. See Cursor Display. 

M-x size-indication-mode
Toggle automatic display of the size of the buffer. See Optional Mode Line. 
M-x what-line computes the current line number and displays it in the echo area. You can also see the current line number in the mode line; see Mode Line. If you narrow the buffer, then the line number in the mode line is relative to the accessible portion (see Narrowing). By contrast, what-line shows both the line number relative to the narrowed region and the line number relative to the whole buffer. 

M-x what-page counts pages from the beginning of the file, and counts lines within the page, showing both numbers in the echo area. See Pages. 

While on this subject, we might as well mention M-= (count-lines-region), which displays the number of lines in the region (see Mark). See Pages, for the command C-x l which counts the lines in the current page. 

The command C-x = (what-cursor-position) shows what column the cursor is in, and other miscellaneous information about point and the character after it. It displays a line in the echo area that looks like this: 

     Char: c (99, #o143, #x63) point=28062 of 36168 (78%) column=53

The four values after  Char:  describe the character that follows point, first by showing it and then by giving its character code in decimal, octal and hex. For a non-ASCII multibyte character, these are followed by  file  and the character's representation, in hex, in the buffer's coding system, if that coding system encodes the character safely and with a single byte (see Coding Systems). If the character's encoding is longer than one byte, Emacs shows  file ... . 

However, if the character displayed is in the range 0200 through 0377 octal, it may actually stand for an invalid UTF-8 byte read from a file. In Emacs, that byte is represented as a sequence of 8-bit characters, but all of them together display as the original invalid byte, in octal code. In this case, C-x = shows  part of display ...  instead of  file . 

 point=  is followed by the position of point expressed as a character count. The front of the buffer counts as position 1, one character later as 2, and so on. The next, larger, number is the total number of characters in the buffer. Afterward in parentheses comes the position expressed as a percentage of the total size. 

 column=  is followed by the horizontal position of point, in columns from the left edge of the window. 

If the buffer has been narrowed, making some of the text at the beginning and the end temporarily inaccessible, C-x = displays additional text describing the currently accessible range. For example, it might display this: 

     Char: C (67, #o103, #x43) point=252 of 889 (28%) <231-599> column=0

where the two extra numbers give the smallest and largest character position that point is allowed to assume. The characters between those two positions are the accessible ones. See Narrowing. 

If point is at the end of the buffer (or the end of the accessible part), the C-x = output does not describe a character after point. The output might look like this: 

     point=36169 of 36168 (EOB) column=0

C-u C-x = displays the following additional information about a character. 

The character set name, and the codes that identify the character within that character set; ASCII characters are identified as belonging to the ascii character set. 
The character's syntax and categories. 
The character's encodings, both internally in the buffer, and externally if you were to save the file. 
What to type to input the character in the current input method (if it supports the character). 
If you are running Emacs on a window system, the font name and glyph code for the character. If you are running Emacs on a terminal, the code(s) sent to the terminal. 
The character's text properties (see Text Properties), and any overlays containing it (see Overlays). 
Here's an example showing the Latin-1 character A with grave accent, in a buffer whose coding system is iso-latin-1, whose terminal coding system is iso-latin-1 (so the terminal actually displays the character as  A ), and which has font-lock-mode (see Font Lock) enabled: 

       character: A (2240, #o4300, #x8c0, U+00C0)
         charset: [latin-iso8859-1]
                  (Right-Hand Part of Latin Alphabet 1...
      code point: [64]
          syntax: w 	which means: word
        category: l:Latin
        to input: type "`A" with [latin-1-prefix]
     buffer code: #x81 #xC0
       file code: ESC #x2C #x41 #x40 (encoded by coding system iso-2022-7bit)
         display: terminal code #xC0
     
     There are text properties here:
       fontified            t



--------------------------------------------------------------------------------
Next: Repeating, Previous: Position Info, Up: Basic 
8.10 Numeric Arguments
In mathematics and computer usage, the word argument means  data provided to a function or operation.  You can give any Emacs command a numeric argument (also called a prefix argument). Some commands interpret the argument as a repetition count. For example, C-f with an argument of ten moves forward ten characters instead of one. With these commands, no argument is equivalent to an argument of one. Negative arguments tell most such commands to move or act in the opposite direction. 

If your terminal keyboard has a <META> key (labeled <ALT> on PC keyboards), the easiest way to specify a numeric argument is to type digits and/or a minus sign while holding down the <META> key. For example, 

     M-5 C-n

would move down five lines. The characters Meta-1, Meta-2, and so on, as well as Meta--, do this because they are keys bound to commands (digit-argument and negative-argument) that are defined to contribute to an argument for the next command. Meta-- without digits normally means ?1. Digits and - modified with Control, or Control and Meta, also specify numeric arguments. 

Another way of specifying an argument is to use the C-u (universal-argument) command followed by the digits of the argument. With C-u, you can type the argument digits without holding down modifier keys; C-u works on all terminals. To type a negative argument, type a minus sign after C-u. Just a minus sign without digits normally means ?1. 

C-u followed by a character which is neither a digit nor a minus sign has the special meaning of  multiply by four.  It multiplies the argument for the next command by four. C-u twice multiplies it by sixteen. Thus, C-u C-u C-f moves forward sixteen characters. This is a good way to move forward  fast,  since it moves about 1/5 of a line in the usual size screen. Other useful combinations are C-u C-n, C-u C-u C-n (move down a good fraction of a screen), C-u C-u C-o (make  a lot  of blank lines), and C-u C-k (kill four lines). 

Some commands care only about whether there is an argument, and not about its value. For example, the command M-q (fill-paragraph) with no argument fills text; with an argument, it justifies the text as well. (See Filling, for more information on M-q.) Plain C-u is a handy way of providing an argument for such commands. 

Some commands use the value of the argument as a repeat count, but do something peculiar when there is no argument. For example, the command C-k (kill-line) with argument n kills n lines, including their terminating newlines. But C-k with no argument is special: it kills the text up to the next newline, or, if point is right at the end of the line, it kills the newline itself. Thus, two C-k commands with no arguments can kill a nonblank line, just like C-k with an argument of one. (See Killing, for more information on C-k.) 

A few commands treat a plain C-u differently from an ordinary argument. A few others may treat an argument of just a minus sign differently from an argument of ?1. These unusual cases are described when they come up; they are always for reasons of convenience of use of the individual command, and they are documented in the command's documentation string. 

You can use a numeric argument to insert multiple copies of a character. This is straightforward unless the character is a digit; for example, C-u 6 4 a inserts 64 copies of the character  a . But this does not work for inserting digits; C-u 6 4 1 specifies an argument of 641, rather than inserting anything. To separate the digit to insert from the argument, type another C-u; for example, C-u 6 4 C-u 1 does insert 64 copies of the character  1 . 

We use the term  prefix argument  as well as  numeric argument  to emphasize that you type the argument before the command, and to distinguish these arguments from minibuffer arguments that come after the command. 



--------------------------------------------------------------------------------
Previous: Arguments, Up: Basic 
8.11 Repeating a Command
Many simple commands, such as those invoked with a single key or with M-x command-name <RET>, can be repeated by invoking them with a numeric argument that serves as a repeat count (see Arguments). However, if the command you want to repeat prompts for some input, or uses a numeric argument in another way, repetition using a numeric argument might be problematical. 

The command C-x z (repeat) provides another way to repeat an Emacs command many times. This command repeats the previous Emacs command, whatever that was. Repeating a command uses the same arguments that were used before; it does not read new arguments each time. 

To repeat the command more than once, type additional z's: each z repeats the command one more time. Repetition ends when you type a character other than z, or press a mouse button. 

For example, suppose you type C-u 2 0 C-d to delete 20 characters. You can repeat that command (including its argument) three additional times, to delete a total of 80 characters, by typing C-x z z z. The first C-x z repeats the command once, and each subsequent z repeats it once again. 



--------------------------------------------------------------------------------
Next: M-x, Previous: Basic, Up: Top 
9 The Minibuffer
The minibuffer is the facility used by Emacs commands to read arguments more complicated than a single number. Minibuffer arguments can be file names, buffer names, Lisp function names, Emacs command names, Lisp expressions, and many other things, depending on the command reading the argument. You can use the usual Emacs editing commands in the minibuffer to edit the argument text. 

When the minibuffer is in use, it appears in the echo area, and the terminal's cursor moves there. The beginning of the minibuffer line displays a prompt in a special color, to say what kind of input you should supply and how it will be used. Often this prompt is derived from the name of the command that the argument is for. The prompt normally ends with a colon. 

Sometimes a default argument appears in parentheses before the colon; it too is part of the prompt. The default will be used as the argument value if you enter an empty argument (that is, just type <RET>). For example, commands that read buffer names always show a default, which is the name of the buffer that will be used if you type just <RET>. 

The simplest way to enter a minibuffer argument is to type the text you want, terminated by <RET> which exits the minibuffer. You can cancel the command that wants the argument, and get out of the minibuffer, by typing C-g. 

Since the minibuffer uses the screen space of the echo area, it can conflict with other ways Emacs customarily uses the echo area. Here is how Emacs handles such conflicts: 

If a command gets an error while you are in the minibuffer, this does not cancel the minibuffer. However, the echo area is needed for the error message and therefore the minibuffer itself is hidden for a while. It comes back after a few seconds, or as soon as you type anything. 
If in the minibuffer you use a command whose purpose is to display a message in the echo area, such as C-x =, the message hides the minibuffer for a while. The minibuffer contents come back after a few seconds, or as soon as you type anything. 
Echoing of keystrokes does not take place while the minibuffer is in use. 
File: Entering file names with the minibuffer. 
Edit: How to edit in the minibuffer. 
Completion: An abbreviation facility for minibuffer input. 
Minibuffer History: Reusing recent minibuffer arguments. 
Repetition: Re-executing commands that used the minibuffer. 


--------------------------------------------------------------------------------
Next: Minibuffer Edit, Up: Minibuffer 
9.1 Minibuffers for File Names
Sometimes the minibuffer starts out with text in it. For example, when you are supposed to give a file name, the minibuffer starts out containing the default directory, which ends with a slash. This is to inform you which directory the file will be found in if you do not specify a directory. 

For example, the minibuffer might start out with these contents: 

     Find File: /u2/emacs/src/

where  Find File:   is the prompt. Typing buffer.c as input specifies the file /u2/emacs/src/buffer.c. To find files in nearby directories, use ..; thus, if you type ../lisp/simple.el, you will get the file named /u2/emacs/lisp/simple.el. Alternatively, you can kill with M-<DEL> the directory names you don't want (see Words). 

If you don't want any of the default, you can kill it with C-a C-k. But you don't need to kill the default; you can simply ignore it. Insert an absolute file name, one starting with a slash or a tilde, after the default directory. For example, to specify the file /etc/termcap, just insert that name, giving these minibuffer contents: 

     Find File: /u2/emacs/src//etc/termcap

GNU Emacs gives a special meaning to a double slash (which is not normally a useful thing to write): it means,  ignore everything before the second slash in the pair.  Thus,  /u2/emacs/src/  is ignored in the example above, and you get the file /etc/termcap. The ignored part of the file name is dimmed if the terminal allows it; to disable this, turn off file-name-shadow-mode minor mode. 

If you set insert-default-directory to nil, the default directory is not inserted in the minibuffer. This way, the minibuffer starts out empty. But the name you type, if relative, is still interpreted with respect to the same default directory. 



--------------------------------------------------------------------------------
Next: Completion, Previous: Minibuffer File, Up: Minibuffer 
9.2 Editing in the Minibuffer
The minibuffer is an Emacs buffer (albeit a peculiar one), and the usual Emacs commands are available for editing the text of an argument you are entering. 

Since <RET> in the minibuffer is defined to exit the minibuffer, you can't use it to insert a newline in the minibuffer. To do that, type C-o or C-q C-j. (On text terminals, newline is really the ASCII character control-J.) 

The minibuffer has its own window which always has space on the screen but acts as if it were not there when the minibuffer is not in use. When the minibuffer is in use, its window is just like the others; you can switch to another window with C-x o, edit text in other windows and perhaps even visit more files, before returning to the minibuffer to submit the argument. You can kill text in another window, return to the minibuffer window, and then yank the text to use it in the argument. See Windows. 

There are some restrictions on the use of the minibuffer window, however. You cannot switch buffers in it the minibuffer and its window are permanently attached. Also, you cannot split or kill the minibuffer window. But you can make it taller in the normal fashion with C-x ^. 

The minibuffer window expands vertically as necessary to hold the text that you put in the minibuffer. If resize-mini-windows is t (the default), the window is always resized to fit the size of the text it displays. If its value is the symbol grow-only, the window grows when the size of displayed text increases, but shrinks (back to the normal size) only when the minibuffer becomes inactive. If its value is nil, you have to adjust the height yourself. 

The variable max-mini-window-height controls the maximum height for resizing the minibuffer window: a floating-point number specifies a fraction of the frame's height; an integer specifies the maximum number of lines; nil means do not resize the minibuffer window automatically. The default value is 0.25. 

If, while in the minibuffer, you issue a command that displays help text of any sort in another window, you can use the C-M-v command while in the minibuffer to scroll the help text. (M-<PAGEUP> and M-<PAGEDOWN> also operate on that help text.) This lasts until you exit the minibuffer. This feature is especially useful when you display a buffer listing possible completions. See Other Window. 

Emacs normally disallows most commands that use the minibuffer while the minibuffer is active. This rule is to prevent recursive minibuffers from confusing novice users. If you want to be able to use such commands in the minibuffer, set the variable enable-recursive-minibuffers to a non-nil value. 



--------------------------------------------------------------------------------
Next: Minibuffer History, Previous: Minibuffer Edit, Up: Minibuffer 
9.3 Completion
For certain kinds of arguments, you can use completion to enter the argument value. Completion means that you type part of the argument, then Emacs visibly fills in the rest, or as much as can be determined from the part you have typed. 

When completion is available, certain keys <TAB>, <RET>, and <SPC> are rebound to complete the text in the minibuffer before point into a longer string that it stands for, by matching it against a set of completion alternatives provided by the command reading the argument. ? is defined to display a list of possible completions of what you have inserted. 

For example, when M-x uses the minibuffer to read the name of a command, it provides a list of all available Emacs command names to complete against. The completion keys match the minibuffer text against all the command names, find any additional name characters implied by the ones already present in the minibuffer, and add those characters to the ones you have given. This is what makes it possible to type M-x ins <SPC> b <RET> instead of M-x insert-buffer <RET> (for example). (<SPC> does not do completion in reading file names, because it is common to use spaces in file names on some systems.) 

Case is normally significant in completion, because it is significant in most of the names that you can complete (buffer names, file names and command names). Thus,  fo  does not complete to  Foo . Completion does ignore case distinctions for certain arguments in which case does not matter. 

Completion acts only on the text before point. If there is text in the minibuffer after point i.e., if you move point backward after typing some text into the minibuffer it remains unchanged. 

Example: Examples of using completion. 
Commands: A list of completion commands. 
Strict Completion: Different types of completion. 
Options: Options for completion. 


--------------------------------------------------------------------------------
Next: Completion Commands, Up: Completion 
9.3.1 Completion Example
A concrete example may help here. If you type M-x au <TAB>, the <TAB> looks for alternatives (in this case, command names) that start with  au . There are several, including auto-fill-mode and auto-save-mode but they are all the same as far as auto-, so the  au  in the minibuffer changes to  auto- . 

If you type <TAB> again immediately, there are multiple possibilities for the very next character it could be any of  cfilrs  so no more characters are added; instead, <TAB> displays a list of all possible completions in another window. 

If you go on to type f <TAB>, this <TAB> sees  auto-f . The only command name starting this way is auto-fill-mode, so completion fills in the rest of that. You now have  auto-fill-mode  in the minibuffer after typing just au <TAB> f <TAB>. Note that <TAB> has this effect because in the minibuffer it is bound to the command minibuffer-complete when completion is available. 



--------------------------------------------------------------------------------
Next: Strict Completion, Previous: Completion Example, Up: Completion 
9.3.2 Completion Commands
Here is a list of the completion commands defined in the minibuffer when completion is available. 

<TAB>
Complete the text before point in the minibuffer as much as possible (minibuffer-complete). 

<SPC>
Complete the minibuffer text before point, but don't go beyond one word (minibuffer-complete-word). <SPC> for completion is not available when entering a file name, since some users often put spaces in filenames. 

<RET>
Submit the text in the minibuffer as the argument, possibly completing first as described in the next node (minibuffer-complete-and-exit). See Strict Completion. 

?
Display a list of all possible completions of the text in the minibuffer (minibuffer-completion-help). 
<SPC> completes much like <TAB>, but never goes beyond the next hyphen or space. If you have  auto-f  in the minibuffer and type <SPC>, it finds that the completion is  auto-fill-mode , but it stops completing after  fill- . This gives  auto-fill- . Another <SPC> at this point completes all the way to  auto-fill-mode . The command that implements this behavior is called minibuffer-complete-word. 

Here are some commands you can use to choose a completion from a window that displays a list of completions: 

Mouse-1
Mouse-2
Clicking mouse button 1 or 2 on a completion in the list of possible completions chooses that completion (mouse-choose-completion). You normally use this command while point is in the minibuffer, but you must click in the list of completions, not in the minibuffer itself. 



<PRIOR>
M-v
Typing <PRIOR> or <PAGE-UP>, or M-v, while in the minibuffer, selects the window showing the completion list buffer (switch-to-completions). This paves the way for using the commands below. (Selecting that window in the usual ways has the same effect, but this way is more convenient.) 



<RET>
Typing <RET> in the completion list buffer chooses the completion that point is in or next to (choose-completion). To use this command, you must first switch windows to the window that shows the list of completions. 



<RIGHT>
Typing the right-arrow key <RIGHT> in the completion list buffer moves point to the following completion (next-completion). 



<LEFT>
Typing the left-arrow key <LEFT> in the completion list buffer moves point toward the beginning of the buffer, to the previous completion (previous-completion). 


--------------------------------------------------------------------------------
Next: Completion Options, Previous: Completion Commands, Up: Completion 
9.3.3 Strict Completion
There are three different ways that <RET> can work in completing minibuffers, depending on how the argument will be used. 

Strict completion is used when it is meaningless to give any argument except one of the known alternatives. For example, when C-x k reads the name of a buffer to kill, it is meaningless to give anything but the name of an existing buffer. In strict completion, <RET> refuses to exit if the text in the minibuffer does not complete to an exact match. 
Cautious completion is similar to strict completion, except that <RET> exits only if the text was an exact match already, not needing completion. If the text is not an exact match, <RET> does not exit, but it does complete the text. If it completes to an exact match, a second <RET> will exit. 
Cautious completion is used for reading file names for files that must already exist. 

Permissive completion is used when any string whatever is meaningful, and the list of completion alternatives is just a guide. For example, when C-x C-f reads the name of a file to visit, any file name is allowed, in case you want to create a file. In permissive completion, <RET> takes the text in the minibuffer exactly as given, without completing it. 
The completion commands display a list of all possible completions in a window whenever there is more than one possibility for the very next character. Also, typing ? explicitly requests such a list. If the list of completions is long, you can scroll it with C-M-v (see Other Window). 



--------------------------------------------------------------------------------
Previous: Strict Completion, Up: Completion 
9.3.4 Completion Options
When completion is done on file names, certain file names are usually ignored. The variable completion-ignored-extensions contains a list of strings; a file whose name ends in any of those strings is ignored as a possible completion. The standard value of this variable has several elements including ".o", ".elc", ".dvi" and "~". The effect is that, for example,  foo  can complete to  foo.c  even though  foo.o  exists as well. However, if all the possible completions end in  ignored  strings, then they are not ignored. Ignored extensions do not apply to lists of completions those always mention all possible completions. 

If an element of the list in completion-ignored-extensions ends in a slash /, it indicates a subdirectory that should be ignored when completing file names. (Elements of completion-ignored-extensions which do not end in a slash are never considered when a completion candidate is a directory; thus, completion returns directories whose names end in .elc even though there's an element ".elc" in the list.) 

Normally, a completion command that cannot determine even one additional character automatically displays a list of all possible completions. If the variable completion-auto-help is set to nil, this automatic display is disabled, so you must type ? to display the list of completions. 

Partial Completion mode implements a more powerful kind of completion that can complete multiple words in parallel. For example, it can complete the command name abbreviation p-b into print-buffer, because no other command starts with two words whose initials are  p  and  b . 

Partial completion of directories in file names uses  *  to indicate the places for completion; thus, /u*/b*/f* might complete to /usr/bin/foo. 

To enable this mode, use the command M-x partial-completion-mode, or customize the variable partial-completion-mode. This binds the partial completion commands to <TAB>, <SPC>, <RET>, and ?. The usual completion commands are available on M-<TAB> (or C-M-i), M-<SPC>, M-<RET> and M-?. 

Another feature of Partial Completion mode is to extend find-file so that  <include>  stands for the file named include in some directory in the path PC-include-file-path. If you set PC-disable-includes to non-nil, this feature is disabled. 

Icomplete mode presents a constantly-updated display that tells you what completions are available for the text you've entered so far. The command to enable or disable this minor mode is M-x icomplete-mode. 



--------------------------------------------------------------------------------
Next: Repetition, Previous: Completion, Up: Minibuffer 
9.4 Minibuffer History
Every argument that you enter with the minibuffer is saved on a minibuffer history list so that you can use it again later in another argument. Special commands load the text of an earlier argument in the minibuffer. They discard the old minibuffer contents, so you can think of them as moving through the history of previous arguments. 

<UP>
M-p
Move to the next earlier argument string saved in the minibuffer history (previous-history-element). 

<DOWN>
M-n
Move to the next later argument string saved in the minibuffer history (next-history-element). 

M-r regexp <RET>
Move to an earlier saved argument in the minibuffer history that has a match for regexp (previous-matching-history-element). 

M-s regexp <RET>
Move to a later saved argument in the minibuffer history that has a match for regexp (next-matching-history-element). 
The simplest way to reuse the saved arguments in the history list is to move through the history list one element at a time. While in the minibuffer, use M-p or up-arrow (previous-history-element) to  move to  the next earlier minibuffer input, and use M-n or down-arrow (next-history-element) to  move to  the next later input. These commands don't move the cursor, they bring different saved strings into the minibuffer. But you can think of them as  moving  through the history list. 

The previous input that you fetch from the history entirely replaces the contents of the minibuffer. To use it as the argument, exit the minibuffer as usual with <RET>. You can also edit the text before you reuse it; this does not change the history element that you  moved  to, but your new argument does go at the end of the history list in its own right. 

For many minibuffer arguments there is a  default  value. In some cases, the minibuffer history commands know the default value. Then you can insert the default value into the minibuffer as text by using M-n to move  into the future  in the history. Eventually we hope to make this feature available whenever the minibuffer has a default value. 

There are also commands to search forward or backward through the history; they search for history elements that match a regular expression that you specify with the minibuffer. M-r (previous-matching-history-element) searches older elements in the history, while M-s (next-matching-history-element) searches newer elements. By special dispensation, these commands can use the minibuffer to read their arguments even though you are already in the minibuffer when you issue them. As with incremental searching, an upper-case letter in the regular expression makes the search case-sensitive (see Search Case). 

All uses of the minibuffer record your input on a history list, but there are separate history lists for different kinds of arguments. For example, there is a list for file names, used by all the commands that read file names. (As a special feature, this history list records the absolute file name, no more and no less, even if that is not how you entered the file name.) 

There are several other very specific history lists, including one for command names read by M-x, one for buffer names, one for arguments of commands like query-replace, and one for compilation commands read by compile. Finally, there is one  miscellaneous  history list that most minibuffer arguments use. 

The variable history-length specifies the maximum length of a minibuffer history list; once a list gets that long, the oldest element is deleted each time an element is added. If the value of history-length is t, though, there is no maximum length and elements are never deleted. 

The variable history-delete-duplicates specifies whether to delete duplicates in history. If the value of history-delete-duplicates is t, that means when adding a new history element, all previous identical elements are deleted. 



--------------------------------------------------------------------------------
Previous: Minibuffer History, Up: Minibuffer 
9.5 Repeating Minibuffer Commands
Every command that uses the minibuffer at least once is recorded on a special history list, together with the values of its arguments, so that you can repeat the entire command. In particular, every use of M-x is recorded there, since M-x uses the minibuffer to read the command name. 


C-x <ESC> <ESC>
Re-execute a recent minibuffer command (repeat-complex-command). 

M-x list-command-history
Display the entire command history, showing all the commands C-x <ESC> <ESC> can repeat, most recent first. 
C-x <ESC> <ESC> is used to re-execute a recent minibuffer-using command. With no argument, it repeats the last such command. A numeric argument specifies which command to repeat; one means the last one, and larger numbers specify earlier ones. 

C-x <ESC> <ESC> works by turning the previous command into a Lisp expression and then entering a minibuffer initialized with the text for that expression. If you type just <RET>, the command is repeated as before. You can also change the command by editing the Lisp expression. Whatever expression you finally submit is what will be executed. The repeated command is added to the front of the command history unless it is identical to the most recently executed command already there. 

Even if you don't understand Lisp syntax, it will probably be obvious which command is displayed for repetition. If you do not change the text, it will repeat exactly as before. 

Once inside the minibuffer for C-x <ESC> <ESC>, you can use the minibuffer history commands (M-p, M-n, M-r, M-s; see Minibuffer History) to move through the history list of saved entire commands. After finding the desired previous command, you can edit its expression as usual and then resubmit it by typing <RET> as usual. 

Incremental search does not, strictly speaking, use the minibuffer, but it does something similar. Although it behaves like a complex command, it normally does not appear in the history list for C-x <ESC> <ESC>. You can make it appear in the history by setting isearch-resume-in-command-history to a non-nil value. See Incremental Search. 

The list of previous minibuffer-using commands is stored as a Lisp list in the variable command-history. Each element is a Lisp expression which describes one command and its arguments. Lisp programs can re-execute a command by calling eval with the command-history element. 



--------------------------------------------------------------------------------
Next: Help, Previous: Minibuffer, Up: Top 
10 Running Commands by Name
Every Emacs command has a name that you can use to run it. Commands that are used often, or that must be quick to type, are also bound to keys short sequences of characters for convenient use. You can run them by name if you don't remember the keys. Other Emacs commands that do not need to be quick are not bound to keys; the only way to run them is by name. See Key Bindings, for the description of how to bind commands to keys. 

By convention, a command name consists of one or more words, separated by hyphens; for example, auto-fill-mode or manual-entry. The use of English words makes the command name easier to remember than a key made up of obscure characters, even though it is more characters to type. 

The way to run a command by name is to start with M-x, type the command name, and finish it with <RET>. M-x uses the minibuffer to read the command name. <RET> exits the minibuffer and runs the command. The string  M-x  appears at the beginning of the minibuffer as a prompt to remind you to enter the name of a command to be run. See Minibuffer, for full information on the features of the minibuffer. 

You can use completion to enter the command name. For example, you can invoke the command forward-char by name by typing either 

     M-x forward-char <RET>

or 

     M-x forw <TAB> c <RET>

Note that forward-char is the same command that you invoke with the key C-f. You can run any Emacs command by name using M-x, whether or not any keys are bound to it. 

If you type C-g while the command name is being read, you cancel the M-x command and get out of the minibuffer, ending up at command level. 

To pass a numeric argument to the command you are invoking with M-x, specify the numeric argument before the M-x. M-x passes the argument along to the command it runs. The argument value appears in the prompt while the command name is being read. 

If the command you type has a key binding of its own, Emacs mentions this in the echo area. For example, if you type M-x forward-word, the message says that you can run the same command more easily by typing M-f. You can turn off these messages by setting suggest-key-bindings to nil. 

Normally, when describing in this manual a command that is run by name, we omit the <RET> that is needed to terminate the name. Thus we might speak of M-x auto-fill-mode rather than M-x auto-fill-mode <RET>. We mention the <RET> only when there is a need to emphasize its presence, such as when we show the command together with following arguments. 

M-x works by running the command execute-extended-command, which is responsible for reading the name of another command and invoking it. 



--------------------------------------------------------------------------------
Next: Mark, Previous: M-x, Up: Top 
11 Help
Emacs provides extensive help features accessible through a single character, C-h. C-h is a prefix key that is used for commands that display documentation. The characters that you can type after C-h are called help options. One help option is C-h; that is how you ask for help about using C-h. To cancel, type C-g. The function key <F1> is equivalent to C-h. 

C-h C-h (help-for-help) displays a list of the possible help options, each with a brief description. Before you type a help option, you can use <SPC> or <DEL> to scroll through the list. 

C-h or <F1> means  help  in various other contexts as well. After a prefix key, it displays a list of the alternatives that can follow the prefix key. (A few prefix keys don't support C-h, because they define other meanings for it, but they all support <F1>.) 

Most help buffers use a special major mode, Help mode, which lets you scroll conveniently with <SPC> and <DEL>. It also offers hyperlinks to URLs and further help regarding cross-referenced names, Info nodes, customization buffers and the like. See Help Mode. 

If you are looking for a certain feature, but don't know where exactly it is documented, and aren't sure of the name of a related command or variable, we recommend trying these methods. Usually it is best to start with an apropos command, then try searching the manual index, then finally look in the FAQ and the package keywords. 

C-h a topics <RET>
This searches for commands whose names match topics, which should be a keyword, a list of keywords, or a regular expression (see Regexps). This command displays all the matches in a new buffer. See Apropos. 

C-h i d m emacs <RET> i topic <RET>
This looks up topic in the indices of the Emacs on-line manual. If there are several matches, Emacs displays the first one. You can then press , to move to other matches, until you find what you are looking for. 

C-h i d m emacs <RET> s topic <RET>
Similar, but searches for topic (which can be a regular expression) in the text of the manual rather than in its indices. 

C-h C-f
This brings up the Emacs FAQ. You can use the Info commands to browse it. 

C-h p
Finally, you can try looking up a suitable package using keywords pertinent to the feature you need. See Library Keywords. 
To find the documentation of a key sequence or a menu item, type C-h K and then type that key sequence or select the menu item. This looks up the description of the command invoked by the key or the menu in the appropriate manual (not necessarily the Emacs manual). Likewise, use C-h F for reading documentation of a command. 

Help Summary: Brief list of all Help commands. 
Key Help: Asking what a key does in Emacs. 
Name Help: Asking about a command, variable or function name. 
Apropos: Asking what pertains to a given topic. 
Library Keywords: Finding Lisp libraries by keywords (topics). 
Language Help: Help relating to international language support. 
Help Mode: Special features of Help mode and Help buffers. 
Misc Help: Other help commands. 
Help Files: Commands to display pre-written help files. 
Help Echo: Help on active text and tooltips (`balloon help') 


--------------------------------------------------------------------------------
Next: Key Help, Up: Help 
11.1 Help Summary
Here is a summary of the Emacs interactive help commands. See Help Files, for other help commands that just display a pre-written file of information. 

C-h a topics <RET>
Display a list of commands whose names match topics (apropos-command; see Apropos). 

C-h b
Display a table of all key bindings in effect now, in this order: minor mode bindings, major mode bindings, and global bindings (describe-bindings). 

C-h c key
Show the name of the command that key runs (describe-key-briefly). Here c stands for  character.  For more extensive information on key, use C-h k. 

C-h d topics <RET>
Display a list of commands and variables whose documentation matches topics (apropos-documentation). 

C-h e
Display the *Messages* buffer (view-echo-area-messages). 

C-h f function <RET>
Display documentation on the Lisp function named function (describe-function). Since commands are Lisp functions, a command name may be used. 

C-h h
Display the HELLO file, which shows examples of various character sets. 

C-h i
Run Info, the program for browsing documentation files (info). The complete Emacs manual is available on-line in Info. 

C-h k key
Display the name and documentation of the command that key runs (describe-key). 

C-h l
Display a description of the last 100 characters you typed (view-lossage). 

C-h m
Display documentation of the current major mode (describe-mode). 

C-h p
Find packages by topic keyword (finder-by-keyword). 

C-h s
Display the current contents of the syntax table, plus an explanation of what they mean (describe-syntax). See Syntax. 

C-h t
Enter the Emacs interactive tutorial (help-with-tutorial). 

C-h v var <RET>
Display the documentation of the Lisp variable var (describe-variable). 

C-h w command <RET>
Show which keys run the command named command (where-is). 

C-h C coding <RET>
Describe coding system coding (describe-coding-system). 

C-h C <RET>
Describe the coding systems currently in use. 

C-h I method <RET>
Describe an input method (describe-input-method). 

C-h L language-env <RET>
Display information on the character sets, coding systems, and input methods used for language environment language-env (describe-language-environment). 

C-h F function <RET>
Enter Info and go to the node documenting the Emacs function function (Info-goto-emacs-command-node). 

C-h K key
Enter Info and go to the node where the key sequence key is documented (Info-goto-emacs-key-command-node). 

C-h S symbol <RET>
Display the Info documentation on symbol symbol according to the programming language you are editing (info-lookup-symbol). 

C-h .
Display a help message associated with special text areas, such as links in  *Help*  buffers (display-local-help). 


--------------------------------------------------------------------------------
Next: Name Help, Previous: Help Summary, Up: Help 
11.2 Documentation for a Key
The most basic C-h options are C-h c (describe-key-briefly) and C-h k (describe-key). C-h c key displays in the echo area the name of the command that key is bound to. For example, C-h c C-f displays  forward-char . Since command names are chosen to describe what the commands do, this is a good way to get a very brief description of what key does. 

C-h k key is similar but gives more information: it displays the documentation string of the command as well as its name. This is too big for the echo area, so a window is used for the display. 

C-h c and C-h k work for any sort of key sequences, including function keys and mouse events. 

C-h w command <RET> tells you what keys are bound to command. It displays a list of the keys in the echo area. If it says the command is not on any key, you must use M-x to run it. C-h w runs the command where-is. 



--------------------------------------------------------------------------------
Next: Apropos, Previous: Key Help, Up: Help 
11.3 Help by Command or Variable Name
C-h f (describe-function) reads the name of a Lisp function using the minibuffer, then displays that function's documentation string in a window. Since commands are Lisp functions, you can use this to get the documentation of a command that you know by name. For example, 

     C-h f auto-fill-mode <RET>

displays the documentation of auto-fill-mode. This is the only way to get the documentation of a command that is not bound to any key (one which you would normally run using M-x). 

C-h f is also useful for Lisp functions that you are planning to use in a Lisp program. For example, if you have just written the expression (make-vector len) and want to check that you are using make-vector properly, type C-h f make-vector <RET>. Because C-h f allows all function names, not just command names, you may find that some of your favorite completion abbreviations that work in M-x don't work in C-h f. An abbreviation may be unique among command names, yet fail to be unique when other function names are allowed. 

The default function name for C-h f to describe, if you type just <RET>, is the name of the function called by the innermost Lisp expression in the buffer around point, provided that is a valid, defined Lisp function name. For example, if point is located following the text  (make-vector (car x) , the innermost list containing point is the one that starts with  (make-vector , so the default is to describe the function make-vector. 

C-h f is often useful just to verify that you have the right spelling for the function name. If C-h f mentions a name from the buffer as the default, that name must be defined as a Lisp function. If that is all you want to know, just type C-g to cancel the C-h f command, then go on editing. 

C-h v (describe-variable) is like C-h f but describes Lisp variables instead of Lisp functions. Its default is the Lisp symbol around or before point, but only if that is the name of a known Lisp variable. See Variables. 

Help buffers describing Emacs variables and functions normally have hyperlinks to the definition, if you have the source files installed. (See Hyperlinking.) If you know Lisp (or C), this provides the ultimate documentation. If you don't know Lisp, you should learn it. If you are just using Emacs, treating Emacs as an object (file), then you don't really love it. For true intimacy with your editor, you need to read the source code. 



--------------------------------------------------------------------------------
Next: Library Keywords, Previous: Name Help, Up: Help 
11.4 Apropos
A more sophisticated sort of question to ask is,  What are the commands for working with files?  The apropos commands ask such questions they look for things whose names match an apropos pattern, which means either a word, a list of words, or a regular expression. Each apropos command displays a list of matching items in a special buffer. 

C-h a pattern <RET>
Search for commands whose names match pattern. 

M-x apropos <RET> pattern <RET>
Similar, but it searches for noninteractive functions and for variables, as well as commands. 

M-x apropos-variable <RET> pattern <RET>
Similar, but it searches for variables only. 

M-x apropos-value <RET> pattern <RET>
Similar, but it searches for variables based on their values, or functions based on their definitions. 

C-h d pattern <RET>
Search the documentation strings (the built-in short descriptions) of all variables and functions (not their names) for a match for pattern. 
To find the commands that work on files, type C-h a file <RET>. This displays a list of all command names that contain  file , including copy-file, find-file, and so on. With each command name appears a brief description of how to use the command, and what keys you can currently invoke it with. For example, it would say that you can invoke find-file by typing C-x C-f. The a in C-h a stands for  Apropos ; C-h a runs the command apropos-command. This command normally checks only commands (interactive functions); if you specify a prefix argument, it checks noninteractive functions as well. 

If you want more information about a function definition, variable or symbol property listed in the Apropos buffer, you can click on it with Mouse-1 or Mouse-2, or move there and type <RET>. 

C-h a with a single word can find too many matches. Don't just give up; you can give Apropos a list of words to search for. When you specify more than one word in the apropos pattern, a name must contain at least two of the words in order to match. Thus, if you are looking for commands to kill a chunk of text before point, you could try C-h a kill back backward behind before <RET>. 

For even greater flexibility, you can specify a regular expression (see Regexps). An apropos pattern is interpreted as a regular expression if it contains any of the regular expression special characters,  ^$*+?.\[ . 

Here is a set of arguments to give to C-h a that covers many classes of Emacs commands, since there are strong conventions for naming the standard Emacs commands. By giving you a feel for the naming conventions, this set should also serve to aid you in developing a technique for picking Apropos keywords. 

char, line, word, sentence, paragraph, region, page, sexp, list, defun, rect, buffer, frame, window, face, file, dir, register, mode, beginning, end, forward, backward, next, previous, up, down, search, goto, kill, delete, mark, insert, yank, fill, indent, case, change, set, what, list, find, view, describe, default. 
To list all Lisp symbols that contain a match for an Apropos pattern, not just the ones that are defined as commands, use the command M-x apropos instead of C-h a. This command does not check key bindings by default; specify a numeric argument if you want it to check them. 

To list user-customizable variables that match an apropos pattern, use the command M-x apropos-variable. If you specify a prefix argument, it checks all variables. 

The apropos-documentation command is like apropos except that it searches documentation strings instead of symbol names for matches for the specified Apropos pattern. 

The apropos-value command is like apropos except that it searches variables' values for matches for the pattern. With a prefix argument, it also checks symbols' function definitions and property lists. 

If the variable apropos-do-all is non-nil, the commands above all behave as if they had been given a prefix argument. 

By default, Apropos lists the search results in alphabetical order. If the variable apropos-sort-by-scores is non-nil, Apropos tries to guess the relevance of each result, and displays the most relevant ones first. 

By default, Apropos lists the search results for apropos-documentation in order of relevance of the match. If the variable apropos-documentation-sort-by-scores is nil, Apropos lists the symbols found in alphabetical order. 



--------------------------------------------------------------------------------
Next: Language Help, Previous: Apropos, Up: Help 
11.5 Keyword Search for Lisp Libraries
The C-h p command lets you search the standard Emacs Lisp libraries by topic keywords. Here is a partial list of keywords you can use: 

abbrev abbreviation handling, typing shortcuts, macros. 
 
bib code related to the bib bibliography processor. 
 
c support for the C language and related languages. 
 
calendar calendar and time management support. 
 
comm communications, networking, remote access to files. 
 
convenience convenience features for faster editing. 
 
data support for editing files of data. 
 
docs support for Emacs documentation. 
 
emulations emulations of other editors. 
 
extensions Emacs Lisp language extensions. 
 
faces support for multiple fonts. 
 
files support for editing and manipulating files. 
 
frames support for Emacs frames and window systems. 
 
games games, jokes and amusements. 
 
hardware support for interfacing with exotic hardware. 
 
help support for on-line help systems. 
 
hypermedia support for links between text or other media types. 
 
i18n internationalization and alternate character-set support. 
 
internal code for Emacs internals, build process, defaults. 
 
languages specialized modes for editing programming languages. 
 
lisp Lisp support, including Emacs Lisp. 
 
local code local to your site. 
 
maint maintenance aids for the Emacs development group. 
 
mail modes for electronic-mail handling. 
 
matching various sorts of searching and matching. 
 
mouse mouse support. 
 
multimedia images and sound support. 
 
news support for netnews reading and posting. 
 
oop support for object-oriented programming. 
 
outlines support for hierarchical outlining. 
 
processes process, subshell, compilation, and job control support. 
 
terminals support for terminal types. 
 
tex supporting code for the TeX formatter. 
 
tools programming tools. 
 
unix front-ends/assistants for, or emulators of, UNIX-like features. 
 
wp word processing. 
 




--------------------------------------------------------------------------------
Next: Help Mode, Previous: Library Keywords, Up: Help 
11.6 Help for International Language Support
You can use the command C-h L (describe-language-environment) to find out information about the support for a specific language environment. See Language Environments. This tells you which languages this language environment is useful for, and lists the character sets, coding systems, and input methods that go with it. It also shows some sample text to illustrate scripts. 

The command C-h h (view-hello-file) displays the file etc/HELLO, which shows how to say  hello  in many languages. 

The command C-h I (describe-input-method) describes information about input methods either a specified input method, or by default the input method in use. See Input Methods. 

The command C-h C (describe-coding-system) describes information about coding systems either a specified coding system, or the ones currently in use. See Coding Systems. 



--------------------------------------------------------------------------------
Next: Misc Help, Previous: Language Help, Up: Help 
11.7 Help Mode Commands
Help buffers provide the same commands as View mode (see Misc File Ops), plus a few special commands of their own. 

<SPC>
Scroll forward. 

<DEL>
<BS>
Scroll backward. On some keyboards, this key is known as <BS> or <backspace>. 

<RET>
Follow a cross reference at point. 

<TAB>
Move point forward to the next cross reference. 

S-<TAB>
Move point back to the previous cross reference. 

Mouse-1
Mouse-2
Follow a cross reference that you click on. 
When a function name (see Running Commands by Name) or variable name (see Variables) appears in the documentation, it normally appears inside paired single-quotes. You can click on the name with Mouse-1 or Mouse-2, or move point there and type <RET>, to view the documentation of that command or variable. Use C-c C-b to retrace your steps. 

You can follow cross references to URLs (web pages) as well. When you follow a cross reference that is a URL, the browse-url command is used to view the web page in a browser of your choosing. See Browse-URL. 

There are convenient commands for moving point to cross references in the help text. <TAB> (help-next-ref) moves point down to the next cross reference. Use S-<TAB> to move point up to the previous cross reference (help-previous-ref). 



--------------------------------------------------------------------------------
Next: Help Files, Previous: Help Mode, Up: Help 
11.8 Other Help Commands
C-h i (info) runs the Info program, which is used for browsing through structured documentation files. The entire Emacs manual is available within Info. Eventually all the documentation of the GNU system will be available. Type h after entering Info to run a tutorial on using Info. 

With a numeric argument, C-h i selects an Info buffer with the number appended to the default  *info*  buffer name (e.g.  *info*<2> ). This is useful if you want to browse multiple Info manuals simultaneously. If you specify just C-u as the prefix argument, C-h i prompts for the name of a documentation file. This way, you can browse a file which doesn't have an entry in the top-level Info menu. It is also handy when you need to get to the documentation quickly, and you know the exact name of the file. 

There are two special help commands for accessing Emacs documentation through Info. C-h F function <RET> enters Info and goes straight to the documentation of the Emacs function function. C-h K key enters Info and goes straight to the documentation of the key key. These two keys run the commands Info-goto-emacs-command-node and Info-goto-emacs-key-command-node. You can use C-h K to find the documentation of a menu item: just select that menu item when C-h K prompts for a key. 

C-h F and C-h K know about commands and keys described in manuals other than the Emacs manual. Thus, they make it easier to find the documentation of commands and keys when you are not sure which manual describes them, like when using some specialized mode. 

When editing a program, if you have an Info version of the manual for the programming language, you can use the command C-h S (info-lookup-symbol) to refer to the manual documentation for a symbol (keyword, function or variable). The details of how this command works depend on the major mode. 

If something surprising happens, and you are not sure what commands you typed, use C-h l (view-lossage). C-h l displays the last 100 command characters you typed in. If you see commands that you don't know, you can use C-h c to find out what they do. 

To review messages that recently appeared in the echo area, use C-h e (view-echo-area-messages). This displays the buffer *Messages*, where those messages are kept. 

Emacs has numerous major modes, each of which redefines a few keys and makes a few other changes in how editing works. C-h m (describe-mode) displays documentation on the current major mode, which normally describes all the commands that are changed in this mode. 

C-h b (describe-bindings) and C-h s (describe-syntax) present other information about the current Emacs mode. C-h b displays a list of all the key bindings now in effect, showing the local bindings defined by the current minor modes first, then the local bindings defined by the current major mode, and finally the global bindings (see Key Bindings). C-h s displays the contents of the syntax table, with explanations of each character's syntax (see Syntax). 

You can get a similar list for a particular prefix key by typing C-h after the prefix key. (There are a few prefix keys for which this does not work those that provide their own bindings for C-h. One of these is <ESC>, because <ESC> C-h is actually C-M-h, which marks a defun.) 



--------------------------------------------------------------------------------
Next: Help Echo, Previous: Misc Help, Up: Help 
11.9 Help Files
The Emacs help commands described above display the state of data bases within Emacs. Emacs has a few other help commands that display pre-written help files. These commands all have the form C-h C-char; that is, C-h followed by a control character. 

The other C-h options display various files containing useful information. 

C-h C-c
Displays the Emacs copying conditions (describe-copying). These are the rules under which you can copy and redistribute Emacs. 

C-h C-d
Displays information on how to download or order the latest version of Emacs and other GNU software (describe-distribution). 

C-h C-e
Displays the list of known Emacs problems, sometimes with suggested workarounds (view-emacs-problems). 

C-h C-f
Displays the Emacs frequently-answered-questions list (view-emacs-FAQ). 

C-h C-n
Displays the Emacs  news  file, which lists new Emacs features, most recent first (view-emacs-news). 

C-h C-p
Displays general information about the GNU Project (describe-project). 

C-h C-t
Displays the Emacs to-do list (view-todo). 

C-h C-w
Displays the full details on the complete absence of warranty for GNU Emacs (describe-no-warranty). 


--------------------------------------------------------------------------------
Previous: Help Files, Up: Help 
11.10 Help on Active Text and Tooltips
When a region of text is  active,  so that you can select it with the mouse or a key like RET, it often has associated help text. Areas of the mode line are examples. On most window systems, the help text is displayed as a  tooltip  (sometimes known as  balloon help ), when you move the mouse over the active text. See Tooltips. On some systems, it is shown in the echo area. On text-only terminals, Emacs may not be able to follow the mouse and hence will not show the help text on mouse-over. 

You can also access text region help info using the keyboard. The command C-h . (display-local-help) displays any help text associated with the text at point, using the echo area. If you want help text to be displayed automatically whenever it is available at point, set the variable help-at-pt-display-when-idle to t. 



--------------------------------------------------------------------------------
Next: Killing, Previous: Help, Up: Top 
12 The Mark and the Region
Many Emacs commands operate on an arbitrary contiguous part of the current buffer. To specify the text for such a command to operate on, you set the mark at one end of it, and move point to the other end. The text between point and the mark is called the region. Emacs highlights the region whenever there is one, if you enable Transient Mark mode (see Transient Mark). 

Certain Emacs commands set the mark; other editing commands do not affect it, so the mark remains where you set it last. Each Emacs buffer has its own mark, and setting the mark in one buffer has no effect on other buffers' marks. When you return to a buffer that was current earlier, its mark is at the same place as before. 

The ends of the region are always point and the mark. It doesn't matter which of them was put in its current place first, or which one comes earlier in the text the region starts from point or the mark (whichever comes first), and ends at point or the mark (whichever comes last). Every time you move point, or set the mark in a new place, the region changes. 

Many commands that insert text, such as C-y (yank) and M-x insert-buffer, position point and the mark at opposite ends of the inserted text, so that the region consists of the text just inserted. 

Aside from delimiting the region, the mark is also useful for remembering a spot that you may want to go back to. To make this feature more useful, each buffer remembers 16 previous locations of the mark in the mark ring. 

Setting Mark: Commands to set the mark. 
Transient Mark: How to make Emacs highlight the region-- when there is one. 
Momentary Mark: Enabling Transient Mark mode momentarily. 
Using Region: Summary of ways to operate on contents of the region. 
Marking Objects: Commands to put region around textual units. 
Mark Ring: Previous mark positions saved so you can go back there. 
Global Mark Ring: Previous mark positions in various buffers. 


--------------------------------------------------------------------------------
Next: Transient Mark, Up: Mark 
12.1 Setting the Mark
Here are some commands for setting the mark: 

C-<SPC>
Set the mark where point is (set-mark-command). 

C-@
The same. 

C-x C-x
Interchange mark and point (exchange-point-and-mark). 

Drag-Mouse-1
Set point and the mark around the text you drag across. 

Mouse-3
Set the mark where point is, then move point to where you click (mouse-save-then-kill). 
For example, suppose you wish to convert part of the buffer to upper case, using the C-x C-u (upcase-region) command, which operates on the text in the region. You can first go to the beginning of the text to be capitalized, type C-<SPC> to put the mark there, move to the end, and then type C-x C-u. Or, you can set the mark at the end of the text, move to the beginning, and then type C-x C-u. 

The most common way to set the mark is with the C-<SPC> command (set-mark-command). This sets the mark where point is. Then you can move point away, leaving the mark behind. 

There are two ways to set the mark with the mouse. You can drag mouse button one across a range of text; that puts point where you release the mouse button, and sets the mark at the other end of that range. Or you can click mouse button three, which sets the mark at point (like C-<SPC>) and then moves point where you clicked (like Mouse-1). Both of these methods copy the region into the kill ring in addition to setting the mark; that gives behavior consistent with other window-driven applications, but if you don't want to modify the kill ring, you must use keyboard commands to set the mark. See Mouse Commands. 

When Emacs was developed, terminals had only one cursor, so Emacs does not show where the mark is located you have to remember. If you enable Transient Mark mode (see below), then the region is highlighted when it is active; you can tell mark is at the other end of the highlighted region. But this only applies when the mark is active. 

The usual solution to this problem is to set the mark and then use it soon, before you forget where it is. Alternatively, you can see where the mark is with the command C-x C-x (exchange-point-and-mark) which puts the mark where point was and point where the mark was. The extent of the region is unchanged, but the cursor and point are now at the previous position of the mark. In Transient Mark mode, this command also reactivates the mark. 

C-x C-x is also useful when you are satisfied with the position of point but want to move the other end of the region (where the mark is); do C-x C-x to put point at that end of the region, and then move it. Using C-x C-x a second time, if necessary, puts the mark at the new position with point back at its original position. 

For more facilities that allow you to go to previously set marks, see Mark Ring. 

There is no such character as C-<SPC> in ASCII; when you type <SPC> while holding down <CTRL> on a text terminal, what you get is the character C-@. This key is also bound to set-mark-command so unless you are unlucky enough to have a text terminal where typing C-<SPC> does not produce C-@, you might as well think of this character as C-<SPC>. 



--------------------------------------------------------------------------------
Next: Momentary Mark, Previous: Setting Mark, Up: Mark 
12.2 Transient Mark Mode
On a terminal that supports colors, Emacs has the ability to highlight the current region. But normally it does not. Why not? 

Once you have set the mark in a buffer, there is always a region in that buffer. This is because every command that sets the mark also activates it, and nothing ever deactivates it. Highlighting the region all the time would be a nuisance. So normally Emacs highlights the region only immediately after you have selected one with the mouse. 

If you want region highlighting, you can use Transient Mark mode. This is a more rigid mode of operation in which the region always  lasts  only until you use it; you explicitly must set up a region for each command that uses one. In Transient Mark mode, most of the time there is no region; therefore, highlighting the region when it exists is useful and not annoying. When Transient Mark mode is enabled, Emacs always highlights the region whenever there is a region. 

To enable Transient Mark mode, type M-x transient-mark-mode. This command toggles the mode; you can use the same command to turn the mode off again. 

Here are the details of Transient Mark mode: 

To set the mark, type C-<SPC> (set-mark-command). This makes the mark active and thus begins highlighting of the region. As you move point, you will see the highlighted region grow and shrink. 
The mouse commands for specifying the mark also make it active. So do keyboard commands whose purpose is to specify a region, including M-@, C-M-@, M-h, C-M-h, C-x C-p, and C-x h. 
You can tell that the mark is active because the region is highlighted. 
When the mark is active, you can execute commands that operate on the region, such as killing, indenting, or writing to a file. 
Any change to the buffer, such as inserting or deleting a character, deactivates the mark. This means any subsequent command that operates on a region will get an error and refuse to operate. You can make the region active again by typing C-x C-x. 
If Delete Selection mode is also enabled, some commands delete the region when used while the mark is active. See Graphical Kill. 
Quitting with C-g deactivates the mark. 
Commands like M-> and C-s, that  leave the mark behind  in addition to some other primary purpose, do not activate the new mark. You can activate the new region by executing C-x C-x (exchange-point-and-mark). 
Commands that normally set the mark before moving long distances (like M-< and C-s) do not alter the mark in Transient Mark mode when the mark is active. 
Some commands operate on the region if a region is active. For instance, C-x u in Transient Mark mode operates on the region, when there is a region. (Outside Transient Mark mode, you must type C-u C-x u if you want it to operate on the region.) See Undo. Other commands that act this way are identified in their own documentation. 
The highlighting of the region uses the region face; you can customize the appearance of the highlighted region by changing this face. See Face Customization. 

When multiple windows show the same buffer, they can have different regions, because they can have different values of point (though they all share one common mark position). Ordinarily, only the selected window highlights its region (see Windows). However, if the variable highlight-nonselected-windows is non-nil, then each window highlights its own region (provided that Transient Mark mode is enabled and the mark in the window's buffer is active). 

If the variable mark-even-if-inactive is non-nil in Transient Mark mode, then commands can use the mark and the region even when it is inactive. Region highlighting appears and disappears just as it normally does in Transient Mark mode, but the mark doesn't really go away when the highlighting disappears, so you can still use region commands. 

Transient Mark mode is also sometimes known as  Zmacs mode  because the Zmacs editor on the MIT Lisp Machine handled the mark in a similar way. 



--------------------------------------------------------------------------------
Next: Using Region, Previous: Transient Mark, Up: Mark 
12.3 Using Transient Mark Mode Momentarily
If you don't like Transient Mark mode in general, you might still want to use it once in a while. To do this, type C-<SPC> C-<SPC> or C-u C-x C-x. These commands set or activate the mark, and enable Transient Mark mode only until the mark is deactivated. 

C-<SPC> C-<SPC>
Set the mark at point (like plain C-<SPC>), and enable Transient Mark mode just once until the mark is deactivated. (This is not really a separate command; you are using the C-<SPC> command twice.) 

C-u C-x C-x
Activate the mark without changing it; enable Transient Mark mode just once, until the mark is deactivated. (This is the C-x C-x command, exchange-point-and-mark, with a prefix argument.) 
One of the secondary features of Transient Mark mode is that certain commands operate only on the region, when there is an active region. If you don't use Transient Mark mode, the region once set never becomes inactive, so there is no way for these commands to make such a distinction. Enabling Transient Mark mode momentarily gives you a way to use these commands on the region. 

Momentary use of Transient Mark mode is also a way to highlight the region for the time being. 



--------------------------------------------------------------------------------
Next: Marking Objects, Previous: Momentary Mark, Up: Mark 
12.4 Operating on the Region
Once you have a region and the mark is active, here are some of the ways you can operate on the region: 

Kill it with C-w (see Killing). 
Save it in a register with C-x r s (see Registers). 
Save it in a buffer or a file (see Accumulating Text). 
Convert case with C-x C-l or C-x C-u (see Case). 
Indent it with C-x <TAB> or C-M-\ (see Indentation). 
Fill it as text with M-x fill-region (see Filling). 
Print hardcopy with M-x print-region (see Printing). 
Evaluate it as Lisp code with M-x eval-region (see Lisp Eval). 
Most commands that operate on the text in the region have the word region in their names. 



--------------------------------------------------------------------------------
Next: Mark Ring, Previous: Using Region, Up: Mark 
12.5 Commands to Mark Textual Objects
Here are the commands for placing point and the mark around a textual object such as a word, list, paragraph or page. 

M-@
Set mark after end of next word (mark-word). This command and the following one do not move point. 

C-M-@
Set mark after end of following balanced expression (mark-sexp). 

M-h
Put region around current paragraph (mark-paragraph). 

C-M-h
Put region around current defun (mark-defun). 

C-x h
Put region around the entire buffer (mark-whole-buffer). 

C-x C-p
Put region around current page (mark-page). 
M-@ (mark-word) puts the mark at the end of the next word, while C-M-@ (mark-sexp) puts it at the end of the next balanced expression (see Expressions). These commands handle arguments just like M-f and C-M-f. If you repeat these commands, that extends the region. For example, you can type either C-u 2 M-@ or M-@ M-@ to mark the next two words. This command also extends the region when the mark is active in Transient Mark mode, regardless of the last command. 

Other commands set both point and mark, to delimit an object in the buffer. For example, M-h (mark-paragraph) moves point to the beginning of the paragraph that surrounds or follows point, and puts the mark at the end of that paragraph (see Paragraphs). It prepares the region so you can indent, case-convert, or kill a whole paragraph. With prefix argument, if the argument's value is positive, M-h marks that many paragraphs starting with the one surrounding point. If the prefix argument is ?n, M-h also marks n paragraphs, running back form the one surrounding point. In that last case, point moves forward to the end of that paragraph, and the mark goes at the start of the region. Repeating the M-h command extends the region, just as with M-@ and C-M-@. 

C-M-h (mark-defun) similarly puts point before, and the mark after, the current (or following) major top-level definition, or defun (see Moving by Defuns). Repeating C-M-h also extends the region. 

C-x C-p (mark-page) puts point before the current page, and mark at the end (see Pages). The mark goes after the terminating page delimiter (to include it in the region), while point goes after the preceding page delimiter (to exclude it). A numeric argument specifies a later page (if positive) or an earlier page (if negative) instead of the current page. 

Finally, C-x h (mark-whole-buffer) sets up the entire buffer as the region, by putting point at the beginning and the mark at the end. 

In Transient Mark mode, all of these commands activate the mark. 



--------------------------------------------------------------------------------
Next: Global Mark Ring, Previous: Marking Objects, Up: Mark 
12.6 The Mark Ring
Aside from delimiting the region, the mark is also useful for remembering a spot that you may want to go back to. To make this feature more useful, each buffer remembers 16 previous locations of the mark, in the mark ring. Commands that set the mark also push the old mark onto this ring. To return to a marked location, use C-u C-<SPC> (or C-u C-@); this is the command set-mark-command given a numeric argument. It moves point to where the mark was, and restores the mark from the ring of former marks. 

If you set set-mark-command-repeat-pop to non-nil, then when you repeat the character C-<SPC> after typing C-u C-<SPC>, each repetition moves point to a previous mark position from the ring. The mark positions you move through in this way are not lost; they go to the end of the ring. 

Each buffer has its own mark ring. All editing commands use the current buffer's mark ring. In particular, C-u C-<SPC> always stays in the same buffer. 

Many commands that can move long distances, such as M-< (beginning-of-buffer), start by setting the mark and saving the old mark on the mark ring. This is to make it easier for you to move back later. Searches set the mark if they move point. However, in Transient Mark mode, these commands do not set the mark when the mark is already active. You can tell when a command sets the mark because it displays  Mark set  in the echo area. 

If you want to move back to the same place over and over, the mark ring may not be convenient enough. If so, you can record the position in a register for later retrieval (see Saving Positions in Registers). 

The variable mark-ring-max specifies the maximum number of entries to keep in the mark ring. If that many entries exist and another one is pushed, the earliest one in the list is discarded. Repeating C-u C-<SPC> cycles through the positions currently in the ring. 

The variable mark-ring holds the mark ring itself, as a list of marker objects, with the most recent first. This variable is local in every buffer. 



--------------------------------------------------------------------------------
Previous: Mark Ring, Up: Mark 
12.7 The Global Mark Ring
In addition to the ordinary mark ring that belongs to each buffer, Emacs has a single global mark ring. It records a sequence of buffers in which you have recently set the mark, so you can go back to those buffers. 

Setting the mark always makes an entry on the current buffer's mark ring. If you have switched buffers since the previous mark setting, the new mark position makes an entry on the global mark ring also. The result is that the global mark ring records a sequence of buffers that you have been in, and, for each buffer, a place where you set the mark. 

The command C-x C-<SPC> (pop-global-mark) jumps to the buffer and position of the latest entry in the global ring. It also rotates the ring, so that successive uses of C-x C-<SPC> take you to earlier and earlier buffers. 



--------------------------------------------------------------------------------
Next: Yanking, Previous: Mark, Up: Top 
13 Killing and Moving Text
Killing means erasing text and copying it into the kill ring, from which you can bring it back into the buffer by yanking it. (Some systems use the terms  cutting  and  pasting  for these operations.) This is the most common way of moving or copying text within Emacs. Killing and yanking is very safe because Emacs remembers several recent kills, not just the last one. It is versatile, because the many commands for killing syntactic units can also be used for moving those units. But there are other ways of copying text for special purposes. 

Most commands which erase text from the buffer save it in the kill ring. These commands are known as kill commands. The commands that erase text but do not save it in the kill ring are known as delete commands. The C-x u (undo) command (see Undo) can undo both kill and delete commands; the importance of the kill ring is that you can also yank the text in a different place or places. Emacs has only one kill ring for all buffers, so you can kill text in one buffer and yank it in another buffer. 

The delete commands include C-d (delete-char) and <DEL> (delete-backward-char), which delete only one character at a time, and those commands that delete only spaces or newlines. Commands that can erase significant amounts of nontrivial data generally do a kill operation instead. The commands' names and individual descriptions use the words  kill  and  delete  to say which kind of operation they perform. 

You cannot kill read-only text, since such text does not allow any kind of modification. But some users like to use the kill commands to copy read-only text into the kill ring, without actually changing it. Therefore, the kill commands work specially in a read-only buffer: they move over text, and copy it to the kill ring, without actually deleting it from the buffer. Normally, kill commands beep and display an error message when this happens. But if you set the variable kill-read-only-ok to a non-nil value, they just print a message in the echo area to explain why the text has not been erased. 

Deletion: Commands for deleting small amounts of text and blank areas. 
Killing by Lines: How to kill entire lines of text at one time. 
Other Kill Commands: Commands to kill large regions of text and syntactic units such as words and sentences. 
Graphical Kill: The kill ring on graphical terminals: yanking between applications. 


--------------------------------------------------------------------------------
Next: Killing by Lines, Up: Killing 
13.1 Deletion
Deletion means erasing text and not saving it in the kill ring. For the most part, the Emacs commands that delete text are those that erase just one character or only whitespace. 

C-d
<Delete>
Delete next character (delete-char). If your keyboard has a <Delete> function key (usually located in the edit keypad), Emacs binds it to delete-char as well. 

<DEL>
<BS>
Delete previous character (delete-backward-char). Some keyboards refer to this key as a  backspace key  and label it with a left arrow. 

M-\
Delete spaces and tabs around point (delete-horizontal-space). 

M-<SPC>
Delete spaces and tabs around point, leaving one space (just-one-space). 

C-x C-o
Delete blank lines around the current line (delete-blank-lines). 

M-^
Join two lines by deleting the intervening newline, along with any indentation following it (delete-indentation). 
The most basic delete commands are C-d (delete-char) and <DEL> (delete-backward-char). C-d deletes the character after point, the one the cursor is  on top of.  This doesn't move point. <DEL> deletes the character before the cursor, and moves point back. You can delete newlines like any other characters in the buffer; deleting a newline joins two lines. Actually, C-d and <DEL> aren't always delete commands; when given arguments, they kill instead, since they can erase more than one character this way. 

Every keyboard has a large key, labeled <DEL>, <BACKSPACE>, <BS> or <DELETE>, which is a short distance above the <RET> or <ENTER> key and is normally used for erasing what you have typed. Regardless of the actual name on the key, in Emacs it is equivalent to <DEL> or it should be. 

Many keyboards (including standard PC keyboards) have a <BACKSPACE> key a short ways above <RET> or <ENTER>, and a <DELETE> key elsewhere. In that case, the <BACKSPACE> key is <DEL>, and the <DELETE> key is equivalent to C-d or it should be. 

Why do we say  or it should be ? When Emacs starts up using a window system, it determines automatically which key or keys should be equivalent to <DEL>. As a result, <BACKSPACE> and/or <DELETE> keys normally do the right things. But in some unusual cases Emacs gets the wrong information from the system. If these keys don't do what they ought to do, you need to tell Emacs which key to use for <DEL>. See DEL Does Not Delete, for how to do this. 

On most text-only terminals, Emacs cannot tell which keys the keyboard really has, so it follows a uniform plan which may or may not fit your keyboard. The uniform plan is that the ASCII <DEL> character deletes, and the ASCII <BS> (backspace) character asks for help (it is the same as C-h). If this is not right for your keyboard, such as if you find that the key which ought to delete backwards enters Help instead, see DEL Does Not Delete. 

The other delete commands are those which delete only whitespace characters: spaces, tabs and newlines. M-\ (delete-horizontal-space) deletes all the spaces and tab characters before and after point. M-<SPC> (just-one-space) does likewise but leaves a single space after point, regardless of the number of spaces that existed previously (even if there were none before). With a numeric argument n, it leaves n spaces after point. 

C-x C-o (delete-blank-lines) deletes all blank lines after the current line. If the current line is blank, it deletes all blank lines preceding the current line as well (leaving one blank line, the current line). On a solitary blank line, it deletes that line. 

M-^ (delete-indentation) joins the current line and the previous line, by deleting a newline and all surrounding spaces, usually leaving a single space. See M-^. 



--------------------------------------------------------------------------------
Next: Other Kill Commands, Previous: Deletion, Up: Killing 
13.2 Killing by Lines
C-k
Kill rest of line or one or more lines (kill-line). 

C-S-backspace
Kill an entire line at once (kill-whole-line) 
The simplest kill command is C-k. If given at the beginning of a line, it kills all the text on the line, leaving it blank. When used on a blank line, it kills the whole line including its newline. To kill an entire non-blank line, go to the beginning and type C-k twice. 

More generally, C-k kills from point up to the end of the line, unless it is at the end of a line. In that case it kills the newline following point, thus merging the next line into the current one. Spaces and tabs that you can't see at the end of the line are ignored when deciding which case applies, so if point appears to be at the end of the line, you can be sure C-k will kill the newline. 

When C-k is given a positive argument, it kills that many lines and the newlines that follow them (however, text on the current line before point is not killed). With a negative argument ?n, it kills n lines preceding the current line (together with the text on the current line before point). Thus, C-u - 2 C-k at the front of a line kills the two previous lines. 

C-k with an argument of zero kills the text before point on the current line. 

If the variable kill-whole-line is non-nil, C-k at the very beginning of a line kills the entire line including the following newline. This variable is normally nil. 

C-S-backspace (kill-whole-line) will kill a whole line including its newline regardless of the position of point within the line. Note that many character terminals will prevent you from typing the key sequence C-S-backspace. 



--------------------------------------------------------------------------------
Next: Graphical Kill, Previous: Killing by Lines, Up: Killing 
13.3 Other Kill Commands

C-w
Kill region (from point to the mark) (kill-region). 

M-d
Kill word (kill-word). See Words. 

M-<DEL>
Kill word backwards (backward-kill-word). 

C-x <DEL>
Kill back to beginning of sentence (backward-kill-sentence). See Sentences. 

M-k
Kill to end of sentence (kill-sentence). 

C-M-k
Kill the following balanced expression (kill-sexp). See Expressions. 

M-z char
Kill through the next occurrence of char (zap-to-char). 
The most general kill command is C-w (kill-region), which kills everything between point and the mark. With this command, you can kill any contiguous sequence of characters, if you first set the region around them. 

A convenient way of killing is combined with searching: M-z (zap-to-char) reads a character and kills from point up to (and including) the next occurrence of that character in the buffer. A numeric argument acts as a repeat count. A negative argument means to search backward and kill text before point. 

Other syntactic units can be killed: words, with M-<DEL> and M-d (see Words); balanced expressions, with C-M-k (see Expressions); and sentences, with C-x <DEL> and M-k (see Sentences). 



--------------------------------------------------------------------------------
Previous: Other Kill Commands, Up: Killing 
13.4 Killing on Graphical Terminals
On multi-window terminals, the most recent kill done in Emacs is also the primary selection, if it is more recent than any selection you made in another program. This means that the paste commands of other applications with separate windows copy the text that you killed in Emacs. In addition, Emacs yank commands treat other applications' selections as part of the kill ring, so you can yank them into Emacs. 

Many window systems follow the convention that insertion while text is selected deletes the selected text. You can make Emacs behave this way by enabling Delete Selection mode, with M-x delete-selection-mode, or using Custom. Another effect of this mode is that <DEL>, C-d and some other keys, when a selection exists, will kill the whole selection. It also enables Transient Mark mode (see Transient Mark). 



--------------------------------------------------------------------------------
Next: Accumulating Text, Previous: Killing, Up: Top 
14 Yanking
Yanking means reinserting text previously killed. This is what some systems call  pasting.  The usual way to move or copy text is to kill it and then yank it elsewhere one or more times. This is very safe because Emacs remembers many recent kills, not just the last one. 

C-y
Yank last killed text (yank). 

M-y
Replace text just yanked with an earlier batch of killed text (yank-pop). 

M-w
Save region as last killed text without actually killing it (kill-ring-save). Some systems call this  copying . 

C-M-w
Append next kill to last batch of killed text (append-next-kill). 
On window systems, if there is a current selection in some other application, and you selected it more recently than you killed any text in Emacs, C-y copies the selection instead of text killed within Emacs. 

Kill Ring: Where killed text is stored. Basic yanking. 
Appending Kills: Several kills in a row all yank together. 
Earlier Kills: Yanking something killed some time ago. 


--------------------------------------------------------------------------------
Next: Appending Kills, Up: Yanking 
14.1 The Kill Ring
All killed text is recorded in the kill ring, a list of blocks of text that have been killed. There is only one kill ring, shared by all buffers, so you can kill text in one buffer and yank it in another buffer. This is the usual way to move text from one file to another. (See Accumulating Text, for some other ways.) 

The command C-y (yank) reinserts the text of the most recent kill. It leaves the cursor at the end of the text. It sets the mark at the beginning of the text. See Mark. 

C-u C-y leaves the cursor in front of the text, and sets the mark after it. This happens only if the argument is specified with just a C-u, precisely. Any other sort of argument, including C-u and digits, specifies an earlier kill to yank (see Earlier Kills). 

The yank commands discard certain text properties from the text that is yanked, those that might lead to annoying results. For instance, they discard text properties that respond to the mouse or specify key bindings. The variable yank-excluded-properties specifies the properties to discard. Yanking of register contents and rectangles also discard these properties. 

To copy a block of text, you can use M-w (kill-ring-save), which copies the region into the kill ring without removing it from the buffer. This is approximately equivalent to C-w followed by C-x u, except that M-w does not alter the undo history and does not temporarily change the screen. 



--------------------------------------------------------------------------------
Next: Earlier Kills, Previous: Kill Ring, Up: Yanking 
14.2 Appending Kills
Normally, each kill command pushes a new entry onto the kill ring. However, two or more kill commands in a row combine their text into a single entry, so that a single C-y yanks all the text as a unit, just as it was before it was killed. 

Thus, if you want to yank text as a unit, you need not kill all of it with one command; you can keep killing line after line, or word after word, until you have killed it all, and you can still get it all back at once. 

Commands that kill forward from point add onto the end of the previous killed text. Commands that kill backward from point add text onto the beginning. This way, any sequence of mixed forward and backward kill commands puts all the killed text into one entry without rearrangement. Numeric arguments do not break the sequence of appending kills. For example, suppose the buffer contains this text: 

     This is a line -!-of sample text.

with point shown by -!-. If you type M-d M-<DEL> M-d M-<DEL>, killing alternately forward and backward, you end up with  a line of sample  as one entry in the kill ring, and  This is text.  in the buffer. (Note the double space between  is  and  text , which you can clean up with M-<SPC> or M-q.) 

Another way to kill the same text is to move back two words with M-b M-b, then kill all four words forward with C-u M-d. This produces exactly the same results in the buffer and in the kill ring. M-f M-f C-u M-<DEL> kills the same text, all going backward; once again, the result is the same. The text in the kill ring entry always has the same order that it had in the buffer before you killed it. 

If a kill command is separated from the last kill command by other commands (not just numeric arguments), it starts a new entry on the kill ring. But you can force it to append by first typing the command C-M-w (append-next-kill) right before it. The C-M-w tells the following command, if it is a kill command, to append the text it kills to the last killed text, instead of starting a new entry. With C-M-w, you can kill several separated pieces of text and accumulate them to be yanked back in one place. 

A kill command following M-w does not append to the text that M-w copied into the kill ring. 



--------------------------------------------------------------------------------
Previous: Appending Kills, Up: Yanking 
14.3 Yanking Earlier Kills
To recover killed text that is no longer the most recent kill, use the M-y command (yank-pop). It takes the text previously yanked and replaces it with the text from an earlier kill. So, to recover the text of the next-to-the-last kill, first use C-y to yank the last kill, and then use M-y to replace it with the previous kill. M-y is allowed only after a C-y or another M-y. 

You can understand M-y in terms of a  last yank  pointer which points at an entry in the kill ring. Each time you kill, the  last yank  pointer moves to the newly made entry at the front of the ring. C-y yanks the entry which the  last yank  pointer points to. M-y moves the  last yank  pointer to a different entry, and the text in the buffer changes to match. Enough M-y commands can move the pointer to any entry in the ring, so you can get any entry into the buffer. Eventually the pointer reaches the end of the ring; the next M-y loops back around to the first entry again. 

M-y moves the  last yank  pointer around the ring, but it does not change the order of the entries in the ring, which always runs from the most recent kill at the front to the oldest one still remembered. 

M-y can take a numeric argument, which tells it how many entries to advance the  last yank  pointer by. A negative argument moves the pointer toward the front of the ring; from the front of the ring, it moves  around  to the last entry and continues forward from there. 

Once the text you are looking for is brought into the buffer, you can stop doing M-y commands and it will stay there. It's just a copy of the kill ring entry, so editing it in the buffer does not change what's in the ring. As long as no new killing is done, the  last yank  pointer remains at the same place in the kill ring, so repeating C-y will yank another copy of the same previous kill. 

If you know how many M-y commands it would take to find the text you want, you can yank that text in one step using C-y with a numeric argument. C-y with an argument restores the text from the specified kill ring entry, counting back from the most recent as 1. Thus, C-u 2 C-y gets the next-to-the-last block of killed text it is equivalent to C-y M-y. C-y with a numeric argument starts counting from the  last yank  pointer, and sets the  last yank  pointer to the entry that it yanks. 

The length of the kill ring is controlled by the variable kill-ring-max; no more than that many blocks of killed text are saved. 

The actual contents of the kill ring are stored in a variable named kill-ring; you can view the entire contents of the kill ring with the command C-h v kill-ring. 



--------------------------------------------------------------------------------
Next: Rectangles, Previous: Yanking, Up: Top 
15 Accumulating Text
Usually we copy or move text by killing it and yanking it, but there are other convenient methods for copying one block of text in many places, or for copying many scattered blocks of text into one place. To copy one block to many places, store it in a register (see Registers). Here we describe the commands to accumulate scattered pieces of text into a buffer or into a file. 

M-x append-to-buffer
Append region to the contents of a specified buffer. 

M-x prepend-to-buffer
Prepend region to the contents of a specified buffer. 

M-x copy-to-buffer
Copy region into a specified buffer, deleting that buffer's old contents. 

M-x insert-buffer
Insert the contents of a specified buffer into current buffer at point. 

M-x append-to-file
Append region to the contents of a specified file, at the end. 
To accumulate text into a buffer, use M-x append-to-buffer. This reads a buffer name, then inserts a copy of the region into the buffer specified. If you specify a nonexistent buffer, append-to-buffer creates the buffer. The text is inserted wherever point is in that buffer. If you have been using the buffer for editing, the copied text goes into the middle of the text of the buffer, starting from wherever point happens to be at that moment. 

Point in that buffer is left at the end of the copied text, so successive uses of append-to-buffer accumulate the text in the specified buffer in the same order as they were copied. Strictly speaking, append-to-buffer does not always append to the text already in the buffer it appends only if point in that buffer is at the end. However, if append-to-buffer is the only command you use to alter a buffer, then point is always at the end. 

M-x prepend-to-buffer is just like append-to-buffer except that point in the other buffer is left before the copied text, so successive prependings add text in reverse order. M-x copy-to-buffer is similar, except that any existing text in the other buffer is deleted, so the buffer is left containing just the text newly copied into it. 

To retrieve the accumulated text from another buffer, use the command M-x insert-buffer; this too takes buffername as an argument. It inserts a copy of the whole text in buffer buffername into the current buffer at point, and sets the mark after the inserted text. Alternatively, you can select the other buffer for editing, then copy text from it by killing. See Buffers, for background information on buffers. 

Instead of accumulating text within Emacs, in a buffer, you can append text directly into a file with M-x append-to-file, which takes filename as an argument. It adds the text of the region to the end of the specified file. The file is changed immediately on disk. 

You should use append-to-file only with files that are not being visited in Emacs. Using it on a file that you are editing in Emacs would change the file behind Emacs's back, which can lead to losing some of your editing. 



--------------------------------------------------------------------------------
Next: CUA Bindings, Previous: Accumulating Text, Up: Top 
16 Rectangles
The rectangle commands operate on rectangular areas of the text: all the characters between a certain pair of columns, in a certain range of lines. Commands are provided to kill rectangles, yank killed rectangles, clear them out, fill them with blanks or text, or delete them. Rectangle commands are useful with text in multicolumn formats, and for changing text into or out of such formats. 

When you must specify a rectangle for a command to work on, you do it by putting the mark at one corner and point at the opposite corner. The rectangle thus specified is called the region-rectangle because you control it in much the same way as the region is controlled. But remember that a given combination of point and mark values can be interpreted either as a region or as a rectangle, depending on the command that uses them. 

If point and the mark are in the same column, the rectangle they delimit is empty. If they are in the same line, the rectangle is one line high. This asymmetry between lines and columns comes about because point (and likewise the mark) is between two columns, but within a line. 

C-x r k
Kill the text of the region-rectangle, saving its contents as the  last killed rectangle  (kill-rectangle). 

C-x r d
Delete the text of the region-rectangle (delete-rectangle). 

C-x r y
Yank the last killed rectangle with its upper left corner at point (yank-rectangle). 

C-x r o
Insert blank space to fill the space of the region-rectangle (open-rectangle). This pushes the previous contents of the region-rectangle rightward. 

C-x r c
Clear the region-rectangle by replacing its contents with spaces (clear-rectangle). 

M-x delete-whitespace-rectangle
Delete whitespace in each of the lines on the specified rectangle, starting from the left edge column of the rectangle. 

C-x r t string <RET>
Replace rectangle contents with string on each line. (string-rectangle). 

M-x string-insert-rectangle <RET> string <RET>
Insert string on each line of the rectangle. 
The rectangle operations fall into two classes: commands for deleting and inserting rectangles, and commands for blank rectangles. 

There are two ways to get rid of the text in a rectangle: you can discard the text (delete it) or save it as the  last killed  rectangle. The commands for these two ways are C-x r d (delete-rectangle) and C-x r k (kill-rectangle). In either case, the portion of each line that falls inside the rectangle's boundaries is deleted, causing any following text on the line to move left into the gap. 

Note that  killing  a rectangle is not killing in the usual sense; the rectangle is not stored in the kill ring, but in a special place that can only record the most recent rectangle killed. This is because yanking a rectangle is so different from yanking linear text that different yank commands have to be used and yank-popping is hard to make sense of. 

To yank the last killed rectangle, type C-x r y (yank-rectangle). Yanking a rectangle is the opposite of killing one. Point specifies where to put the rectangle's upper left corner. The rectangle's first line is inserted there, the rectangle's second line is inserted at the same horizontal position, but one line vertically down, and so on. The number of lines affected is determined by the height of the saved rectangle. 

You can convert single-column lists into double-column lists using rectangle killing and yanking; kill the second half of the list as a rectangle and then yank it beside the first line of the list. See Two-Column, for another way to edit multi-column text. 

You can also copy rectangles into and out of registers with C-x r r r and C-x r i r. See Rectangle Registers. 

There are two commands you can use for making blank rectangles: C-x r c (clear-rectangle) which blanks out existing text, and C-x r o (open-rectangle) which inserts a blank rectangle. Clearing a rectangle is equivalent to deleting it and then inserting a blank rectangle of the same size. 

The command M-x delete-whitespace-rectangle deletes horizontal whitespace starting from a particular column. This applies to each of the lines in the rectangle, and the column is specified by the left edge of the rectangle. The right edge of the rectangle does not make any difference to this command. 

The command C-x r t (string-rectangle) replaces the contents of a region-rectangle with a string on each line. The string's width need not be the same as the width of the rectangle. If the string's width is less, the text after the rectangle shifts left; if the string is wider than the rectangle, the text after the rectangle shifts right. 

The command M-x string-insert-rectangle is similar to string-rectangle, but inserts the string on each line, shifting the original text to the right. 

Next: Registers, Previous: Rectangles, Up: Top 
17 CUA Bindings
The command M-x cua-mode sets up key bindings that are compatible with the Common User Access (CUA) system used in many other applications. C-x means cut (kill), C-c copy, C-v paste (yank), and C-z undo. Standard Emacs commands like C-x C-c still work, because C-x and C-c only take effect when the mark is active (and the region is highlighted). However, if you don't want these bindings at all, set cua-enable-cua-keys to nil. 

In CUA mode, using Shift together with the movement keys activates and highlights the region over which they move. The standard (unshifted) movement keys deactivate the mark, and typed text replaces the active region as in Delete-Selection mode (see Graphical Kill). 

To run a command like C-x C-f while the mark is active, use one of the following methods: either hold Shift together with the prefix key, e.g. S-C-x C-f, or quickly type the prefix key twice, e.g. C-x C-x C-f. 

CUA mode provides enhanced rectangle support with visible rectangle highlighting. Use C-RET to start a rectangle, extend it using the movement commands, and cut or copy it using C-x or C-c. RET moves the cursor to the next (clockwise) corner of the rectangle, so you can easily expand it in any direction. Normal text you type is inserted to the left or right of each line in the rectangle (on the same side as the cursor). 

With CUA you can easily copy text and rectangles into and out of registers by providing a one-digit numeric prefix the the kill, copy, and yank commands, e.g. C-1 C-c copies the region into register 1, and C-2 C-v yanks the contents of register 2. 

CUA mode also has a global mark feature which allows easy moving and copying of text between buffers. Use C-S-SPC to toggle the global mark on and off. When the global mark is on, all text that you kill or copy is automatically inserted at the global mark, and text you type is inserted at the global mark rather than at the current position. 

For example, to copy words from various buffers into a word list in a given buffer, set the global mark in the target buffer, then navigate to each of the words you want in the list, mark it (e.g. with S-M-f), copy it to the list with C-c or M-w, and insert a newline after the word in the target list by pressing <RET>. 
Node:Introduction, Next:Common options, Previous:Top, Up:Top 

Introduction
This manual is incomplete: No attempt is made to explain basic file concepts in a way suitable for novices. Thus, if you are interested, please get involved in improving this manual. The entire GNU community will benefit. 

The GNU file utilities are mostly compatible with the POSIX.2 standard. 

Please report bugs to bug-fileutils@gnu.org. Remember to include the version number, machine architecture, input files, and any other information needed to reproduce the bug: your input, what you expected, what you got, and why it is wrong. Diffs are welcome, but please include a description of the problem as well, since this is sometimes difficult to infer. See Bugs. 

This manual was originally derived from the Unix man pages in the distribution, which were written by David MacKenzie and updated by Jim Meyering. What you are reading now is the authoritative documentation for these utilities; the man pages are no longer being maintained. Francois Pinard did the initial conversion to Texinfo format. Karl Berry did the indexing, some reorganization, and editing of the results. Richard Stallman contributed his usual invaluable insights to the overall process. 



--------------------------------------------------------------------------------
Node:Common options, Next:File permissions, Previous:Introduction, Up:Top 

Common options
Certain options are available in all of these programs (in fact, every GNU program should accept them). Rather than writing identical descriptions for each of the programs, they are described here. 

--help 
Print a usage message listing all available options, then exit successfully. 

--version 
Print the version number, then exit successfully. 
Backup options: -b -S -V, in some programs. 
Block size: BLOCK_SIZE and --block-size, in some programs. 
Target directory: --target-directory, in some programs. 
Trailing slashes: --strip-trailing-slashes, in some programs. 


--------------------------------------------------------------------------------
Node:Backup options, Next:Block size, Up:Common options 

Backup options
Some GNU programs (at least cp, install, ln, and mv) optionally make backups of files before writing new versions. These options control the details of these backups. The options are also briefly mentioned in the descriptions of the particular programs. 



-b 
--backup[=method] 
Make a backup of each file that would otherwise be overwritten or removed. Without this option, the original versions are destroyed. Use method to determine the type of backups to make. When this option is used but method is not specified, then the value of the VERSION_CONTROL environment variable is used. And if VERSION_CONTROL is not set, the default backup type is existing. 
Note that the short form of this option, -b does not accept any argument. Using -b is equivalent to using --backup=existing. 

This option corresponds to the Emacs variable version-control; the values for method are the same as those used in Emacs. This option also accepts more descriptive names. The valid methods are (unique abbreviations are accepted): 

none 
off 
Never make backups. 

numbered 
t 
Always make numbered backups. 

existing 
nil 
Make numbered backups of files that already have them, simple backups of the others. 

simple 
never 
Always make simple backups. Please note never is not to be confused with none. 


-S suffix 
--suffix=suffix 
Append suffix to each backup file made with -b. If this option is not specified, the value of the SIMPLE_BACKUP_SUFFIX environment variable is used. And if SIMPLE_BACKUP_SUFFIX is not set, the default is ~, just as in Emacs. 
--version-control=method 
This option is obsolete and will be removed in a future release. It has been replaced with --backup. 


--------------------------------------------------------------------------------
Node:Block size, Next:Target directory, Previous:Backup options, Up:Common options 

Block size
Some GNU programs (at least df, du, and ls) display file sizes in "blocks". You can adjust the block size to make file sizes easier to read. The block size used for display is independent of any filesystem block size. 

Normally, disk usage sizes are rounded up, disk free space sizes are rounded down, and other sizes are rounded to the nearest value with ties rounding to an even value. 

The default block size is chosen by examining the following environment variables in turn; the first one that is set determines the block size. 



DF_BLOCK_SIZE 
This specifies the default block size for the df command. Similarly, DU_BLOCK_SIZE specifies the default for du and LS_BLOCK_SIZE for ls. 

BLOCK_SIZE 
This specifies the default block size for all three commands, if the above command-specific environment variables are not set. 

POSIXLY_CORRECT 
If neither the command_BLOCK_SIZE nor the BLOCK_SIZE variables are set, but this variable is set, the block size defaults to 512. 
If none of the above environment variables are set, the block size currently defaults to 1024 bytes, but this number may change in the future. 

A block size specification can be a positive integer specifying the number of bytes per block, or it can be human-readable or si to select a human-readable format. 

With human-readable formats, output sizes are followed by a size letter such as M for megabytes. BLOCK_SIZE=human-readable uses powers of 1024; M stands for 1,048,576 bytes. BLOCK_SIZE=si is similar, but uses powers of 1000; M stands for 1,000,000 bytes. (SI, the International System of Units, defines these power-of-1000 prefixes.) 

An integer block size can be followed by a size letter to specify a multiple of that size. When this notation is used, the size letters normally stand for powers of 1024, and can be followed by an optional B for "byte"; but if followed by D (for "decimal byte"), they stand for powers of 1000. For example, BLOCK_SIZE=4MB is equivalent to BLOCK_SIZE=4194304, and BLOCK_SIZE=4MD is equivalent to BLOCK_SIZE=4000000. 

The following size letters are defined. Large sizes like 1Y may be rejected by your computer due to limitations of its arithmetic. 

k 
kilo: 2^10 = 1024 for human-readable, or 10^3 = 1000 for si. 

M 
Mega: 2^20 = 1,048,576 or 10^6 = 1,000,000. 

G 
Giga: 2^30 = 1,073,741,824 or 10^9 = 1,000,000,000. 

T 
Tera: 2^40 = 1,099,511,627,776 or 10^12 = 1,000,000,000,000. 

P 
Peta: 2^50 = 1,125,899,906,842,624 or 10^15 = 1,000,000,000,000,000. 

E 
Exa: 2^60 = 1,152,921,504,606,846,976 or 10^18 = 1,000,000,000,000,000,000. 

Z 
Zetta: 2^70 = 1,180,591,620,717,411,303,424 or 10^21 = 1,000,000,000,000,000,000,000. 

Y 
Yotta: 2^80 = 1,208,925,819,614,629,174,706,176 or 10^24 = 1,000,000,000,000,000,000,000,000. 
Block size defaults can be overridden by an explicit --block-size=size option. The -k or --kilobytes option is equivalent to --block-size=1k, which is the default unless the POSIXLY_CORRECT environment variable is set. The -h or --human-readable option is equivalent to --block-size=human-readable. The --si option is equivalent to --block-size=si. 



--------------------------------------------------------------------------------
Node:Target directory, Next:Trailing slashes, Previous:Block size, Up:Common options 

Target directory
Some GNU programs (at least cp, install, ln, and mv) allow you to specify the target directory via this option: 

--target-directory=directory 
Specify the destination directory. 
The interface for most programs is that after processing options and a finite (possibly zero) number of fixed-position arguments, the remaining argument list is either expected to be empty, or is a list of items (usually files) that will all be handled identically. The xargs program is designed to work well with this convention. 

The commands in the mv-family are unusual in that they take a variable number of arguments with a special case at the end (namely, the target directory). This makes it nontrivial to perform some operations, e.g., "move all files from here to ../d/", because mv * ../d/ might exhaust the argument space, and ls | xargs ... doesn't have a clean way to specify an extra final argument for each invocation of the subject command. (It can be done by going through a shell command, but that requires more human labor and brain power than it should.) 

The --target-directory option allows the cp, install, ln, and mv programs to be used conveniently with xargs. For example, you can move the files from the current directory to a sibling directory, d like this: (However, this doesn't move files whose names begin with ..) 

ls |xargs mv --target-directory=../d

If you use the GNU find program, you can move all files with this command: 

find . -mindepth 1 -maxdepth 1 \
  | xargs mv --target-directory=../d

But that will fail if there are no files in the current directory or if any file has a name containing a newline character. The following example removes those limitations and requires both GNU find and GNU xargs: 

find . -mindepth 1 -maxdepth 1 -print0 \
  | xargs --null --no-run-if-empty \
      mv --target-directory=../d



--------------------------------------------------------------------------------
Node:Trailing slashes, Previous:Target directory, Up:Common options 

Trailing slashes
Some GNU programs (at least cp and mv) allow you to remove any trailing slashes from each source argument before operating on it. The --strip-trailing-slashes option enables this behavior. 

This is useful when a source argument may have a trailing slash and specify a symbolic link to a directory. This scenario is in fact rather common because some shells can automatically append a trailing slash when performing file name completion on such symbolic links. Without this option, mv, for example, (via the system's rename function) must interpret a trailing slash as a request to dereference the symbolic link and so must rename the indirectly referenced directory and not the symbolic link. Although it may seem surprising that such behavior be the default, it is required by POSIX.2 and is consistent with other parts of that standard. 



--------------------------------------------------------------------------------
Node:File permissions, Next:Date input formats, Previous:Common options, Up:Top 

File permissions
Each file has a set of permissions that control the kinds of access that users have to that file. The permissions for a file are also called its access mode. They can be represented either in symbolic form or as an octal number. 

Mode Structure: Structure of file permissions. 
Symbolic Modes: Mnemonic permissions representation. 
Numeric Modes: Permissions as octal numbers. 


--------------------------------------------------------------------------------
Node:Mode Structure, Next:Symbolic Modes, Up:File permissions 

Structure of File Permissions
There are three kinds of permissions that a user can have for a file: 


permission to read the file. For directories, this means permission to list the contents of the directory. 
permission to write to (change) the file. For directories, this means permission to create and remove files in the directory. 
permission to execute the file (run it as a program). For directories, this means permission to access files in the directory. 
There are three categories of users who may have different permissions to perform any of the above operations on a file: 


the file's owner; 
other users who are in the file's group; 
everyone else. 
Files are given an owner and group when they are created. Usually the owner is the current user and the group is the group of the directory the file is in, but this varies with the operating system, the filesystem the file is created on, and the way the file is created. You can change the owner and group of a file by using the chown and chgrp commands. 

In addition to the three sets of three permissions listed above, a file's permissions have three special components, which affect only executable files (programs) and, on some systems, directories: 


set the process's effective user ID to that of the file upon execution (called the setuid bit). No effect on directories. 
set the process's effective group ID to that of the file upon execution (called the setgid bit). For directories on some systems, put files created in the directory into the same group as the directory, no matter what group the user who creates them is in. 
save the program's text image on the swap device so it will load more quickly when run (called the sticky bit). For directories on some systems, prevent users from removing or renaming a file in a directory unless they own the file or the directory; this is called the restriction deletion flag for the directory. 


--------------------------------------------------------------------------------
Node:Symbolic Modes, Next:Numeric Modes, Previous:Mode Structure, Up:File permissions 

Symbolic Modes
Symbolic modes represent changes to files' permissions as operations on single-character symbols. They allow you to modify either all or selected parts of files' permissions, optionally based on their previous values, and perhaps on the current umask as well (see Umask and Protection). 

The format of symbolic modes is: 

[ugoa...][[+-=][rwxXstugo...]...][,...]

The following sections describe the operators and other details of symbolic modes. 

Setting Permissions: Basic operations on permissions. 
Copying Permissions: Copying existing permissions. 
Changing Special Permissions: Special permissions. 
Conditional Executability: Conditionally affecting executability. 
Multiple Changes: Making multiple changes. 
Umask and Protection: The effect of the umask. 


--------------------------------------------------------------------------------
Node:Setting Permissions, Next:Copying Permissions, Up:Symbolic Modes 

Setting Permissions
The basic symbolic operations on a file's permissions are adding, removing, and setting the permission that certain users have to read, write, and execute the file. These operations have the following format: 

users operation permissions

The spaces between the three parts above are shown for readability only; symbolic modes cannot contain spaces. 

The users part tells which users' access to the file is changed. It consists of one or more of the following letters (or it can be empty; see Umask and Protection, for a description of what happens then). When more than one of these letters is given, the order that they are in does not matter. 

u 
the user who owns the file; 

g 
other users who are in the file's group; 

o 
all other users; 

a 
all users; the same as ugo. 
The operation part tells how to change the affected users' access to the file, and is one of the following symbols: 

+ 
to add the permissions to whatever permissions the users already have for the file; 

- 
to remove the permissions from whatever permissions the users already have for the file; 

= 
to make the permissions the only permissions that the users have for the file. 
The permissions part tells what kind of access to the file should be changed; it is zero or more of the following letters. As with the users part, the order does not matter when more than one letter is given. Omitting the permissions part is useful only with the = operation, where it gives the specified users no access at all to the file. 

r 
the permission the users have to read the file; 

w 
the permission the users have to write to the file; 

x 
the permission the users have to execute the file. 
For example, to give everyone permission to read and write a file, but not to execute it, use: 

a=rw

To remove write permission for from all users other than the file's owner, use: 

go-w

The above command does not affect the access that the owner of the file has to it, nor does it affect whether other users can read or execute the file. 

To give everyone except a file's owner no permission to do anything with that file, use the mode below. Other users could still remove the file, if they have write permission on the directory it is in. 

go=

Another way to specify the same thing is: 

og-rxw



--------------------------------------------------------------------------------
Node:Copying Permissions, Next:Changing Special Permissions, Previous:Setting Permissions, Up:Symbolic Modes 

Copying Existing Permissions
You can base a file's permissions on its existing permissions. To do this, instead of using r, w, or x after the operator, you use the letter u, g, or o. For example, the mode 

o+g

adds the permissions for users who are in a file's group to the permissions that other users have for the file. Thus, if the file started out as mode 664 (rw-rw-r--), the above mode would change it to mode 666 (rw-rw-rw-). If the file had started out as mode 741 (rwxr----x), the above mode would change it to mode 745 (rwxr--r-x). The - and = operations work analogously. 



--------------------------------------------------------------------------------
Node:Changing Special Permissions, Next:Conditional Executability, Previous:Copying Permissions, Up:Symbolic Modes 

Changing Special Permissions
In addition to changing a file's read, write, and execute permissions, you can change its special permissions. See Mode Structure, for a summary of these permissions. 

To change a file's permission to set the user ID on execution, use u in the users part of the symbolic mode and s in the permissions part. 

To change a file's permission to set the group ID on execution, use g in the users part of the symbolic mode and s in the permissions part. 

To change a file's permission to stay permanently on the swap device, use o in the users part of the symbolic mode and t in the permissions part. 

For example, to add set user ID permission to a program, you can use the mode: 

u+s

To remove both set user ID and set group ID permission from it, you can use the mode: 

ug-s

To cause a program to be saved on the swap device, you can use the mode: 

o+t

Remember that the special permissions only affect files that are executable, plus, on some systems, directories (on which they have different meanings; see Mode Structure). Also, the combinations u+t, g+t, and o+s have no effect. 

The = operator is not very useful with special permissions; for example, the mode: 

o=t

does cause the file to be saved on the swap device, but it also removes all read, write, and execute permissions that users not in the file's group might have had for it. 



--------------------------------------------------------------------------------
Node:Conditional Executability, Next:Multiple Changes, Previous:Changing Special Permissions, Up:Symbolic Modes 

Conditional Executability
There is one more special type of symbolic permission: if you use X instead of x, execute permission is affected only if the file already had execute permission or is a directory. It affects directories' execute permission even if they did not initially have any execute permissions set. 

For example, this mode: 

a+X

gives all users permission to execute files (or search directories) if anyone could before. 



--------------------------------------------------------------------------------
Node:Multiple Changes, Next:Umask and Protection, Previous:Conditional Executability, Up:Symbolic Modes 

Making Multiple Changes
The format of symbolic modes is actually more complex than described above (see Setting Permissions). It provides two ways to make multiple changes to files' permissions. 

The first way is to specify multiple operation and permissions parts after a users part in the symbolic mode. 

For example, the mode: 

og+rX-w

gives users other than the owner of the file read permission and, if it is a directory or if someone already had execute permission to it, gives them execute permission; and it also denies them write permission to the file. It does not affect the permission that the owner of the file has for it. The above mode is equivalent to the two modes: 

og+rX
og-w

The second way to make multiple changes is to specify more than one simple symbolic mode, separated by commas. For example, the mode: 

a+r,go-w

gives everyone permission to read the file and removes write permission on it for all users except its owner. Another example: 

u=rwx,g=rx,o=

sets all of the non-special permissions for the file explicitly. (It gives users who are not in the file's group no permission at all for it.) 

The two methods can be combined. The mode: 

a+r,g+x-w

gives all users permission to read the file, and gives users who are in the file's group permission to execute it, as well, but not permission to write to it. The above mode could be written in several different ways; another is: 

u+r,g+rx,o+r,g-w



--------------------------------------------------------------------------------
Node:Umask and Protection, Previous:Multiple Changes, Up:Symbolic Modes 

The Umask and Protection
If the users part of a symbolic mode is omitted, it defaults to a (affect all users), except that any permissions that are set in the system variable umask are not affected. The value of umask can be set using the umask command. Its default value varies from system to system. 

Omitting the users part of a symbolic mode is generally not useful with operations other than +. It is useful with + because it allows you to use umask as an easily customizable protection against giving away more permission to files than you intended to. 

As an example, if umask has the value 2, which removes write permission for users who are not in the file's group, then the mode: 

+w

adds permission to write to the file to its owner and to other users who are in the file's group, but not to other users. In contrast, the mode: 

a+w

ignores umask, and does give write permission for the file to all users. 



--------------------------------------------------------------------------------
Node:Numeric Modes, Previous:Symbolic Modes, Up:File permissions 

Numeric Modes
File permissions are stored internally as integers. As an alternative to giving a symbolic mode, you can give an octal (base 8) number that corresponds to the internal representation of the new mode. This number is always interpreted in octal; you do not have to add a leading 0, as you do in C. Mode 0055 is the same as mode 55. 

A numeric mode is usually shorter than the corresponding symbolic mode, but it is limited in that it cannot take into account a file's previous permissions; it can only set them absolutely. 

On most systems, the permissions granted to the user, to other users in the file's group, and to other users not in the file's group are each stored as three bits, which are represented as one octal digit. The three special permissions are also each stored as one bit, and they are as a group represented as another octal digit. Here is how the bits are arranged, starting with the lowest valued bit: 

Value in  Corresponding
Mode      Permission

          Other users not in the file's group:
   1      Execute
   2      Write
   4      Read

          Other users in the file's group:
  10      Execute
  20      Write
  40      Read

          The file's owner:
 100      Execute
 200      Write
 400      Read

          Special permissions:
1000      Save text image on swap device
2000      Set group ID on execution
4000      Set user ID on execution

For example, numeric mode 4755 corresponds to symbolic mode u=rwxs,go=rx, and numeric mode 664 corresponds to symbolic mode ug=rw,o=r. Numeric mode 0 corresponds to symbolic mode ugo=. 



--------------------------------------------------------------------------------
Node:Date input formats, Next:Directory listing, Previous:File permissions, Up:Top 

Date input formats
First, a quote: 

Our units of temporal measurement, from seconds on up to months, are so complicated, asymmetrical and disjunctive so as to make coherent mental reckoning in time all but impossible. Indeed, had some tyrannical god contrived to enslave our minds to time, to make it all but impossible for us to escape subjection to sodden routines and unpleasant surprises, he could hardly have done better than handing down our present system. It is like a set of trapezoidal building blocks, with no vertical or horizontal surfaces, like a language in which the simplest thought demands ornate constructions, useless particles and lengthy circumlocutions. Unlike the more successful patterns of language and science, which enable us to face experience boldly or at least level-headedly, our system of temporal calculation silently and persistently encourages our terror of time. ... It is as though architects had to measure length in feet, width in meters and height in ells; as though basic instruction manuals demanded a knowledge of five different languages. It is no wonder then that we often look into our own immediate past or future, last Tuesday or a week from Sunday, with feelings of helpless confusion. ... 
-- Robert Grudin, Time and the Art of Living. 

This section describes the textual date representations that GNU programs accept. These are the strings you, as a user, can supply as arguments to the various programs. The C interface (via the getdate function) is not described here. 

Although the date syntax here can represent any possible time since the year zero, computer integers often cannot represent such a wide range of time. On POSIX systems, the clock starts at 1970-01-01 00:00:00 UTC: POSIX does not require support for times before the POSIX Epoch and times far in the future. Traditional Unix systems have 32-bit signed time_t and can represent times from 1901-12-13 20:45:52 through 2038-01-19 03:14:07 UTC. Systems with 64-bit signed time_t can represent all the times in the known lifetime of the universe. 

General date syntax: Common rules. 
Calendar date items: 19 Dec 1994. 
Time of day items: 9:20pm. 
Time zone items: EST, PDT, GMT, ... 
Day of week items: Monday and others. 
Relative items in date strings: next tuesday, 2 years ago. 
Pure numbers in date strings: 19931219, 1440. 
Authors of getdate: Bellovin, Eggert, Salz, Berets, et al. 


--------------------------------------------------------------------------------
Node:General date syntax, Next:Calendar date items, Up:Date input formats 

General date syntax
A date is a string, possibly empty, containing many items separated by whitespace. The whitespace may be omitted when no ambiguity arises. The empty string means the beginning of today (i.e., midnight). Order of the items is immaterial. A date string may contain many flavors of items: 

calendar date items 
time of the day items 
time zone items 
day of the week items 
relative items 
pure numbers. 
We describe each of these item types in turn, below. 

A few numbers may be written out in words in most contexts. This is most useful for specifying day of the week items or relative items (see below). Here is the list: first for 1, next for 2, third for 3, fourth for 4, fifth for 5, sixth for 6, seventh for 7, eighth for 8, ninth for 9, tenth for 10, eleventh for 11 and twelfth for 12. Also, last means exactly -1. 

When a month is written this way, it is still considered to be written numerically, instead of being "spelled in full"; this changes the allowed strings. 

In the current implementation, only English is supported for words and abbreviations like AM, DST, EST, first, January, Sunday, tomorrow, and year. 

The output of date is not always acceptable as a date string, not only because of the language problem, but also because there is no standard meaning for time zone items like IST. When using date to generate a date string intended to be parsed later, specify a date format that is independent of language and that does not use time zone items other than UTC and Z. Here are some ways to do this: 

$ LC_ALL=C TZ=UTC0 date
Fri Dec 15 19:48:05 UTC 2000
$ TZ=UTC0 date +"%Y-%m-%d %H:%M:%SZ"
2000-12-15 19:48:05Z
$ date --iso-8601=seconds  # a GNU extension
2000-12-15T11:48:05-0800
$ date --rfc-822  # a GNU extension
Fri, 15 Dec 2000 11:48:05 -0800
$ date +"%Y-%m-%d %H:%M:%S %z"  # %z is a GNU extension.
2000-12-15 11:48:05 -0800

Alphabetic case is completely ignored in dates. Comments may be introduced between round parentheses, as long as included parentheses are properly nested. Hyphens not followed by a digit are currently ignored. Leading zeros on numbers are ignored. 



--------------------------------------------------------------------------------
Node:Calendar date items, Next:Time of day items, Previous:General date syntax, Up:Date input formats 

Calendar date items
A calendar date item specifies a day of the year. It is specified differently, depending on whether the month is specified numerically or literally. All these strings specify the same calendar date: 

1972-09-24     # ISO 8601.
72-9-24        # Assume 19xx for 69 through 99,
               # 20xx for 00 through 68.
72-09-24       # Leading zeros are ignored.
9/24/72        # Common U.S. writing.
24 September 1972
24 Sept 72     # September has a special abbreviation.
24 Sep 72      # Three-letter abbreviations always allowed.
Sep 24, 1972
24-sep-72
24sep72

The year can also be omitted. In this case, the last specified year is used, or the current year if none. For example: 

9/24
sep 24

Here are the rules. 

For numeric months, the ISO 8601 format year-month-day is allowed, where year is any positive number, month is a number between 01 and 12, and day is a number between 01 and 31. A leading zero must be present if a number is less than ten. If year is 68 or smaller, then 2000 is added to it; otherwise, if year is less than 100, then 1900 is added to it. The construct month/day/year, popular in the United States, is accepted. Also month/day, omitting the year. 

Literal months may be spelled out in full: January, February, March, April, May, June, July, August, September, October, November or December. Literal months may be abbreviated to their first three letters, possibly followed by an abbreviating dot. It is also permitted to write Sept instead of September. 

When months are written literally, the calendar date may be given as any of the following: 

day month year
day month
month day year
day-month-year

Or, omitting the year: 

month day



--------------------------------------------------------------------------------
Node:Time of day items, Next:Time zone items, Previous:Calendar date items, Up:Date input formats 

Time of day items
A time of day item in date strings specifies the time on a given day. Here are some examples, all of which represent the same time: 

20:02:0
20:02
8:02pm
20:02-0500      # In EST (U.S. Eastern Standard Time).

More generally, the time of the day may be given as hour:minute:second, where hour is a number between 0 and 23, minute is a number between 0 and 59, and second is a number between 0 and 59. Alternatively, :second can be omitted, in which case it is taken to be zero. 

If the time is followed by am or pm (or a.m. or p.m.), hour is restricted to run from 1 to 12, and :minute may be omitted (taken to be zero). am indicates the first half of the day, pm indicates the second half of the day. In this notation, 12 is the predecessor of 1: midnight is 12am while noon is 12pm. (This is the zero-oriented interpretation of 12am and 12pm, as opposed to the old tradition derived from Latin which uses 12m for noon and 12pm for midnight.) 

The time may alternatively be followed by a time zone correction, expressed as shhmm, where s is + or -, hh is a number of zone hours and mm is a number of zone minutes. When a time zone correction is given this way, it forces interpretation of the time relative to Coordinated Universal Time (UTC), overriding any previous specification for the time zone or the local time zone. The minute part of the time of the day may not be elided when a time zone correction is used. This is the best way to specify a time zone correction by fractional parts of an hour. 

Either am/pm or a time zone correction may be specified, but not both. 



--------------------------------------------------------------------------------
Node:Time zone items, Next:Day of week items, Previous:Time of day items, Up:Date input formats 

Time zone items
A time zone item specifies an international time zone, indicated by a small set of letters, e.g., UTC or Z for Coordinated Universal Time. Any included periods are ignored. By following a non-daylight-saving time zone by the string DST in a separate word (that is, separated by some white space), the corresponding daylight saving time zone may be specified. 

Time zone items other than UTC and Z are obsolescent and are not recommended, because they are ambiguous; for example, EST has a different meaning in Australia than in the United States. Instead, it's better to use unambiguous numeric time zone corrections like -0500, as described in the previous section. 



--------------------------------------------------------------------------------
Node:Day of week items, Next:Relative items in date strings, Previous:Time zone items, Up:Date input formats 

Day of week items
The explicit mention of a day of the week will forward the date (only if necessary) to reach that day of the week in the future. 

Days of the week may be spelled out in full: Sunday, Monday, Tuesday, Wednesday, Thursday, Friday or Saturday. Days may be abbreviated to their first three letters, optionally followed by a period. The special abbreviations Tues for Tuesday, Wednes for Wednesday and Thur or Thurs for Thursday are also allowed. 

A number may precede a day of the week item to move forward supplementary weeks. It is best used in expression like third monday. In this context, last day or next day is also acceptable; they move one week before or after the day that day by itself would represent. 

A comma following a day of the week item is ignored. 



--------------------------------------------------------------------------------
Node:Relative items in date strings, Next:Pure numbers in date strings, Previous:Day of week items, Up:Date input formats 

Relative items in date strings
Relative items adjust a date (or the current date if none) forward or backward. The effects of relative items accumulate. Here are some examples: 

1 year
1 year ago
3 years
2 days

The unit of time displacement may be selected by the string year or month for moving by whole years or months. These are fuzzy units, as years and months are not all of equal duration. More precise units are fortnight which is worth 14 days, week worth 7 days, day worth 24 hours, hour worth 60 minutes, minute or min worth 60 seconds, and second or sec worth one second. An s suffix on these units is accepted and ignored. 

The unit of time may be preceded by a multiplier, given as an optionally signed number. Unsigned numbers are taken as positively signed. No number at all implies 1 for a multiplier. Following a relative item by the string ago is equivalent to preceding the unit by a multiplier with value -1. 

The string tomorrow is worth one day in the future (equivalent to day), the string yesterday is worth one day in the past (equivalent to day ago). 

The strings now or today are relative items corresponding to zero-valued time displacement, these strings come from the fact a zero-valued time displacement represents the current time when not otherwise changed by previous items. They may be used to stress other items, like in 12:00 today. The string this also has the meaning of a zero-valued time displacement, but is preferred in date strings like this thursday. 

When a relative item causes the resulting date to cross a boundary where the clocks were adjusted, typically for daylight-saving time, the resulting date and time are adjusted accordingly. 



--------------------------------------------------------------------------------
Node:Pure numbers in date strings, Next:Authors of getdate, Previous:Relative items in date strings, Up:Date input formats 

Pure numbers in date strings
The precise interpretation of a pure decimal number depends on the context in the date string. 

If the decimal number is of the form yyyymmdd and no other calendar date item (see Calendar date items) appears before it in the date string, then yyyy is read as the year, mm as the month number and dd as the day of the month, for the specified calendar date. 

If the decimal number is of the form hhmm and no other time of day item appears before it in the date string, then hh is read as the hour of the day and mm as the minute of the hour, for the specified time of the day. mm can also be omitted. 

If both a calendar date and a time of day appear to the left of a number in the date string, but no relative item, then the number overrides the year. 



--------------------------------------------------------------------------------
Node:Authors of getdate, Previous:Pure numbers in date strings, Up:Date input formats 

Authors of getdate
getdate was originally implemented by Steven M. Bellovin (smb@research.att.com) while at the University of North Carolina at Chapel Hill. The code was later tweaked by a couple of people on Usenet, then completely overhauled by Rich $alz (rsalz@bbn.com) and Jim Berets (jberets@bbn.com) in August, 1990. Various revisions for the GNU system were made by David MacKenzie, Jim Meyering, Paul Eggert and others. 

This chapter was originally produced by Francois Pinard (pinard@iro.umontreal.ca) from the getdate.y source code, and then edited by K. Berry (kb@cs.umb.edu). 



--------------------------------------------------------------------------------
Node:Directory listing, Next:Basic operations, Previous:Date input formats, Up:Top 

Directory listing
This chapter describes the ls command and its variants dir and vdir, which list information about files. 

ls invocation: List directory contents. 
dir invocation: Briefly ls. 
vdir invocation: Verbosely ls. 
dircolors invocation: Color setup for ls, etc. 


--------------------------------------------------------------------------------
Node:ls invocation, Next:dir invocation, Up:Directory listing 

ls: List directory contents
The ls program lists information about files (of any type, including directories). Options and file arguments can be intermixed arbitrarily, as usual. 

For non-option command-line arguments that are directories, by default ls lists the contents of directories, not recursively, and omitting files with names beginning with .. For other non-option arguments, by default ls lists just the file name. If no non-option arguments are specified, ls lists the contents of the current directory. 

By default, the output is sorted alphabetically. If standard output is a terminal, the output is in columns (sorted vertically) and control characters are output as question marks; otherwise, the output is listed one per line and control characters are output as-is. 

Because ls is such a fundamental program, it has accumulated many options over the years. They are described in the subsections below; within each section, options are listed alphabetically (ignoring case). The division of options into the subsections is not absolute, since some options affect more than one aspect of ls's operation. 

The -g option is accepted but ignored, for compatibility with Unix. Also see Common options. 

Which files are listed: 
What information is listed: 
Sorting the output: 
More details about version sort: 
General output formatting: 
Formatting the file names: 


--------------------------------------------------------------------------------
Node:Which files are listed, Next:What information is listed, Up:ls invocation 

Which files are listed
These options determine which files ls lists information for. By default, any files and the contents of any directories on the command line are shown. 



-a 
--all 
List all files in directories, including files that start with .. 

-A 
--almost-all 
List all files in directories except for . and ... 

-B 
--ignore-backups 
Do not list files that end with ~, unless they are given on the command line. 

-d 
--directory 
List just the names of directories, as with other types of files, rather than listing their contents. 

-I PATTERN 
--ignore=PATTERN 
Do not list files whose names match the shell pattern (not regular expression) pattern unless they are given on the command line. As in the shell, an initial . in a file name does not match a wildcard at the start of pattern. Sometimes it is useful to give this option several times. For example, 
$ ls --ignore='.??*' --ignore='.[^.]' --ignore='#*'

The first option ignores names of length 3 or more that start with ., the second ignores all two-character names that start with . except .., and the third ignores names that start with #. 


-L 
--dereference 
In a long listing, show file information (e.g., times and permissions) for the referents of symbolic links rather than for the symbolic links themselves. 

-R 
--recursive 
List the contents of all directories recursively. 


--------------------------------------------------------------------------------
Node:What information is listed, Next:Sorting the output, Previous:Which files are listed, Up:ls invocation 

What information is listed
These options affect the information that ls displays. By default, only file names are shown. 



-D 
--dired 
With the long listing (-l) format, print an additional line after the main output: 
//DIRED// beg1 end1 beg2 end2 ...

The begN and endN are unsigned integers that record the byte position of the beginning and end of each file name in the output. This makes it easy for Emacs to find the names, even when they contain unusual characters such as space or newline, without fancy searching. 

If directories are being listed recursively (-R), output a similar line after each subdirectory: 

//SUBDIRED// format beg1 end1 ...

Finally, output a line of the form: 

//DIRED-OPTIONS// --quoting-style=word

where word is the quoting style (see Formatting the file names). 

-G 
--no-group 
Inhibit display of group information in a long format directory listing. (This is the default in some non-GNU versions of ls, so we provide this option for compatibility.) 

-h 
--human-readable 
Append a size letter such as M for megabytes to each size. Powers of 1024 are used, not 1000; M stands for 1,048,576 bytes. Use the --si option if you prefer powers of 1000. 

-H 
--si 
Append a size letter such as M for megabytes to each size. (SI is the International System of Units, which defines these letters as prefixes.) Powers of 1000 are used, not 1024; M stands for 1,000,000 bytes. Use the -h or --human-readable option if you prefer powers of 1024. 
Warning: the meaning of -H will change in the future to conform to POSIX. Use --si for the old meaning. 


-i 
--inode 
Print the inode number (also called the file serial number and index number) of each file to the left of the file name. (This number uniquely identifies each file within a particular filesystem.) 

-l 
--format=long 
--format=verbose 
In addition to the name of each file, print the file type, permissions, number of hard links, owner name, group name, size in bytes, and timestamp (by default, the modification time). For files with a time more than six months old or in the future, the timestamp contains the year instead of the time of day. If the timestamp contains today's date with the year rather than a time of day, the file's time is in the future, which means you probably have clock skew problems which may break programs like make that rely on file times. 
For each directory that is listed, preface the files with a line total blocks, where blocks is the total disk allocation for all files in that directory. The block size currently defaults to 1024 bytes, but this can be overridden (see Block size). The blocks computed counts each hard link separately; this is arguably a deficiency. 

The permissions listed are similar to symbolic mode specifications (see Symbolic Modes). But ls combines multiple bits into the third character of each set of permissions as follows: 

s 
If the setuid or setgid bit and the corresponding executable bit are both set. 

S 
If the setuid or setgid bit is set but the corresponding executable bit is not set. 

t 
If the sticky bit and the other-executable bit are both set. 

T 
If the sticky bit is set but the other-executable bit is not set. 

x 
If the executable bit is set and none of the above apply. 

- 
Otherwise. 
Following the permission bits is a single character that specifies whether an alternate access method applies to the file. When that character is a space, there is no alternate access method. When it is a printing character (e.g., +), then there is such a method. 


-o 
Produce long format directory listings, but don't display group information. It is equivalent to using --format=long with --no-group . This option is provided for compatibility with other versions of ls. 

-s 
--size 
Print the disk allocation of each file to the left of the file name. This is the amount of disk space used by the file, which is usually a bit more than the file's size, but it can be less if the file has holes. 
Normally the disk allocation is printed in units of 1024 bytes, but this can be overridden (see Block size). 

For files that are NFS-mounted from an HP-UX system to a BSD system, this option reports sizes that are half the correct values. On HP-UX systems, it reports sizes that are twice the correct values for files that are NFS-mounted from BSD systems. This is due to a flaw in HP-UX; it also affects the HP-UX ls program. 



--------------------------------------------------------------------------------
Node:Sorting the output, Next:More details about version sort, Previous:What information is listed, Up:ls invocation 

Sorting the output
These options change the order in which ls sorts the information it outputs. By default, sorting is done by character code (e.g., ASCII order). 



-c 
--time=ctime 
--time=status 
--time=use 
If the long listing format (e.g., -l, -o) is being used, print the status change time (the ctime in the inode) instead of the modification time. When explicitly sorting by time (--sort=time or -t) or when not using a long listing format, sort according to the status change time. 

-f 
Primarily, like -U--do not sort; list the files in whatever order they are stored in the directory. But also enable -a (list all files) and disable -l, --color, and -s (if they were specified before the -f). 

-r 
--reverse 
Reverse whatever the sorting method is--e.g., list files in reverse alphabetical order, youngest first, smallest first, or whatever. 

-S 
--sort=size 
Sort by file size, largest first. 

-t 
--sort=time 
Sort by modification time (the mtime in the inode), newest first. 

-u 
--time=atime 
--time=access 
If the long listing format (e.g., --format=long) is being used, print the last access time (the atime in the inode). When explicitly sorting by time (--sort=time or -t) or when not using a long listing format, sort according to the access time. 

-U 
--sort=none 
Do not sort; list the files in whatever order they are stored in the directory. (Do not do any of the other unrelated things that -f does.) This is especially useful when listing very large directories, since not doing any sorting can be noticeably faster. 

-v 
--sort=version 
Sort by version name and number, lowest first. It behaves like a default sort, except that each sequence of decimal digits is treated numerically as an index/version number. (See More details about version sort.) 

-X 
--sort=extension 
Sort directory contents alphabetically by file extension (characters after the last .); files with no extension are sorted first. 


--------------------------------------------------------------------------------
Node:More details about version sort, Next:General output formatting, Previous:Sorting the output, Up:ls invocation 

More details about version sort
The version sort takes into account the fact that file names frequently include indices or version numbers. Standard sorting functions usually do not produce the ordering that people expect because comparisons are made on a character-by-character basis. The version sort addresses this problem, and is especially useful when browsing directories that contain many files with indices/version numbers in their names: 

      > ls -1            > ls -1v
      foo.zml-1.gz       foo.zml-1.gz
      foo.zml-100.gz     foo.zml-2.gz
      foo.zml-12.gz      foo.zml-6.gz
      foo.zml-13.gz      foo.zml-12.gz
      foo.zml-2.gz       foo.zml-13.gz
      foo.zml-25.gz      foo.zml-25.gz
      foo.zml-6.gz       foo.zml-100.gz

Note also that numeric parts with leading zeroes are considered as fractional one: 

      > ls -1            > ls -1v
      abc-1.007.tgz      abc-1.007.tgz
      abc-1.012b.tgz     abc-1.01a.tgz
      abc-1.01a.tgz      abc-1.012b.tgz



--------------------------------------------------------------------------------
Node:General output formatting, Next:Formatting the file names, Previous:More details about version sort, Up:ls invocation 

General output formatting
These options affect the appearance of the overall output. 



-1 
--format=single-column 
List one file per line. This is the default for ls when standard output is not a terminal. 

-C 
--format=vertical 
List files in columns, sorted vertically. This is the default for ls if standard output is a terminal. It is always the default for the dir and d programs. GNU ls uses variable width columns to display as many files as possible in the fewest lines. 

--color [=when] 
Specify whether to use color for distinguishing file types. when may be omitted, or one of: 
none - Do not use color at all. This is the default. 
auto - Only use color if standard output is a terminal. 
always - Always use color. 
Specifying --color and no when is equivalent to --color=always. Piping a colorized listing through a pager like more or less usually produces unreadable results. However, using more -f does seem to work. 

-F 
--classify 
--indicator-style=classify 
Append a character to each file name indicating the file type. Also, for regular files that are executable, append *. The file type indicators are / for directories, @ for symbolic links, | for FIFOs, = for sockets, and nothing for regular files. 

--full-time 
List times in full, rather than using the standard abbreviation heuristics. The format is currently similar to that of date, but this is planned to change in a future release, partly because modern file time stamps have more precision. It's not possible to change the format, but you can extract out the date string with cut and then pass the result to date -d. See date invocation. 
This is most useful because the time output includes the seconds. (Unix filesystems store file timestamps only to the nearest second, so this option shows all the information there is.) For example, this can help when you have a Makefile that is not regenerating files properly. 


--indicator-style=word 
Append a character indicator with style word to entry names, as follows: 
none 
Do not append any character indicator; this is the default. 

file-type 
Append / for directories, @ for symbolic links, | for FIFOs, = for sockets, and nothing for regular files. This is the same as the -p or --file-type option. 

classify 
Append * for executable regular files, otherwise behave as for file-type. This is the same as the -F or --classify option. 


-k 
--kilobytes 
Print file sizes in 1024-byte blocks, overriding the default block size (see Block size). 

-m 
--format=commas 
List files horizontally, with as many as will fit on each line, separated by , (a comma and a space). 

-n 
--numeric-uid-gid 
List the numeric UID and GID instead of the names. 

-p 
--file-type 
--indicator-style=file-type 
Append a character to each file name indicating the file type. This is like -F, except that executables are not marked. 

-x format 
--format=across 
--format=horizontal 
List the files in columns, sorted horizontally. 

-T cols 
--tabsize=cols 
Assume that each tabstop is cols columns wide. The default is 8. ls uses tabs where possible in the output, for efficiency. If cols is zero, do not use tabs at all. 

-w 
--width=cols 
Assume the screen is cols columns wide. The default is taken from the terminal settings if possible; otherwise the environment variable COLUMNS is used if it is set; otherwise the default is 80. 


--------------------------------------------------------------------------------
Node:Formatting the file names, Previous:General output formatting, Up:ls invocation 

Formatting the file names
These options change how file names themselves are printed. 



-b 
--escape 
--quoting-style=escape 
Quote nongraphic characters in file names using alphabetic and octal backslash sequences like those used in C. 

-N 
--literal 
Do not quote file names. 

-q 
--hide-control-chars 
Print question marks instead of nongraphic characters in file names. This is the default if the output is a terminal and the program is ls. 

-Q 
--quote-name 
--quoting-style=c 
Enclose file names in double quotes and quote nongraphic characters as in C. 

--quoting-style=word 
Use style word to quote output names. The word should be one of the following: 
literal 
Output names as-is. 

shell 
Quote names for the shell if they contain shell metacharacters or would cause ambiguous output. 

shell-always 
Quote names for the shell, even if they would normally not require quoting. 

c 
Quote names as for a C language string; this is the same as the -Q or --quote-name option. 

escape 
Quote as with c except omit the surrounding double-quote characters; this is the same as the -b or --escape option. 

clocale 
Quote as with c except use quotation marks appropriate for the locale. 

locale 
Like clocale, but quote `like this' instead of "like this" in the default C locale. This looks nicer on many displays. 
You can specify the default value of the --quoting-style option with the environment variable QUOTING_STYLE. If that environment variable is not set, the default value is literal, but this default may change to shell in a future version of this package. 


--show-control-chars 
Print nongraphic characters as-is in file names. This is the default unless the output is a terminal and the program is ls. 


--------------------------------------------------------------------------------
Node:dir invocation, Next:vdir invocation, Previous:ls invocation, Up:Directory listing 

dir: Briefly list directory contents
dir (also installed as d) is equivalent to ls -C -b; that is, by default files are listed in columns, sorted vertically, and special characters are represented by backslash escape sequences. 

See ls. 



--------------------------------------------------------------------------------
Node:vdir invocation, Next:dircolors invocation, Previous:dir invocation, Up:Directory listing 

vdir: Verbosely list directory contents
vdir (also installed as v) is equivalent to ls -l -b; that is, by default files are listed in long format and special characters are represented by backslash escape sequences. 



--------------------------------------------------------------------------------
Node:dircolors invocation, Previous:vdir invocation, Up:Directory listing 

dircolors: Color setup for ls
dircolors outputs a sequence of shell commands to set up the terminal for color output from ls (and dir, etc.). Typical usage: 

eval `dircolors [option]... [file]`

If file is specified, dircolors reads it to determine which colors to use for which file types and extensions. Otherwise, a precompiled database is used. For details on the format of these files, run dircolors --print-database. 

The output is a shell command to set the LS_COLORS environment variable. You can specify the shell syntax to use on the command line, or dircolors will guess it from the value of the SHELL environment variable. 

The program accepts the following options. Also see Common options. 

-b 
--sh 
--bourne-shell 
Output Bourne shell commands. This is the default if the SHELL environment variable is set and does not end with csh or tcsh. 

-c 
--csh 
--c-shell 
Output C shell commands. This is the default if SHELL ends with csh or tcsh. 

-p 
--print-database 
Print the (compiled-in) default color configuration database. This output is itself a valid configuration file, and is fairly descriptive of the possibilities. 


--------------------------------------------------------------------------------
Node:Basic operations, Next:Special file types, Previous:Directory listing, Up:Top 

Basic operations
This chapter describes the commands for basic file manipulation: copying, moving (renaming), and deleting (removing). 

cp invocation: Copy files. 
dd invocation: Convert and copy a file. 
install invocation: Copy files and set attributes. 
mv invocation: Move (rename) files. 
rm invocation: Remove files or directories. 
shred invocation: Remove files more securely. 


--------------------------------------------------------------------------------
Node:cp invocation, Next:dd invocation, Up:Basic operations 

cp: Copy files and directories
cp copies files (or, optionally, directories). The copy is completely independent of the original. You can either copy one file to another, or copy arbitrarily many files to a destination directory. Synopsis: 

cp [option]... source dest
cp [option]... source... directory

If the last argument names an existing directory, cp copies each source file into that directory (retaining the same name). Otherwise, if only two files are given, it copies the first onto the second. It is an error if the last argument is not a directory and more than two non-option arguments are given. 

Generally, files are written just as they are read. For exceptions, see the --sparse option below. 

By default, cp does not copy directories. However, the -R, -a, and -r options cause cp to copy recursively by descending into source directories and copying files to corresponding destination directories. 

By default, cp follows symbolic links only when not copying recursively. This default can be overridden with the --no-dereference (-d), --dereference (-L), and -H options. If more than one of these options is specified, the last one silently overrides the others. 

cp generally refuses to copy a file onto itself, with the following exception: if --force --backup is specified with source and dest identical, and referring to a regular file, cp will make a backup file, either regular or numbered, as specified in the usual ways (see Backup options). This is useful when you simply want to make a backup of an existing file before changing it. 

The program accepts the following options. Also see Common options. 

-a 
--archive 
Preserve as much as possible of the structure and attributes of the original files in the copy (but do not attempt to preserve internal directory structure; i.e., ls -U may list the entries in a copied directory in a different order). Equivalent to -dpR. 

-b 
--backup[=method] 
See Backup options. Make a backup of each file that would otherwise be overwritten or removed. As a special case, cp makes a backup of source when the force and backup options are given and source and dest are the same name for an existing, regular file. One useful application of this combination of options is this tiny Bourne shell script: 
#!/bin/sh
# Usage: backup FILE...
# Create a GNU-style backup of each listed FILE.
for i in "$?; do
  cp --backup --force "$i" "$i"
done



-d 
--no-dereference 
Copy symbolic links as symbolic links rather than copying the files that they point to, and preserve hard links between source files in the copies. 

-f 
--force 
When copying without this option and an existing destination file cannot be opened for writing, the copy fails. However, with --force), when a destination file cannot be opened, cp then unlinks it and tries to open it again. Contrast this behavior with that enabled by --link and --symbolic-link, whereby the destination file is never opened but rather is unlinked unconditionally. Also see the description of --remove-destination. 

-H 
If a command line argument specifies a symbolic link, then copy the file it points to rather than the symbolic link itself. However, copy (preserving its nature) any symbolic link that is encountered via recursive traversal. 

-i 
--interactive 
Prompt whether to overwrite existing regular destination files. 

-l 
--link 
Make hard links instead of copies of non-directories. 

-L 
--dereference 
Always follow symbolic links. 

-p 
--preserve 
Preserve the original files' owner, group, permissions, and timestamps. In the absence of this option, each destination file is created with the permissions of the corresponding source file, minus the bits set in the umask. See File permissions. 

-P 
--parents 
Form the name of each destination file by appending to the target directory a slash and the specified name of the source file. The last argument given to cp must be the name of an existing directory. For example, the command: 
cp --parents a/b/c existing_dir

copies the file a/b/c to existing_dir/a/b/c, creating any missing intermediate directories. 

Warning: the meaning of -P will change in the future to conform to POSIX. Use --parents for the old meaning, and --no-dereference for the new. 


-r 
Copy directories recursively, copying any non-directories and special files (e.g., symbolic links, FIFOs and device files) as if they were regular files. This means trying to read the data in each source file and writing it to the destination. It is usually a mistake to apply cp -r to special files like FIFOs and the ones typically found in the /dev directory. In most cases, cp -r will hang indefinitely trying to read from FIFOs and special files like /dev/console, and it will fill up your destination disk if you use it to copy /dev/zero. Use the --recursive (-R) option instead if you want to copy special files, preserving their special nature rather than reading from them to copy their contents. 

-R 
--recursive 
Copy directories recursively, preserving non-directories (contrast with -r just above). 

--remove-destination 
Remove each existing destination file before attempting to open it (contrast with -f above). 

--sparse=when 
A sparse file contains holes--a sequence of zero bytes that does not occupy any physical disk blocks; the read system call reads these as zeroes. This can both save considerable disk space and increase speed, since many binary files contain lots of consecutive zero bytes. By default, cp detects holes in input source files via a crude heuristic and makes the corresponding output file sparse as well. 
The when value can be one of the following: 

auto 
The default behavior: the output file is sparse if the input file is sparse. 

always 
Always make the output file sparse. This is useful when the input file resides on a filesystem that does not support sparse files (the most notable example is efs filesystems in SGI IRIX 5.3 and earlier), but the output file is on another type of filesystem. 

never 
Never make the output file sparse. This is useful in creating a file for use with the mkswap command, since such a file must not have any holes. 
--strip-trailing-slashes 
Remove any trailing slashes from each source argument. See Trailing slashes. 

-s 
--symbolic-link 
Make symbolic links instead of copies of non-directories. All source file names must be absolute (starting with /) unless the destination files are in the current directory. This option merely results in an error message on systems that do not support symbolic links. 

-S suffix 
--suffix=suffix 
Append suffix to each backup file made with -b. See Backup options. 
--target-directory=directory 
Specify the destination directory. See Target directory. 

-v 
--verbose 
Print the name of each file before copying it. 

-V method 
--version-control=method 
Change the type of backups made with -b. The method argument can be none (or off), numbered (or t), existing (or nil), or never (or simple). See Backup options. 

-x 
--one-file-system 
Skip subdirectories that are on different filesystems from the one that the copy started on. However, mount point directories are copied. 


--------------------------------------------------------------------------------
Node:dd invocation, Next:install invocation, Previous:cp invocation, Up:Basic operations 

dd: Convert and copy a file
dd copies a file (from standard input to standard output, by default) with a changeable I/O block size, while optionally performing conversions on it. Synopsis: 

dd [option]...

The program accepts the following options. Also see Common options. 

The numeric-valued options below (bytes and blocks) can be followed by a multiplier: b=512, c=1, w=2, xm=m, or any of the standard block size suffixes like k=1024 (see Block size). 

Use different dd invocations to use different block sizes for skipping and I/O. For example, the following shell commands copy data in 512 kB blocks between a disk and a tape, but do not save or restore a 4 kB label at the start of the disk: 

disk=/dev/rdsk/c0t1d0s2
tape=/dev/rmt/0

# Copy all but the label from disk to tape.
(dd bs=4k skip=1 count=0 && dd bs=512k) <$disk >$tape

# Copy from tape back to disk, but leave the disk label alone.
(dd bs=4k seek=1 count=0 && dd bs=512k) <$tape >$disk



if=file 
Read from file instead of standard input. 

of=file 
Write to file instead of standard output. Unless conv=notrunc is given, dd truncates file to zero bytes (or the size specified with seek=). 

ibs=bytes 
Read bytes bytes at a time. 

obs=bytes 
Write bytes bytes at a time. 

bs=bytes 
Both read and write bytes bytes at a time. This overrides ibs and obs. 

cbs=bytes 
Convert bytes bytes at a time. 

skip=blocks 
Skip blocks ibs-byte blocks in the input file before copying. 

seek=blocks 
Skip blocks obs-byte blocks in the output file before copying. 

count=blocks 
Copy blocks ibs-byte blocks from the input file, instead of everything until the end of the file. 

conv=conversion[,conversion]... 
Convert the file as specified by the conversion argument(s). (No spaces around any comma(s).) 
Conversions: 



ascii 
Convert EBCDIC to ASCII. 

ebcdic 
Convert ASCII to EBCDIC. 

ibm 
Convert ASCII to alternate EBCDIC. 

block 
For each line in the input, output cbs bytes, replacing the input newline with a space and padding with spaces as necessary. 

unblock 
Replace trailing spaces in each cbs-sized input block with a newline. 

lcase 
Change uppercase letters to lowercase. 

ucase 
Change lowercase letters to uppercase. 

swab 
Swap every pair of input bytes. GNU dd, unlike others, works when an odd number of bytes are read--the last byte is simply copied (since there is nothing to swap it with). 

noerror 
Continue after read errors. 

notrunc 
Do not truncate the output file. 

sync 
Pad every input block to size of ibs with trailing zero bytes. When use with block or unblock, pad with spaces instead of zero bytes. 


--------------------------------------------------------------------------------
Node:install invocation, Next:mv invocation, Previous:dd invocation, Up:Basic operations 

install: Copy files and set attributes
install copies files while setting their permission modes and, if possible, their owner and group. Synopses: 

install [option]... source dest
install [option]... source... directory
install -d [option]... directory...

In the first of these, the source file is copied to the dest target file. In the second, each of the source files are copied to the destination directory. In the last, each directory (and any missing parent directories) is created. 

install is similar to cp, but allows you to control the attributes of destination files. It is typically used in Makefiles to copy programs into their destination directories. It refuses to copy files onto themselves. 

The program accepts the following options. Also see Common options. 



-b 
--backup[=method] 
See Backup options. Make a backup of each file that would otherwise be overwritten or removed. 

-c 
Ignored; for compatibility with old Unix versions of install. 

-d 
--directory 
Create each given directory and any missing parent directories, setting the owner, group and mode as given on the command line or to the defaults. It also gives any parent directories it creates those attributes. (This is different from the SunOS 4.x install, which gives directories that it creates the default attributes.) 

-g group 
--group=group 
Set the group ownership of installed files or directories to group. The default is the process' current group. group may be either a group name or a numeric group id. 

-m mode 
--mode=mode 
Set the permissions for the installed file or directory to mode, which can be either an octal number, or a symbolic mode as in chmod, with 0 as the point of departure (see File permissions). The default mode is u=rwx,go=rx--read, write, and execute for the owner, and read and execute for group and other. 

-o owner 
--owner=owner 
If install has appropriate privileges (is run as root), set the ownership of installed files or directories to owner. The default is root. owner may be either a user name or a numeric user ID. 

-p 
--preserve-timestamps 
Set the time of last access and the time of last modification of each installed file to match those of each corresponding original file. When a file is installed without this option, its last access and last modification times are both set to the time of installation. This option is useful if you want to use the last modification times of installed files to keep track of when they were last built as opposed to when they were last installed. 

-s 
--strip 
Strip the symbol tables from installed binary executables. 

-S suffix 
--suffix=suffix 
Append suffix to each backup file made with -b. See Backup options. 
--target-directory=directory 
Specify the destination directory. See Target directory. 

-v 
--verbose 
Print the name of each file before copying it. 

-V method 
--version-control=method 
Change the type of backups made with -b. The method argument can be none (or off), numbered (or t), existing (or nil), or never (or simple). See Backup options. 


--------------------------------------------------------------------------------
Node:mv invocation, Next:rm invocation, Previous:install invocation, Up:Basic operations 

mv: Move (rename) files
mv moves or renames files (or directories). Synopsis: 

mv [option]... source dest
mv [option]... source... directory

If the last argument names an existing directory, mv moves each other given file into a file with the same name in that directory. Otherwise, if only two files are given, it renames the first as the second. It is an error if the last argument is not a directory and more than two files are given. 

mv can move any type of file from one filesystem to another. Prior to version 4.0 of the fileutils, mv could move only regular files between filesystems. For example, now mv can move an entire directory hierarchy including special device files from one partition to another. It first uses some of the same code that's used by cp -a to copy the requested directories and files, then (assuming the copy succeeded) it removes the originals. If the copy fails, then the part that was copied to the destination partition is removed. If you were to copy three directories from one partition to another and the copy of the first directory succeeded, but the second didn't, the first would be left on the destination partion and the second and third would be left on the original partition. 

If a destination file exists but is normally unwritable, standard input is a terminal, and the -f or --force option is not given, mv prompts the user for whether to replace the file. (You might own the file, or have write permission on its directory.) If the response does not begin with y or Y, the file is skipped. 

Warning: If you try to move a symlink that points to a directory, and you specify the symlink with a trailing slash, then mv doesn't move the symlink but instead moves the directory referenced by the symlink. See Trailing slashes. 

The program accepts the following options. Also see Common options. 



-b 
--backup[=method] 
See Backup options. Make a backup of each file that would otherwise be overwritten or removed. 

-f 
--force 
Do not prompt the user before removing an unwritable destination file. 

-i 
--interactive 
Prompt whether to overwrite each existing destination file, regardless of its permissions. If the response does not begin with y or Y, the file is skipped. 

-u 
--update 
Do not move a nondirectory that has an existing destination with the same or newer modification time. 

-v 
--verbose 
Print the name of each file before moving it. 
--strip-trailing-slashes 
Remove any trailing slashes from each source argument. See Trailing slashes. 

-S suffix 
--suffix=suffix 
Append suffix to each backup file made with -b. See Backup options. 
--target-directory=directory 
Specify the destination directory. See Target directory. 

-V method 
--version-control=method 
Change the type of backups made with -b. The method argument can be none (or off), numbered (or t), existing (or nil), or never (or simple). See Backup options. 


--------------------------------------------------------------------------------
Node:rm invocation, Next:shred invocation, Previous:mv invocation, Up:Basic operations 

rm: Remove files or directories
rm removes each given file. By default, it does not remove directories. Synopsis: 

rm [option]... [file]...

If a file is unwritable, standard input is a terminal, and the -f or --force option is not given, or the -i or --interactive option is given, rm prompts the user for whether to remove the file. If the response does not begin with y or Y, the file is skipped. 

The program accepts the following options. Also see Common options. 



-d 
--directory 
Attempt to remove directories with unlink instead of rmdir, and don't require a directory to be empty before trying to unlink it. This works only if you have appropriate privileges and if your operating system supports unlink for directories. Because unlinking a directory causes any files in the deleted directory to become unreferenced, it is wise to fsck the filesystem after doing this. 

-f 
--force 
Ignore nonexistent files and never prompt the user. Ignore any previous --interactive (-i) option. 

-i 
--interactive 
Prompt whether to remove each file. If the response does not begin with y or Y, the file is skipped. Ignore any previous --force (-f) option. 

-r 
-R 
--recursive 
Remove the contents of directories recursively. 

-v 
--verbose 
Print the name of each file before removing it. 
One common question is how to remove files whose names begin with a -. GNU rm, like every program that uses the getopt function to parse its arguments, lets you use the -- option to indicate that all following arguments are non-options. To remove a file called -f in the current directory, you could type either: 

rm -- -f

or: 

rm ./-f

The Unix rm program's use of a single - for this purpose predates the development of the getopt standard syntax. 



--------------------------------------------------------------------------------
Node:shred invocation, Previous:rm invocation, Up:Basic operations 

shred: Remove files more securely
shred overwrites devices or files, to help prevent even very expensive hardware from recovering the data. 

Ordinarily when you remove a file (see rm invocation), the data is not actually destroyed. Only the index listing where the file is stored is destroyed, and the storage is made available for reuse. There are undelete utilities that will attempt to reconstruct the index and can bring the file back if the parts were not reused. 

On a busy system with a nearly-full drive, space can get reused in a few seconds. But there is no way to know for sure. If you have sensitive data, you may want to be sure that recovery is not possible by actually overwriting the file with non-sensitive data. 

However, even after doing that, it is possible to take the disk back to a laboratory and use a lot of sensitive (and expensive) equipment to look for the faint "echoes" of the original data underneath the overwritten data. If the data has only been overwritten once, it's not even that hard. 

The best way to remove something irretrievably is to destroy the media it's on with acid, melt it down, or the like. For cheap removable media like floppy disks, this is the preferred method. However, hard drives are expensive and hard to melt, so the shred utility tries to achieve a similar effect non-destructively. 

This uses many overwrite passes, with the data patterns chosen to maximize the damage they do to the old data. While this will work on floppies, the patterns are designed for best effect on hard drives. For more details, see the source code and Peter Gutmann's paper Secure Deletion of Data from Magnetic and Solid-State Memory, from the proceedings of the Sixth USENIX Security Symposium (San Jose, California, 22-25 July, 1996). The paper is also available online <http://www.cs.auckland.ac.nz/~pgut001/pubs/secure_del.html>. 

Please note that shred relies on a very important assumption: that the filesystem overwrites data in place. This is the traditional way to do things, but many modern filesystem designs do not satisfy this assumption. Exceptions include: 

Log-structured or journaled filesystems, such as those supplied with AIX and Solaris. 
Filesystems that write redundant data and carry on even if some writes fail, such as RAID-based filesystems. 
Filesystems that make snapshots, such as Network Appliance's NFS server. 
Filesystems that cache in temporary locations, such as NFS version 3 clients. 
Compressed filesystems. 
If you are not sure how your filesystem operates, then you should assume that it does not overwrite data in place, which means that shred cannot reliably operate on regular files in your filesystem. 

Generally speaking, it is more reliable to shred a device than a file, since this bypasses the problem of filesystem design mentioned above. However, even shredding devices is not always completely reliable. For example, most disks map out bad sectors invisibly to the application; if the bad sectors contain sensitive data, shred won't be able to destroy it. 

shred makes no attempt to detect or report these problem, just as it makes no attempt to do anything about backups. However, since it is more reliable to shred devices than files, shred by default does not truncate or remove the output file. This default is more suitable for devices, which typically cannot be truncated and should not be removed. 

shred [option]... file[...]

The program accepts the following options. Also see Common options. 



-f 
--force 
Override file permissions if necessary to allow overwriting. 

-NUMBER 
-n NUMBER 
--iterations=NUMBER 
By default, shred uses 25 passes of overwrite. This is enough for all of the useful overwrite patterns to be used at least once. You can reduce this to save time, or increase it if you have a lot of time to waste. 

-s BYTES 
--size=BYTES 
Shred the first BYTES bytes of the file. The default is to shred the whole file. BYTES can be followed by a size specification like k, M, or G to specify a multiple. See Block size. 

-u 
--remove 
After shredding a file, truncate it (if possible) and then remove it. If a file has multiple links, only the named links will be removed. 

-v 
--verbose 
Display status updates as sterilization proceeds. 

-x 
--exact 
Normally, shred rounds the file size up to the next multiple of the filesystem block size to fully erase the last block of the file. This option suppresses that behavior. Thus, by default if you shred a 10-byte file on a system with 512-byte blocks, the resulting file will be 512 bytes long. With this option, shred does not increase the size of the file. 

-z 
--zero 
Normally, the last pass that shred writes is made up of random data. If this would be conspicuous on your hard drive (for example, because it looks like encrypted data), or you just think it's tidier, the --zero option adds an additional overwrite pass with all zero bits. This is in addition to the number of passes specified by the --iterations option. 

- 
Shred standard output. 
This argument is considered an option. If the common -- option has been used to indicate the end of options on the command line, then - will be interpreted as an ordinary file name. 

The intended use of this is to shred a removed temporary file. For example 

i=`tempfile -m 0600`
exec 3<>"$i"
rm -- "$i"
echo "Hello, world" >&3
shred - >&3
exec 3>-

Note that the shell command shred - >file does not shred the contents of file, since it truncates file before invoking shred. Use the command shred file or (if using a Bourne-compatible shell) the command shred - 1<>file instead. 

You might use the following command to erase all trace of the file system you'd created on the floppy disk in your first drive. That command takes about 20 minutes to erase a 1.44MB floppy. 

shred --verbose /dev/fd0

Similarly, to erase all data on a selected partition of your hard disk, you could give a command like this: 

shred --verbose /dev/sda5



--------------------------------------------------------------------------------
Node:Special file types, Next:Changing file attributes, Previous:Basic operations, Up:Top 

Special file types
This chapter describes commands which create special types of files (and rmdir, which removes directories, one special file type). 

Although Unix-like operating systems have markedly fewer special file types than others, not everything can be treated only as the undifferentiated byte stream of normal files. For example, when a file is created or removed, the system must record this information, which it does in a directory--a special type of file. Although you can read directories as normal files, if you're curious, in order for the system to do its job it must impose a structure, a certain order, on the bytes of the file. Thus it is a "special" type of file. 

Besides directories, other special file types include named pipes (FIFOs), symbolic links, sockets, and so-called special files. 

ln invocation: Make links between files. 
mkdir invocation: Make directories. 
mkfifo invocation: Make FIFOs (named pipes). 
mknod invocation: Make block or character special files. 
rmdir invocation: Remove empty directories. 


--------------------------------------------------------------------------------
Node:ln invocation, Next:mkdir invocation, Up:Special file types 

ln: Make links between files
ln makes links between files. By default, it makes hard links; with the -s option, it makes symbolic (or soft) links. Synopses: 

ln [option]... target [linkname]
ln [option]... target... directory

If the last argument names an existing directory, ln creates a link to each target file in that directory, using the targets' names. (But see the description of the --no-dereference option below.) 
If two filenames are given, ln creates a link from the second to the first. 
If one target is given, ln creates a link to that file in the current directory. 
It is an error if the last argument is not a directory and more than two files are given. Without -f or -i (see below), ln will not remove an existing file. Use the --backup option to make ln rename existing files. 
A hard link is another name for an existing file; the link and the original are indistinguishable. Technically speaking, they share the same inode, and the inode contains all the information about a file--indeed, it is not incorrect to say that the inode is the file. On all existing implementations, you cannot make a hard link to a directory, and hard links cannot cross filesystem boundaries. (These restrictions are not mandated by POSIX, however.) 

Symbolic links (symlinks for short), on the other hand, are a special file type (which not all kernels support: System V release 3 (and older) systems lack symlinks) in which the link file actually refers to a different file, by name. When most operations (opening, reading, writing, and so on) are passed the symbolic link file, the kernel automatically dereferences the link and operates on the target of the link. But some operations (e.g., removing) work on the link file itself, rather than on its target. See Symbolic Links. 

The program accepts the following options. Also see Common options. 



-b 
--backup[=method] 
See Backup options. Make a backup of each file that would otherwise be overwritten or removed. 

-d 
-F 
--directory 
Allow the super-user to make hard links to directories. 

-f 
--force 
Remove existing destination files. 

-i 
--interactive 
Prompt whether to remove existing destination files. 

-n 
--no-dereference 
When given an explicit destination that is a symlink to a directory, treat that destination as if it were a normal file. 
When the destination is an actual directory (not a symlink to one), there is no ambiguity. The link is created in that directory. But when the specified destination is a symlink to a directory, there are two ways to treat the user's request. ln can treat the destination just as it would a normal directory and create the link in it. On the other hand, the destination can be viewed as a non-directory--as the symlink itself. In that case, ln must delete or backup that symlink before creating the new link. The default is to treat a destination that is a symlink to a directory just like a directory. 


-s 
--symbolic 
Make symbolic links instead of hard links. This option merely produces an error message on systems that do not support symbolic links. 

-S suffix 
--suffix=suffix 
Append suffix to each backup file made with -b. See Backup options. 
--target-directory=directory 
Specify the destination directory. See Target directory. 

-v 
--verbose 
Print the name of each file before linking it. 

-V method 
--version-control=method 
Change the type of backups made with -b. The method argument can be none (or off), numbered (or t), existing (or nil), or never (or simple). See Backup options. 
Examples: 

ln -s /some/name  # creates link ./name pointing to /some/name
ln -s /some/name myname  # creates link ./myname pointing to /some/name
ln -s a b ..      # creates links ../a and ../b pointing to ./a and ./b



--------------------------------------------------------------------------------
Node:mkdir invocation, Next:mkfifo invocation, Previous:ln invocation, Up:Special file types 

mkdir: Make directories
mkdir creates directories with the specified names. Synopsis: 

mkdir [option]... name...

If a name is an existing file but not a directory, mkdir prints a warning message on stderr and will exit with a status of 1 after processing any remaining names. The same is done when a name is an existing directory and the -p option is not given. If a name is an existing directory and the -p option is given, mkdir will ignore it. That is, mkdir will not print a warning, raise an error, or change the mode of the directory (even if the -m option is given), and will move on to processing any remaining names. 

The program accepts the following options. Also see Common options. 



-m mode 
--mode=mode 
Set the mode of created directories to mode, which is symbolic as in chmod and uses a=rwx (read, write and execute allowed for everyone) minus the bits set in the umask for the point of the departure. See File permissions. 

-p 
--parents 
Make any missing parent directories for each argument. The mode for parent directories is set to the umask modified by u+wx. Ignore arguments corresponding to existing directories. 

-v 

--verbose 
Print a message for each created directory. This is most useful with --parents. 


--------------------------------------------------------------------------------
Node:mkfifo invocation, Next:mknod invocation, Previous:mkdir invocation, Up:Special file types 

mkfifo: Make FIFOs (named pipes)
mkfifo creates FIFOs (also called named pipes) with the specified names. Synopsis: 

mkfifo [option] name...

A FIFO is a special file type that permits independent processes to communicate. One process opens the FIFO file for writing, and another for reading, after which data can flow as with the usual anonymous pipe in shells or elsewhere. 

The program accepts the following option. Also see Common options. 



-m mode 
--mode=mode 
Set the mode of created FIFOs to mode, which is symbolic as in chmod and uses a=rw (read and write allowed for everyone) minus the bits set in the umask for the point of departure. See File permissions. 


--------------------------------------------------------------------------------
Node:mknod invocation, Next:rmdir invocation, Previous:mkfifo invocation, Up:Special file types 

mknod: Make block or character special files
mknod creates a FIFO, character special file, or block special file with the specified name. Synopsis: 

mknod [option]... name type [major minor]

Unlike the phrase "special file type" above, the term special file has a technical meaning on Unix: something that can generate or receive data. Usually this corresponds to a physical piece of hardware, e.g., a printer or a disk. (These files are typically created at system-configuration time.) The mknod command is what creates files of this type. Such devices can be read either a character at a time or a "block" (many characters) at a time, hence we say there are block special files and character special files. 

The arguments after name specify the type of file to make: 



p 
for a FIFO 

b 
for a block special file 

c 
for a character special file 
When making a block or character special file, the major and minor device numbers must be given after the file type. 

The program accepts the following option. Also see Common options. 



-m mode 
--mode=mode 
Set the mode of created files to mode, which is symbolic as in chmod and uses a=rw minus the bits set in the umask as the point of departure. See File permissions. 


--------------------------------------------------------------------------------
Node:rmdir invocation, Previous:mknod invocation, Up:Special file types 

rmdir: Remove empty directories
rmdir removes empty directories. Synopsis: 

rmdir [option]... directory...

If any directory argument does not refer to an existing empty directory, it is an error. 

The program accepts the following option. Also see Common options. 



--ignore-fail-on-non-empty 
Ignore each failure to remove a directory that is solely because the directory is non-empty. 

-p 
--parents 
Remove directory, then try to remove each component of directory. So, for example, rmdir -p a/b/c is similar to rmdir a/b/c a/b a. As such, it fails if any of those directories turns out not to be empty. Use the --ignore-fail-on-non-empty option to make it so such a failure does not evoke a diagnostic and does not cause rmdir to exit unsuccessfully. 

-v 

--verbose 
Give a diagnostic for each successful removal. directory is removed. 
See rm invocation, for how to remove non-empty directories (recursively). 



Node:Changing file attributes, Next:Disk usage, Previous:Special file types, Up:Top 

Changing file attributes
A file is not merely its contents, a name, and a file type (see Special file types). A file also has an owner (a userid), a group (a group id), permissions (what the owner can do with the file, what people in the group can do, and what everyone else can do), various timestamps, and other information. Collectively, we call these a file's attributes. 

These commands change file attributes. 

chown invocation: Change file owners and groups. 
chgrp invocation: Change file groups. 
chmod invocation: Change access permissions. 
touch invocation: Change file timestamps. 



Node:chown invocation, Next:chgrp invocation, Up:Changing file attributes 

chown: Change file owner and group
chown changes the user and/or group ownership of each given file to new-owner or to the user and group of an existing reference file. Synopsis: 

GNU Go Development Documentation
Currently GNU Go documentation is too long to keep on line. 
--------------------------------------------------------------------------------
Return to GNU's home page. 
Please send FSF & GNU inquiries & questions to gnu@gnu.org. There are also other ways to contact the FSF. 

Please send comments on these web pages to webmasters@www.gnu.org, send other questions to gnu@gnu.org. 

Copyright (C) 1999 Free Software Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111, USA 

Verbatim copying and distribution of this entire article is permitted in any medium, provided this notice is preserved.

GNU Go Development Documentation
Currently GNU Go documentation is too long to keep on line. 
--------------------------------------------------------------------------------
Return to GNU's home page. 
Please send FSF & GNU inquiries & questions to gnu@gnu.org. There are also other ways to contact the FSF. 

Please send comments on these web pages to webmasters@www.gnu.org, send other questions to gnu@gnu.org. 

Copyright (C) 1999 Free Software Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111, USA 

Verbatim copying and distribution of this entire article is permitted in any medium, provided this notice is preserved.

Everyone is permitted to copy and distribute verbatim copies
of this license document, but changing it is not allowed.

Preamble
The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Library General Public License instead.) You can apply it to your programs, too. 

When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things. 

To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it. 

For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. 

We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software. 

Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations. 

Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all. 

The precise terms and conditions for copying, distribution and modification follow. 

TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The "Program", below, refers to any such program or work, and a "work based on the Program" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term "modification".) Each licensee is addressed as "you". Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does. 
You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. 
You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: 
You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change. 
You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License. 
If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.) 
These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program. In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. 
You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following: 
Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, 
Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, 
Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.) 
The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code. 
You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. 
You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it. 
Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License. 
If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. 
If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. 
The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and "any later version", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation. 
If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. 
NO WARRANTY

BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 
IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 
END OF TERMS AND CONDITIONS
How to Apply These Terms to Your New Programs
If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. 

To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the "copyright" line and a pointer to where the full notice is found. 

one line to give the program's name and an idea of what it does.
Copyright (C) 19yy  name of author

This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License
as published by the Free Software Foundation; either version 2
of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.

Also add information on how to contact you by electronic and paper mail. 

If the program is interactive, make it output a short notice like this when it starts in an interactive mode: 

Gnomovision version 69, Copyright (C) 19yy name of author
Gnomovision comes with ABSOLUTELY NO WARRANTY; for details
type `show w'.  This is free software, and you are welcome
to redistribute it under certain conditions; type `show c' 
for details.

The hypothetical commands `show w' and `show c' should show the appropriate parts of the General Public License. Of course, the commands you use may be called something other than `show w' and `show c'; they could even be mouse-clicks or menu items--whatever suits your program. 

You should also get your employer (if you work as a programmer) or your school, if any, to sign a "copyright disclaimer" for the program, if necessary. Here is a sample; alter the names: 

Yoyodyne, Inc., hereby disclaims all copyright
interest in the program `Gnomovision'
(which makes passes at compilers) written 
by James Hacker.

signature of Ty Coon, 1 April 1989
Ty Coon, President of Vice

This General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Library General Public License instead of this License. 

Overview

gzip reduces the size of the named files using Lempel-Ziv coding (LZ77). Whenever possible, each file is replaced by one with the extension `.gz', while keeping the same ownership modes, access and modification times. (The default extension is `-gz' for VMS, `z' for MSDOS, OS/2 FAT and Atari.) If no files are specified or if a file name is "-", the standard input is compressed to the standard output. gzip will only attempt to compress regular files. In particular, it will ignore symbolic links. 

If the new file name is too long for its file system, gzip truncates it. gzip attempts to truncate only the parts of the file name longer than 3 characters. (A part is delimited by dots.) If the name consists of small parts only, the longest parts are truncated. For example, if file names are limited to 14 characters, gzip.msdos.exe is compressed to gzi.msd.exe.gz. Names are not truncated on systems which do not have a limit on file name length. 

By default, gzip keeps the original file name and timestamp in the compressed file. These are used when decompressing the file with the `-N' option. This is useful when the compressed file name was truncated or when the time stamp was not preserved after a file transfer. 

Compressed files can be restored to their original form using `gzip -d' or gunzip or zcat. If the original name saved in the compressed file is not suitable for its file system, a new name is constructed from the original one to make it legal. 

gunzip takes a list of files on its command line and replaces each file whose name ends with `.gz', `.z', `.Z', `-gz', `-z' or `_z' and which begins with the correct magic number with an uncompressed file without the original extension. gunzip also recognizes the special extensions `.tgz' and `.taz' as shorthands for `.tar.gz' and `.tar.Z' respectively. When compressing, gzip uses the `.tgz' extension if necessary instead of truncating a file with a `.tar' extension. 

gunzip can currently decompress files created by gzip, zip, compress or pack. The detection of the input format is automatic. When using the first two formats, gunzip checks a 32 bit CRC (cyclic redundancy check). For pack, gunzip checks the uncompressed length. The compress format was not designed to allow consistency checks. However gunzip is sometimes able to detect a bad `.Z' file. If you get an error when uncompressing a `.Z' file, do not assume that the `.Z' file is correct simply because the standard uncompress does not complain. This generally means that the standard uncompress does not check its input, and happily generates garbage output. The SCO `compress -H' format (lzh compression method) does not include a CRC but also allows some consistency checks. 

Files created by zip can be uncompressed by gzip only if they have a single member compressed with the 'deflation' method. This feature is only intended to help conversion of tar.zip files to the tar.gz format. To extract zip files with several members, use unzip instead of gunzip. 

zcat is identical to `gunzip -c'. zcat uncompresses either a list of files on the command line or its standard input and writes the uncompressed data on standard output. zcat will uncompress files that have the correct magic number whether they have a `.gz' suffix or not. 

gzip uses the Lempel-Ziv algorithm used in zip and PKZIP. The amount of compression obtained depends on the size of the input and the distribution of common substrings. Typically, text such as source code or English is reduced by 60-70%. Compression is generally much better than that achieved by LZW (as used in compress), Huffman coding (as used in pack), or adaptive Huffman coding (compact). 

Compression is always performed, even if the compressed file is slightly larger than the original. The worst case expansion is a few bytes for the gzip file header, plus 5 bytes every 32K block, or an expansion ratio of 0.015% for large files. Note that the actual number of used disk blocks almost never increases. gzip preserves the mode, ownership and timestamps of files when compressing or decompressing. 

GNU make
This file documents the GNU make utility, which determines automatically which pieces of a large program need to be recompiled, and issues the commands to recompile them. 

This is Edition 0.70, last updated 1 April 2006, of The GNU Make Manual, for GNU make version 3.81. 

Copyright   1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2002, 2003, 2004, 2005, 2006 Free Software Foundation, Inc. 

Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, with the Front-Cover Texts being  A GNU Manual,  and with the Back-Cover Texts as in (a) below. A copy of the license is included in the section entitled  GNU Free Documentation License.  
(a) The FSF's Back-Cover Text is:  You have freedom to copy and modify this GNU Manual, like GNU software. Copies published by the Free Software Foundation raise funds for GNU development.  

Overview: Overview of make. 
Introduction: An introduction to make. 
Makefiles: Makefiles tell make what to do. 
Rules: Rules describe when a file must be remade. 
Commands: Commands say how to remake a file. 
Using Variables: You can use variables to avoid repetition. 
Conditionals: Use or ignore parts of the makefile based on the values of variables. 
Functions: Many powerful ways to manipulate text. 
Invoking make: How to invoke make on the command line. 
Implicit Rules: Use implicit rules to treat many files alike, based on their file names. 
Archives: How make can update library archives. 
Features: Features GNU make has over other makes. 
Missing: What GNU make lacks from other makes. 
Makefile Conventions: Conventions for writing makefiles for GNU programs. 
Quick Reference: A quick reference for experienced users. 
Error Messages: A list of common errors generated by make. 
Complex Makefile: A real example of a straightforward, but nontrivial, makefile. 
GNU Free Documentation License: License for copying this manual 
Concept Index: Index of Concepts 
Name Index: Index of Functions, Variables, & Directives 
--- The Detailed Node Listing --- 

Overview of make 

Preparing: Preparing and Running Make 
Reading: On Reading this Text 
Bugs: Problems and Bugs 
An Introduction to Makefiles 

Rule Introduction: What a rule looks like. 
Simple Makefile: A Simple Makefile 
How Make Works: How make Processes This Makefile 
Variables Simplify: Variables Make Makefiles Simpler 
make Deduces: Letting make Deduce the Commands 
Combine By Prerequisite: Another Style of Makefile 
Cleanup: Rules for Cleaning the Directory 
Writing Makefiles 

Makefile Contents: What makefiles contain. 
Makefile Names: How to name your makefile. 
Include: How one makefile can use another makefile. 
MAKEFILES Variable: The environment can specify extra makefiles. 
MAKEFILE_LIST Variable: Discover which makefiles have been read. 
Special Variables: Other special variables. 
Remaking Makefiles: How makefiles get remade. 
Overriding Makefiles: How to override part of one makefile with another makefile. 
Reading Makefiles: How makefiles are parsed. 
Secondary Expansion: How and when secondary expansion is performed. 
Writing Rules 

Rule Example: An example explained. 
Rule Syntax: General syntax explained. 
Prerequisite Types: There are two types of prerequisites. 
Wildcards: Using wildcard characters such as `*'. 
Directory Search: Searching other directories for source files. 
Phony Targets: Using a target that is not a real file's name. 
Force Targets: You can use a target without commands or prerequisites to mark other targets as phony. 
Empty Targets: When only the date matters and the files are empty. 
Special Targets: Targets with special built-in meanings. 
Multiple Targets: When to make use of several targets in a rule. 
Multiple Rules: How to use several rules with the same target. 
Static Pattern: Static pattern rules apply to multiple targets and can vary the prerequisites according to the target name. 
Double-Colon: How to use a special kind of rule to allow several independent rules for one target. 
Automatic Prerequisites: How to automatically generate rules giving prerequisites from source files themselves. 
Using Wildcard Characters in File Names 

Wildcard Examples: Several examples 
Wildcard Pitfall: Problems to avoid. 
Wildcard Function: How to cause wildcard expansion where it does not normally take place. 
Searching Directories for Prerequisites 

General Search: Specifying a search path that applies to every prerequisite. 
Selective Search: Specifying a search path for a specified class of names. 
Search Algorithm: When and how search paths are applied. 
Commands/Search: How to write shell commands that work together with search paths. 
Implicit/Search: How search paths affect implicit rules. 
Libraries/Search: Directory search for link libraries. 
Static Pattern Rules 

Static Usage: The syntax of static pattern rules. 
Static versus Implicit: When are they better than implicit rules? 
Writing the Commands in Rules 

Command Syntax: Command syntax features and pitfalls. 
Echoing: How to control when commands are echoed. 
Execution: How commands are executed. 
Parallel: How commands can be executed in parallel. 
Errors: What happens after a command execution error. 
Interrupts: What happens when a command is interrupted. 
Recursion: Invoking make from makefiles. 
Sequences: Defining canned sequences of commands. 
Empty Commands: Defining useful, do-nothing commands. 
Command Syntax 

Splitting Lines: Breaking long command lines for readability. 
Variables in Commands: Using make variables in commands. 
Command Execution 

Choosing the Shell: How make chooses the shell used to run commands. 
Recursive Use of make 

MAKE Variable: The special effects of using `$(MAKE)'. 
Variables/Recursion: How to communicate variables to a sub-make. 
Options/Recursion: How to communicate options to a sub-make. 
-w Option: How the `-w' or `--print-directory' option helps debug use of recursive make commands. 
How to Use Variables 

Reference: How to use the value of a variable. 
Flavors: Variables come in two flavors. 
Advanced: Advanced features for referencing a variable. 
Values: All the ways variables get their values. 
Setting: How to set a variable in the makefile. 
Appending: How to append more text to the old value of a variable. 
Override Directive: How to set a variable in the makefile even if the user has set it with a command argument. 
Defining: An alternate way to set a variable to a verbatim string. 
Environment: Variable values can come from the environment. 
Target-specific: Variable values can be defined on a per-target basis. 
Pattern-specific: Target-specific variable values can be applied to a group of targets that match a pattern. 
Advanced Features for Reference to Variables 

Substitution Refs: Referencing a variable with substitutions on the value. 
Computed Names: Computing the name of the variable to refer to. 
Conditional Parts of Makefiles 

Conditional Example: Example of a conditional 
Conditional Syntax: The syntax of conditionals. 
Testing Flags: Conditionals that test flags. 
Functions for Transforming Text 

Syntax of Functions: How to write a function call. 
Text Functions: General-purpose text manipulation functions. 
File Name Functions: Functions for manipulating file names. 
Conditional Functions: Functions that implement conditions. 
Foreach Function: Repeat some text with controlled variation. 
Call Function: Expand a user-defined function. 
Value Function: Return the un-expanded value of a variable. 
Eval Function: Evaluate the arguments as makefile syntax. 
Origin Function: Find where a variable got its value. 
Flavor Function: Find out the flavor of a variable. 
Shell Function: Substitute the output of a shell command. 
Make Control Functions: Functions that control how make runs. 
How to Run make 

Makefile Arguments: How to specify which makefile to use. 
Goals: How to use goal arguments to specify which parts of the makefile to use. 
Instead of Execution: How to use mode flags to specify what kind of thing to do with the commands in the makefile other than simply execute them. 
Avoiding Compilation: How to avoid recompiling certain files. 
Overriding: How to override a variable to specify an alternate compiler and other things. 
Testing: How to proceed past some errors, to test compilation. 
Options Summary: Summary of Options 
Using Implicit Rules 

Using Implicit: How to use an existing implicit rule to get the commands for updating a file. 
Catalogue of Rules: A list of built-in implicit rules. 
Implicit Variables: How to change what predefined rules do. 
Chained Rules: How to use a chain of implicit rules. 
Pattern Rules: How to define new implicit rules. 
Last Resort: How to define commands for rules which cannot find any. 
Suffix Rules: The old-fashioned style of implicit rule. 
Implicit Rule Search: The precise algorithm for applying implicit rules. 
Defining and Redefining Pattern Rules 

Pattern Intro: An introduction to pattern rules. 
Pattern Examples: Examples of pattern rules. 
Automatic Variables: How to use automatic variables in the commands of implicit rules. 
Pattern Match: How patterns match. 
Match-Anything Rules: Precautions you should take prior to defining rules that can match any target file whatever. 
Canceling Rules: How to override or cancel built-in rules. 
Using make to Update Archive Files 

Archive Members: Archive members as targets. 
Archive Update: The implicit rule for archive member targets. 
Archive Pitfalls: Dangers to watch out for when using archives. 
Archive Suffix Rules: You can write a special kind of suffix rule for updating archives. 
Implicit Rule for Archive Member Targets 

Archive Symbols: How to update archive symbol directories. 


--------------------------------------------------------------------------------
Next: Introduction, Previous: Top, Up: Top 
1 Overview of make
The make utility automatically determines which pieces of a large program need to be recompiled, and issues commands to recompile them. This manual describes GNU make, which was implemented by Richard Stallman and Roland McGrath. Development since Version 3.76 has been handled by Paul D. Smith. 

GNU make conforms to section 6.2 of IEEE Standard 1003.2-1992 (POSIX.2). Our examples show C programs, since they are most common, but you can use make with any programming language whose compiler can be run with a shell command. Indeed, make is not limited to programs. You can use it to describe any task where some files must be updated automatically from others whenever the others change. 

Preparing: Preparing and Running Make 
Reading: On Reading this Text 
Bugs: Problems and Bugs 


--------------------------------------------------------------------------------
Next: Reading, Previous: Overview, Up: Overview 
Preparing and Running Make
To prepare to use make, you must write a file called the makefile that describes the relationships among files in your program and provides commands for updating each file. In a program, typically, the executable file is updated from object files, which are in turn made by compiling source files. 

Once a suitable makefile exists, each time you change some source files, this simple shell command: 

     make

suffices to perform all necessary recompilations. The make program uses the makefile data base and the last-modification times of the files to decide which of the files need to be updated. For each of those files, it issues the commands recorded in the data base. 

You can provide command line arguments to make to control which files should be recompiled, or how. See How to Run make. 



--------------------------------------------------------------------------------
Next: Bugs, Previous: Preparing, Up: Overview 
1.1 How to Read This Manual
If you are new to make, or are looking for a general introduction, read the first few sections of each chapter, skipping the later sections. In each chapter, the first few sections contain introductory or general information and the later sections contain specialized or technical information. The exception is the second chapter, An Introduction to Makefiles, all of which is introductory. 

If you are familiar with other make programs, see Features of GNU make, which lists the enhancements GNU make has, and Incompatibilities and Missing Features, which explains the few things GNU make lacks that others have. 

For a quick summary, see Options Summary, Quick Reference, and Special Targets. 



--------------------------------------------------------------------------------
Previous: Reading, Up: Overview 
1.2 Problems and Bugs
If you have problems with GNU make or think you've found a bug, please report it to the developers; we cannot promise to do anything but we might well want to fix it. 

Before reporting a bug, make sure you've actually found a real bug. Carefully reread the documentation and see if it really says you can do what you're trying to do. If it's not clear whether you should be able to do something or not, report that too; it's a bug in the documentation! 

Before reporting a bug or trying to fix it yourself, try to isolate it to the smallest possible makefile that reproduces the problem. Then send us the makefile and the exact results make gave you, including any error or warning messages. Please don't paraphrase these messages: it's best to cut and paste them into your report. When generating this small makefile, be sure to not use any non-free or unusual tools in your commands: you can almost always emulate what such a tool would do with simple shell commands. Finally, be sure to explain what you expected to occur; this will help us decide whether the problem was really in the documentation. 

Once you have a precise problem you can report it in one of two ways. Either send electronic mail to: 

         bug-make@gnu.org

or use our Web-based project management tool, at: 

         http://savannah.gnu.org/projects/make/

In addition to the information above, please be careful to include the version number of make you are using. You can get this information with the command `make --version'. Be sure also to include the type of machine and operating system you are using. One way to obtain this information is by looking at the final lines of output from the command `make --help'. 



--------------------------------------------------------------------------------
Next: Makefiles, Previous: Overview, Up: Top 
2 An Introduction to Makefiles
You need a file called a makefile to tell make what to do. Most often, the makefile tells make how to compile and link a program. In this chapter, we will discuss a simple makefile that describes how to compile and link a text editor which consists of eight C source files and three header files. The makefile can also tell make how to run miscellaneous commands when explicitly asked (for example, to remove certain files as a clean-up operation). To see a more complex example of a makefile, see Complex Makefile. 

When make recompiles the editor, each changed C source file must be recompiled. If a header file has changed, each C source file that includes the header file must be recompiled to be safe. Each compilation produces an object file corresponding to the source file. Finally, if any source file has been recompiled, all the object files, whether newly made or saved from previous compilations, must be linked together to produce the new executable editor. 

Rule Introduction: What a rule looks like. 
Simple Makefile: A Simple Makefile 
How Make Works: How make Processes This Makefile 
Variables Simplify: Variables Make Makefiles Simpler 
make Deduces: Letting make Deduce the Commands 
Combine By Prerequisite: Another Style of Makefile 
Cleanup: Rules for Cleaning the Directory 


--------------------------------------------------------------------------------
Next: Simple Makefile, Previous: Introduction, Up: Introduction 
2.1 What a Rule Looks Like
A simple makefile consists of  rules  with the following shape: 


     target ... : prerequisites ...
             command
             ...
             ...

A target is usually the name of a file that is generated by a program; examples of targets are executable or object files. A target can also be the name of an action to carry out, such as `clean' (see Phony Targets). 

A prerequisite is a file that is used as input to create the target. A target often depends on several files. 

A command is an action that make carries out. A rule may have more than one command, each on its own line. Please note: you need to put a tab character at the beginning of every command line! This is an obscurity that catches the unwary. 

Usually a command is in a rule with prerequisites and serves to create a target file if any of the prerequisites change. However, the rule that specifies commands for the target need not have prerequisites. For example, the rule containing the delete command associated with the target `clean' does not have prerequisites. 

A rule, then, explains how and when to remake certain files which are the targets of the particular rule. make carries out the commands on the prerequisites to create or update the target. A rule can also explain how and when to carry out an action. See Writing Rules. 

A makefile may contain other text besides rules, but a simple makefile need only contain rules. Rules may look somewhat more complicated than shown in this template, but all fit the pattern more or less. 



--------------------------------------------------------------------------------
Next: How Make Works, Previous: Rule Introduction, Up: Introduction 
2.2 A Simple Makefile
Here is a straightforward makefile that describes the way an executable file called edit depends on eight object files which, in turn, depend on eight C source and three header files. 

In this example, all the C files include defs.h, but only those defining editing commands include command.h, and only low level files that change the editor buffer include buffer.h. 

     edit : main.o kbd.o command.o display.o \
            insert.o search.o files.o utils.o
             cc -o edit main.o kbd.o command.o display.o \
                        insert.o search.o files.o utils.o
     
     main.o : main.c defs.h
             cc -c main.c
     kbd.o : kbd.c defs.h command.h
             cc -c kbd.c
     command.o : command.c defs.h command.h
             cc -c command.c
     display.o : display.c defs.h buffer.h
             cc -c display.c
     insert.o : insert.c defs.h buffer.h
             cc -c insert.c
     search.o : search.c defs.h buffer.h
             cc -c search.c
     files.o : files.c defs.h buffer.h command.h
             cc -c files.c
     utils.o : utils.c defs.h
             cc -c utils.c
     clean :
             rm edit main.o kbd.o command.o display.o \
                insert.o search.o files.o utils.o

We split each long line into two lines using backslash-newline; this is like using one long line, but is easier to read. To use this makefile to create the executable file called edit, type: 

     make

To use this makefile to delete the executable file and all the object files from the directory, type: 

     make clean

In the example makefile, the targets include the executable file `edit', and the object files `main.o' and `kbd.o'. The prerequisites are files such as `main.c' and `defs.h'. In fact, each `.o' file is both a target and a prerequisite. Commands include `cc -c main.c' and `cc -c kbd.c'. 

When a target is a file, it needs to be recompiled or relinked if any of its prerequisites change. In addition, any prerequisites that are themselves automatically generated should be updated first. In this example, edit depends on each of the eight object files; the object file main.o depends on the source file main.c and on the header file defs.h. 

A shell command follows each line that contains a target and prerequisites. These shell commands say how to update the target file. A tab character must come at the beginning of every command line to distinguish command lines from other lines in the makefile. (Bear in mind that make does not know anything about how the commands work. It is up to you to supply commands that will update the target file properly. All make does is execute the commands in the rule you have specified when the target file needs to be updated.) The target `clean' is not a file, but merely the name of an action. Since you normally do not want to carry out the actions in this rule, `clean' is not a prerequisite of any other rule. Consequently, make never does anything with it unless you tell it specifically. Note that this rule not only is not a prerequisite, it also does not have any prerequisites, so the only purpose of the rule is to run the specified commands. Targets that do not refer to files but are just actions are called phony targets. See Phony Targets, for information about this kind of target. See Errors in Commands, to see how to cause make to ignore errors from rm or any other command. 



--------------------------------------------------------------------------------
Next: Variables Simplify, Previous: Simple Makefile, Up: Introduction 
2.3 How make Processes a Makefile
By default, make starts with the first target (not targets whose names start with `.'). This is called the default goal. (Goals are the targets that make strives ultimately to update. You can override this behavior using the command line (see Arguments to Specify the Goals) or with the .DEFAULT_GOAL special variable (see Other Special Variables). In the simple example of the previous section, the default goal is to update the executable program edit; therefore, we put that rule first. 

Thus, when you give the command: 

     make

make reads the makefile in the current directory and begins by processing the first rule. In the example, this rule is for relinking edit; but before make can fully process this rule, it must process the rules for the files that edit depends on, which in this case are the object files. Each of these files is processed according to its own rule. These rules say to update each `.o' file by compiling its source file. The recompilation must be done if the source file, or any of the header files named as prerequisites, is more recent than the object file, or if the object file does not exist. 

The other rules are processed because their targets appear as prerequisites of the goal. If some other rule is not depended on by the goal (or anything it depends on, etc.), that rule is not processed, unless you tell make to do so (with a command such as make clean). 

Before recompiling an object file, make considers updating its prerequisites, the source file and header files. This makefile does not specify anything to be done for them the `.c' and `.h' files are not the targets of any rules so make does nothing for these files. But make would update automatically generated C programs, such as those made by Bison or Yacc, by their own rules at this time. 

After recompiling whichever object files need it, make decides whether to relink edit. This must be done if the file edit does not exist, or if any of the object files are newer than it. If an object file was just recompiled, it is now newer than edit, so edit is relinked. Thus, if we change the file insert.c and run make, make will compile that file to update insert.o, and then link edit. If we change the file command.h and run make, make will recompile the object files kbd.o, command.o and files.o and then link the file edit. 



--------------------------------------------------------------------------------
Next: make Deduces, Previous: How Make Works, Up: Introduction 
2.4 Variables Make Makefiles Simpler
In our example, we had to list all the object files twice in the rule for edit (repeated here): 

     edit : main.o kbd.o command.o display.o \
                   insert.o search.o files.o utils.o
             cc -o edit main.o kbd.o command.o display.o \
                        insert.o search.o files.o utils.o

Such duplication is error-prone; if a new object file is added to the system, we might add it to one list and forget the other. We can eliminate the risk and simplify the makefile by using a variable. Variables allow a text string to be defined once and substituted in multiple places later (see How to Use Variables). 

It is standard practice for every makefile to have a variable named objects, OBJECTS, objs, OBJS, obj, or OBJ which is a list of all object file names. We would define such a variable objects with a line like this in the makefile: 

     objects = main.o kbd.o command.o display.o \
               insert.o search.o files.o utils.o

Then, each place we want to put a list of the object file names, we can substitute the variable's value by writing `$(objects)' (see How to Use Variables). 

Here is how the complete simple makefile looks when you use a variable for the object files: 

     objects = main.o kbd.o command.o display.o \
               insert.o search.o files.o utils.o
     
     edit : $(objects)
             cc -o edit $(objects)
     main.o : main.c defs.h
             cc -c main.c
     kbd.o : kbd.c defs.h command.h
             cc -c kbd.c
     command.o : command.c defs.h command.h
             cc -c command.c
     display.o : display.c defs.h buffer.h
             cc -c display.c
     insert.o : insert.c defs.h buffer.h
             cc -c insert.c
     search.o : search.c defs.h buffer.h
             cc -c search.c
     files.o : files.c defs.h buffer.h command.h
             cc -c files.c
     utils.o : utils.c defs.h
             cc -c utils.c
     clean :
             rm edit $(objects)



--------------------------------------------------------------------------------
Next: Combine By Prerequisite, Previous: Variables Simplify, Up: Introduction 
2.5 Letting make Deduce the Commands
It is not necessary to spell out the commands for compiling the individual C source files, because make can figure them out: it has an implicit rule for updating a `.o' file from a correspondingly named `.c' file using a `cc -c' command. For example, it will use the command `cc -c main.c -o main.o' to compile main.c into main.o. We can therefore omit the commands from the rules for the object files. See Using Implicit Rules. 

When a `.c' file is used automatically in this way, it is also automatically added to the list of prerequisites. We can therefore omit the `.c' files from the prerequisites, provided we omit the commands. 

Here is the entire example, with both of these changes, and a variable objects as suggested above: 

     objects = main.o kbd.o command.o display.o \
               insert.o search.o files.o utils.o
     
     edit : $(objects)
             cc -o edit $(objects)
     
     main.o : defs.h
     kbd.o : defs.h command.h
     command.o : defs.h command.h
     display.o : defs.h buffer.h
     insert.o : defs.h buffer.h
     search.o : defs.h buffer.h
     files.o : defs.h buffer.h command.h
     utils.o : defs.h
     
     .PHONY : clean
     clean :
             rm edit $(objects)

This is how we would write the makefile in actual practice. (The complications associated with `clean' are described elsewhere. See Phony Targets, and Errors in Commands.) 

Because implicit rules are so convenient, they are important. You will see them used frequently. 



--------------------------------------------------------------------------------
Next: Cleanup, Previous: make Deduces, Up: Introduction 
2.6 Another Style of Makefile
When the objects of a makefile are created only by implicit rules, an alternative style of makefile is possible. In this style of makefile, you group entries by their prerequisites instead of by their targets. Here is what one looks like: 

     objects = main.o kbd.o command.o display.o \
               insert.o search.o files.o utils.o
     
     edit : $(objects)
             cc -o edit $(objects)
     
     $(objects) : defs.h
     kbd.o command.o files.o : command.h
     display.o insert.o search.o files.o : buffer.h

Here defs.h is given as a prerequisite of all the object files; command.h and buffer.h are prerequisites of the specific object files listed for them. 

Whether this is better is a matter of taste: it is more compact, but some people dislike it because they find it clearer to put all the information about each target in one place. 



--------------------------------------------------------------------------------
Previous: Combine By Prerequisite, Up: Introduction 
2.7 Rules for Cleaning the Directory
Compiling a program is not the only thing you might want to write rules for. Makefiles commonly tell how to do a few other things besides compiling a program: for example, how to delete all the object files and executables so that the directory is `clean'. 

Here is how we could write a make rule for cleaning our example editor: 

     clean:
             rm edit $(objects)

In practice, we might want to write the rule in a somewhat more complicated manner to handle unanticipated situations. We would do this: 

     .PHONY : clean
     clean :
             -rm edit $(objects)

This prevents make from getting confused by an actual file called clean and causes it to continue in spite of errors from rm. (See Phony Targets, and Errors in Commands.) 

A rule such as this should not be placed at the beginning of the makefile, because we do not want it to run by default! Thus, in the example makefile, we want the rule for edit, which recompiles the editor, to remain the default goal. 

Since clean is not a prerequisite of edit, this rule will not run at all if we give the command `make' with no arguments. In order to make the rule run, we have to type `make clean'. See How to Run make. 



--------------------------------------------------------------------------------
Next: Rules, Previous: Introduction, Up: Top 
3 Writing Makefiles
The information that tells make how to recompile a system comes from reading a data base called the makefile. 

Makefile Contents: What makefiles contain. 
Makefile Names: How to name your makefile. 
Include: How one makefile can use another makefile. 
MAKEFILES Variable: The environment can specify extra makefiles. 
MAKEFILE_LIST Variable: Discover which makefiles have been read. 
Special Variables: Other special variables. 
Remaking Makefiles: How makefiles get remade. 
Overriding Makefiles: How to override part of one makefile with another makefile. 
Reading Makefiles: How makefiles are parsed. 
Secondary Expansion: How and when secondary expansion is performed. 


--------------------------------------------------------------------------------
Next: Makefile Names, Previous: Makefiles, Up: Makefiles 
3.1 What Makefiles Contain
Makefiles contain five kinds of things: explicit rules, implicit rules, variable definitions, directives, and comments. Rules, variables, and directives are described at length in later chapters. 

An explicit rule says when and how to remake one or more files, called the rule's targets. It lists the other files that the targets depend on, called the prerequisites of the target, and may also give commands to use to create or update the targets. See Writing Rules. 

An implicit rule says when and how to remake a class of files based on their names. It describes how a target may depend on a file with a name similar to the target and gives commands to create or update such a target. See Using Implicit Rules. 

A variable definition is a line that specifies a text string value for a variable that can be substituted into the text later. The simple makefile example shows a variable definition for objects as a list of all object files (see Variables Make Makefiles Simpler). 

A directive is a command for make to do something special while reading the makefile. These include: 
Reading another makefile (see Including Other Makefiles). 
Deciding (based on the values of variables) whether to use or ignore a part of the makefile (see Conditional Parts of Makefiles). 
Defining a variable from a verbatim string containing multiple lines (see Defining Variables Verbatim). 

`#' in a line of a makefile starts a comment. It and the rest of the line are ignored, except that a trailing backslash not escaped by another backslash will continue the comment across multiple lines. A line containing just a comment (with perhaps spaces before it) is effectively blank, and is ignored. If you want a literal #, escape it with a backslash (e.g., \#). Comments may appear on any line in the makefile, although they are treated specially in certain situations. 
Within a command script (if the line begins with a TAB character) the entire line is passed to the shell, just as with any other line that begins with a TAB. The shell decides how to interpret the text: whether or not this is a comment is up to the shell. 

Within a define directive, comments are not ignored during the definition of the variable, but rather kept intact in the value of the variable. When the variable is expanded they will either be treated as make comments or as command script text, depending on the context in which the variable is evaluated. 



--------------------------------------------------------------------------------
Next: Include, Previous: Makefile Contents, Up: Makefiles 
3.2 What Name to Give Your Makefile
By default, when make looks for the makefile, it tries the following names, in order: GNUmakefile, makefile and Makefile. Normally you should call your makefile either makefile or Makefile. (We recommend Makefile because it appears prominently near the beginning of a directory listing, right near other important files such as README.) The first name checked, GNUmakefile, is not recommended for most makefiles. You should use this name if you have a makefile that is specific to GNU make, and will not be understood by other versions of make. Other make programs look for makefile and Makefile, but not GNUmakefile. 

If make finds none of these names, it does not use any makefile. Then you must specify a goal with a command argument, and make will attempt to figure out how to remake it using only its built-in implicit rules. See Using Implicit Rules. 

If you want to use a nonstandard name for your makefile, you can specify the makefile name with the `-f' or `--file' option. The arguments `-f name' or `--file=name' tell make to read the file name as the makefile. If you use more than one `-f' or `--file' option, you can specify several makefiles. All the makefiles are effectively concatenated in the order specified. The default makefile names GNUmakefile, makefile and Makefile are not checked automatically if you specify `-f' or `--file'. 



--------------------------------------------------------------------------------
Next: MAKEFILES Variable, Previous: Makefile Names, Up: Makefiles 
3.3 Including Other Makefiles
The include directive tells make to suspend reading the current makefile and read one or more other makefiles before continuing. The directive is a line in the makefile that looks like this: 

     include filenames...

filenames can contain shell file name patterns. If filenames is empty, nothing is included and no error is printed. Extra spaces are allowed and ignored at the beginning of the line, but a tab is not allowed. (If the line begins with a tab, it will be considered a command line.) Whitespace is required between include and the file names, and between file names; extra whitespace is ignored there and at the end of the directive. A comment starting with `#' is allowed at the end of the line. If the file names contain any variable or function references, they are expanded. See How to Use Variables. 

For example, if you have three .mk files, a.mk, b.mk, and c.mk, and $(bar) expands to bish bash, then the following expression 

     include foo *.mk $(bar)

is equivalent to 

     include foo a.mk b.mk c.mk bish bash

When make processes an include directive, it suspends reading of the containing makefile and reads from each listed file in turn. When that is finished, make resumes reading the makefile in which the directive appears. 

One occasion for using include directives is when several programs, handled by individual makefiles in various directories, need to use a common set of variable definitions (see Setting Variables) or pattern rules (see Defining and Redefining Pattern Rules). 

Another such occasion is when you want to generate prerequisites from source files automatically; the prerequisites can be put in a file that is included by the main makefile. This practice is generally cleaner than that of somehow appending the prerequisites to the end of the main makefile as has been traditionally done with other versions of make. See Automatic Prerequisites. If the specified name does not start with a slash, and the file is not found in the current directory, several other directories are searched. First, any directories you have specified with the `-I' or `--include-dir' option are searched (see Summary of Options). Then the following directories (if they exist) are searched, in this order: prefix/include (normally /usr/local/include 1) /usr/gnu/include, /usr/local/include, /usr/include. 

If an included makefile cannot be found in any of these directories, a warning message is generated, but it is not an immediately fatal error; processing of the makefile containing the include continues. Once it has finished reading makefiles, make will try to remake any that are out of date or don't exist. See How Makefiles Are Remade. Only after it has tried to find a way to remake a makefile and failed, will make diagnose the missing makefile as a fatal error. 

If you want make to simply ignore a makefile which does not exist and cannot be remade, with no error message, use the -include directive instead of include, like this: 

     -include filenames...

This acts like include in every way except that there is no error (not even a warning) if any of the filenames do not exist. For compatibility with some other make implementations, sinclude is another name for -include. 



--------------------------------------------------------------------------------
Next: MAKEFILE_LIST Variable, Previous: Include, Up: Makefiles 
3.4 The Variable MAKEFILES
If the environment variable MAKEFILES is defined, make considers its value as a list of names (separated by whitespace) of additional makefiles to be read before the others. This works much like the include directive: various directories are searched for those files (see Including Other Makefiles). In addition, the default goal is never taken from one of these makefiles and it is not an error if the files listed in MAKEFILES are not found. 

The main use of MAKEFILES is in communication between recursive invocations of make (see Recursive Use of make). It usually is not desirable to set the environment variable before a top-level invocation of make, because it is usually better not to mess with a makefile from outside. However, if you are running make without a specific makefile, a makefile in MAKEFILES can do useful things to help the built-in implicit rules work better, such as defining search paths (see Directory Search). 

Some users are tempted to set MAKEFILES in the environment automatically on login, and program makefiles to expect this to be done. This is a very bad idea, because such makefiles will fail to work if run by anyone else. It is much better to write explicit include directives in the makefiles. See Including Other Makefiles. 



--------------------------------------------------------------------------------
Next: Special Variables, Previous: MAKEFILES Variable, Up: Makefiles 
3.5 The Variable MAKEFILE_LIST
As make reads various makefiles, including any obtained from the MAKEFILES variable, the command line, the default files, or from include directives, their names will be automatically appended to the MAKEFILE_LIST variable. They are added right before make begins to parse them. 

This means that if the first thing a makefile does is examine the last word in this variable, it will be the name of the current makefile. Once the current makefile has used include, however, the last word will be the just-included makefile. 

If a makefile named Makefile has this content: 

     name1 := $(lastword $(MAKEFILE_LIST))
     
     include inc.mk
     
     name2 := $(lastword $(MAKEFILE_LIST))
     
     all:
             @echo name1 = $(name1)
             @echo name2 = $(name2)

then you would expect to see this output: 

     name1 = Makefile
     name2 = inc.mk

See Text Functions, for more information on the word and words functions used above. See The Two Flavors of Variables, for more information on simply-expanded (:=) variable definitions. 



--------------------------------------------------------------------------------
Next: Remaking Makefiles, Previous: MAKEFILE_LIST Variable, Up: Makefiles 
3.6 Other Special Variables
GNU make also supports other special variables. Unless otherwise documented here, these values lose their special properties if they are set by a makefile or on the command line. 

.DEFAULT_GOAL
Sets the default goal to be used if no targets were specified on the command line (see Arguments to Specify the Goals). The .DEFAULT_GOAL variable allows you to discover the current default goal, restart the default goal selection algorithm by clearing its value, or to explicitly set the default goal. The following example illustrates these cases: 
          # Query the default goal.
          ifeq ($(.DEFAULT_GOAL),)
            $(warning no default goal is set)
          endif
          
          .PHONY: foo
          foo: ; @echo $@
          
          $(warning default goal is $(.DEFAULT_GOAL))
          
          # Reset the default goal.
          .DEFAULT_GOAL :=
          
          .PHONY: bar
          bar: ; @echo $@
          
          $(warning default goal is $(.DEFAULT_GOAL))
          
          # Set our own.
          .DEFAULT_GOAL := foo
     
This makefile prints: 

          no default goal is set
          default goal is foo
          default goal is bar
          foo
     
Note that assigning more than one target name to .DEFAULT_GOAL is illegal and will result in an error. 




MAKE_RESTARTS
This variable is set only if this instance of make has restarted (see How Makefiles Are Remade): it will contain the number of times this instance has restarted. Note this is not the same as recursion (counted by the MAKELEVEL variable). You should not set, modify, or export this variable. 



.VARIABLES
Expands to a list of the names of all global variables defined so far. This includes variables which have empty values, as well as built-in variables (see Variables Used by Implicit Rules), but does not include any variables which are only defined in a target-specific context. Note that any value you assign to this variable will be ignored; it will always return its special value. 



.FEATURES
Expands to a list of special features supported by this version of make. Possible values include: 
`archives'
Supports ar (archive) files using special filename syntax. See Using make to Update Archive Files. 

`check-symlink'
Supports the -L (--check-symlink-times) flag. See Summary of Options. 

`else-if'
Supports  else if  non-nested conditionals. See Syntax of Conditionals. 

`jobserver'
Supports  job server  enhanced parallel builds. See Parallel Execution. 

`second-expansion'
Supports secondary expansion of prerequisite lists. 

`order-only'
Supports order-only prerequisites. See Types of Prerequisites. 

`target-specific'
Supports target-specific and pattern-specific variable assignments. See Target-specific Variable Values. 



.INCLUDE_DIRS
Expands to a list of directories that make searches for included makefiles (see Including Other Makefiles). 


--------------------------------------------------------------------------------
Next: Overriding Makefiles, Previous: Special Variables, Up: Makefiles 
3.7 How Makefiles Are Remade
Sometimes makefiles can be remade from other files, such as RCS or SCCS files. If a makefile can be remade from other files, you probably want make to get an up-to-date version of the makefile to read in. 

To this end, after reading in all makefiles, make will consider each as a goal target and attempt to update it. If a makefile has a rule which says how to update it (found either in that very makefile or in another one) or if an implicit rule applies to it (see Using Implicit Rules), it will be updated if necessary. After all makefiles have been checked, if any have actually been changed, make starts with a clean slate and reads all the makefiles over again. (It will also attempt to update each of them over again, but normally this will not change them again, since they are already up to date.) 

If you know that one or more of your makefiles cannot be remade and you want to keep make from performing an implicit rule search on them, perhaps for efficiency reasons, you can use any normal method of preventing implicit rule lookup to do so. For example, you can write an explicit rule with the makefile as the target, and an empty command string (see Using Empty Commands). 

If the makefiles specify a double-colon rule to remake a file with commands but no prerequisites, that file will always be remade (see Double-Colon). In the case of makefiles, a makefile that has a double-colon rule with commands but no prerequisites will be remade every time make is run, and then again after make starts over and reads the makefiles in again. This would cause an infinite loop: make would constantly remake the makefile, and never do anything else. So, to avoid this, make will not attempt to remake makefiles which are specified as targets of a double-colon rule with commands but no prerequisites. 

If you do not specify any makefiles to be read with `-f' or `--file' options, make will try the default makefile names; see What Name to Give Your Makefile. Unlike makefiles explicitly requested with `-f' or `--file' options, make is not certain that these makefiles should exist. However, if a default makefile does not exist but can be created by running make rules, you probably want the rules to be run so that the makefile can be used. 

Therefore, if none of the default makefiles exists, make will try to make each of them in the same order in which they are searched for (see What Name to Give Your Makefile) until it succeeds in making one, or it runs out of names to try. Note that it is not an error if make cannot find or make any makefile; a makefile is not always necessary. 

When you use the `-t' or `--touch' option (see Instead of Executing the Commands), you would not want to use an out-of-date makefile to decide which targets to touch. So the `-t' option has no effect on updating makefiles; they are really updated even if `-t' is specified. Likewise, `-q' (or `--question') and `-n' (or `--just-print') do not prevent updating of makefiles, because an out-of-date makefile would result in the wrong output for other targets. Thus, `make -f mfile -n foo' will update mfile, read it in, and then print the commands to update foo and its prerequisites without running them. The commands printed for foo will be those specified in the updated contents of mfile. 

However, on occasion you might actually wish to prevent updating of even the makefiles. You can do this by specifying the makefiles as goals in the command line as well as specifying them as makefiles. When the makefile name is specified explicitly as a goal, the options `-t' and so on do apply to them. 

Thus, `make -f mfile -n mfile foo' would read the makefile mfile, print the commands needed to update it without actually running them, and then print the commands needed to update foo without running them. The commands for foo will be those specified by the existing contents of mfile. 



--------------------------------------------------------------------------------
Next: Reading Makefiles, Previous: Remaking Makefiles, Up: Makefiles 
3.8 Overriding Part of Another Makefile
Sometimes it is useful to have a makefile that is mostly just like another makefile. You can often use the `include' directive to include one in the other, and add more targets or variable definitions. However, if the two makefiles give different commands for the same target, make will not let you just do this. But there is another way. 

In the containing makefile (the one that wants to include the other), you can use a match-anything pattern rule to say that to remake any target that cannot be made from the information in the containing makefile, make should look in another makefile. See Pattern Rules, for more information on pattern rules. 

For example, if you have a makefile called Makefile that says how to make the target `foo' (and other targets), you can write a makefile called GNUmakefile that contains: 

     foo:
             frobnicate > foo
     
     %: force
             @$(MAKE) -f Makefile $@
     force: ;

If you say `make foo', make will find GNUmakefile, read it, and see that to make foo, it needs to run the command `frobnicate > foo'. If you say `make bar', make will find no way to make bar in GNUmakefile, so it will use the commands from the pattern rule: `make -f Makefile bar'. If Makefile provides a rule for updating bar, make will apply the rule. And likewise for any other target that GNUmakefile does not say how to make. 

The way this works is that the pattern rule has a pattern of just `%', so it matches any target whatever. The rule specifies a prerequisite force, to guarantee that the commands will be run even if the target file already exists. We give force target empty commands to prevent make from searching for an implicit rule to build it otherwise it would apply the same match-anything rule to force itself and create a prerequisite loop! 



--------------------------------------------------------------------------------
Next: Secondary Expansion, Previous: Overriding Makefiles, Up: Makefiles 
3.9 How make Reads a Makefile
GNU make does its work in two distinct phases. During the first phase it reads all the makefiles, included makefiles, etc. and internalizes all the variables and their values, implicit and explicit rules, and constructs a dependency graph of all the targets and their prerequisites. During the second phase, make uses these internal structures to determine what targets will need to be rebuilt and to invoke the rules necessary to do so. 

It's important to understand this two-phase approach because it has a direct impact on how variable and function expansion happens; this is often a source of some confusion when writing makefiles. Here we will present a summary of the phases in which expansion happens for different constructs within the makefile. We say that expansion is immediate if it happens during the first phase: in this case make will expand any variables or functions in that section of a construct as the makefile is parsed. We say that expansion is deferred if expansion is not performed immediately. Expansion of deferred construct is not performed until either the construct appears later in an immediate context, or until the second phase. 

You may not be familiar with some of these constructs yet. You can reference this section as you become familiar with them, in later chapters. 

Variable Assignment
Variable definitions are parsed as follows: 

     immediate = deferred
     immediate ?= deferred
     immediate := immediate
     immediate += deferred or immediate
     
     define immediate
       deferred
     endef

For the append operator, `+=', the right-hand side is considered immediate if the variable was previously set as a simple variable (`:='), and deferred otherwise. 

Conditional Statements
All instances of conditional syntax are parsed immediately, in their entirety; this includes the ifdef, ifeq, ifndef, and ifneq forms. Of course this means that automatic variables cannot be used in conditional statements, as automatic variables are not set until the command script for that rule is invoked. If you need to use automatic variables in a conditional you must use shell conditional syntax, in your command script proper, for these tests, not make conditionals. 

Rule Definition
A rule is always expanded the same way, regardless of the form: 

     immediate : immediate ; deferred
     	deferred

That is, the target and prerequisite sections are expanded immediately, and the commands used to construct the target are always deferred. This general rule is true for explicit rules, pattern rules, suffix rules, static pattern rules, and simple prerequisite definitions. 



--------------------------------------------------------------------------------
Previous: Reading Makefiles, Up: Makefiles 
3.10 Secondary Expansion
In the previous section we learned that GNU make works in two distinct phases: a read-in phase and a target-update phase (see How make Reads a Makefile). GNU make also has the ability to enable a second expansion of the prerequisites (only) for some or all targets defined in the makefile. In order for this second expansion to occur, the special target .SECONDEXPANSION must be defined before the first prerequisite list that makes use of this feature. 

If that special target is defined then in between the two phases mentioned above, right at the end of the read-in phase, all the prerequisites of the targets defined after the special target are expanded a second time. In most circumstances this secondary expansion will have no effect, since all variable and function references will have been expanded during the initial parsing of the makefiles. In order to take advantage of the secondary expansion phase of the parser, then, it's necessary to escape the variable or function reference in the makefile. In this case the first expansion merely un-escapes the reference but doesn't expand it, and expansion is left to the secondary expansion phase. For example, consider this makefile: 

     .SECONDEXPANSION:
     ONEVAR = onefile
     TWOVAR = twofile
     myfile: $(ONEVAR) $$(TWOVAR)

After the first expansion phase the prerequisites list of the myfile target will be onefile and $(TWOVAR); the first (unescaped) variable reference to ONEVAR is expanded, while the second (escaped) variable reference is simply unescaped, without being recognized as a variable reference. Now during the secondary expansion the first word is expanded again but since it contains no variable or function references it remains the static value onefile, while the second word is now a normal reference to the variable TWOVAR, which is expanded to the value twofile. The final result is that there are two prerequisites, onefile and twofile. 

Obviously, this is not a very interesting case since the same result could more easily have been achieved simply by having both variables appear, unescaped, in the prerequisites list. One difference becomes apparent if the variables are reset; consider this example: 

     .SECONDEXPANSION:
     AVAR = top
     onefile: $(AVAR)
     twofile: $$(AVAR)
     AVAR = bottom

Here the prerequisite of onefile will be expanded immediately, and resolve to the value top, while the prerequisite of twofile will not be full expanded until the secondary expansion and yield a value of bottom. 

This is marginally more exciting, but the true power of this feature only becomes apparent when you discover that secondary expansions always take place within the scope of the automatic variables for that target. This means that you can use variables such as $@, $*, etc. during the second expansion and they will have their expected values, just as in the command script. All you have to do is defer the expansion by escaping the $. Also, secondary expansion occurs for both explicit and implicit (pattern) rules. Knowing this, the possible uses for this feature increase dramatically. For example: 

     .SECONDEXPANSION:
     main_OBJS := main.o try.o test.o
     lib_OBJS := lib.o api.o
     
     main lib: $$($$@_OBJS)

Here, after the initial expansion the prerequisites of both the main and lib targets will be $($@_OBJS). During the secondary expansion, the $@ variable is set to the name of the target and so the expansion for the main target will yield $(main_OBJS), or main.o try.o test.o, while the secondary expansion for the lib target will yield $(lib_OBJS), or lib.o api.o. 

You can also mix functions here, as long as they are properly escaped: 

     main_SRCS := main.c try.c test.c
     lib_SRCS := lib.c api.c
     
     .SECONDEXPANSION:
     main lib: $$(patsubst %.c,%.o,$$($$@_SRCS))

This version allows users to specify source files rather than object files, but gives the same resulting prerequisites list as the previous example. 

Evaluation of automatic variables during the secondary expansion phase, especially of the target name variable $$@, behaves similarly to evaluation within command scripts. However, there are some subtle differences and  corner cases  which come into play for the different types of rule definitions that make understands. The subtleties of using the different automatic variables are described below. 

Secondary Expansion of Explicit Rules
During the secondary expansion of explicit rules, $$@ and $$% evaluate, respectively, to the file name of the target and, when the target is an archive member, the target member name. The $$< variable evaluates to the first prerequisite in the first rule for this target. $$^ and $$+ evaluate to the list of all prerequisites of rules that have already appeared for the same target ($$+ with repetitions and $$^ without). The following example will help illustrate these behaviors: 

     .SECONDEXPANSION:
     
     foo: foo.1 bar.1 $$< $$^ $$+    # line #1
     
     foo: foo.2 bar.2 $$< $$^ $$+    # line #2
     
     foo: foo.3 bar.3 $$< $$^ $$+    # line #3

In the first prerequisite list, all three variables ($$<, $$^, and $$+) expand to the empty string. In the second, they will have values foo.1, foo.1 bar.1, and foo.1 bar.1 respectively. In the third they will have values foo.1, foo.1 bar.1 foo.2 bar.2, and foo.1 bar.1 foo.2 bar.2 respectively. 

Rules undergo secondary expansion in makefile order, except that the rule with the command script is always evaluated last. 

The variables $$? and $$* are not available and expand to the empty string. 

Secondary Expansion of Static Pattern Rules
Rules for secondary expansion of static pattern rules are identical to those for explicit rules, above, with one exception: for static pattern rules the $$* variable is set to the pattern stem. As with explicit rules, $$? is not available and expands to the empty string. 

Secondary Expansion of Implicit Rules
As make searches for an implicit rule, it substitutes the stem and then performs secondary expansion for every rule with a matching target pattern. The value of the automatic variables is derived in the same fashion as for static pattern rules. As an example: 

     .SECONDEXPANSION:
     
     foo: bar
     
     foo foz: fo%: bo%
     
     %oo: $$< $$^ $$+ $$*

When the implicit rule is tried for target foo, $$< expands to bar, $$^ expands to bar boo, $$+ also expands to bar boo, and $$* expands to f. 

Note that the directory prefix (D), as described in Implicit Rule Search Algorithm, is appended (after expansion) to all the patterns in the prerequisites list. As an example: 

     .SECONDEXPANSION:
     
     /tmp/foo.o:
     
     %.o: $$(addsuffix /%.c,foo bar) foo.h

The prerequisite list after the secondary expansion and directory prefix reconstruction will be /tmp/foo/foo.c /tmp/var/bar/foo.c foo.h. If you are not interested in this reconstruction, you can use $$* instead of % in the prerequisites list. 



--------------------------------------------------------------------------------
Next: Commands, Previous: Makefiles, Up: Top 
4 Writing Rules
A rule appears in the makefile and says when and how to remake certain files, called the rule's targets (most often only one per rule). It lists the other files that are the prerequisites of the target, and commands to use to create or update the target. 

The order of rules is not significant, except for determining the default goal: the target for make to consider, if you do not otherwise specify one. The default goal is the target of the first rule in the first makefile. If the first rule has multiple targets, only the first target is taken as the default. There are two exceptions: a target starting with a period is not a default unless it contains one or more slashes, `/', as well; and, a target that defines a pattern rule has no effect on the default goal. (See Defining and Redefining Pattern Rules.) 

Therefore, we usually write the makefile so that the first rule is the one for compiling the entire program or all the programs described by the makefile (often with a target called `all'). See Arguments to Specify the Goals. 

Rule Example: An example explained. 
Rule Syntax: General syntax explained. 
Prerequisite Types: There are two types of prerequisites. 
Wildcards: Using wildcard characters such as `*'. 
Directory Search: Searching other directories for source files. 
Phony Targets: Using a target that is not a real file's name. 
Force Targets: You can use a target without commands or prerequisites to mark other targets as phony. 
Empty Targets: When only the date matters and the files are empty. 
Special Targets: Targets with special built-in meanings. 
Multiple Targets: When to make use of several targets in a rule. 
Multiple Rules: How to use several rules with the same target. 
Static Pattern: Static pattern rules apply to multiple targets and can vary the prerequisites according to the target name. 
Double-Colon: How to use a special kind of rule to allow several independent rules for one target. 
Automatic Prerequisites: How to automatically generate rules giving prerequisites from source files themselves. 


--------------------------------------------------------------------------------
Next: Rule Syntax, Previous: Rules, Up: Rules 
4.1 Rule Example
Here is an example of a rule: 

     foo.o : foo.c defs.h       # module for twiddling the frobs
             cc -c -g foo.c

Its target is foo.o and its prerequisites are foo.c and defs.h. It has one command, which is `cc -c -g foo.c'. The command line starts with a tab to identify it as a command. 

This rule says two things: 

How to decide whether foo.o is out of date: it is out of date if it does not exist, or if either foo.c or defs.h is more recent than it. 
How to update the file foo.o: by running cc as stated. The command does not explicitly mention defs.h, but we presume that foo.c includes it, and that that is why defs.h was added to the prerequisites. 


--------------------------------------------------------------------------------
Next: Prerequisite Types, Previous: Rule Example, Up: Rules 
4.2 Rule Syntax
In general, a rule looks like this: 

     targets : prerequisites
             command
             ...

or like this: 

     targets : prerequisites ; command
             command
             ...

The targets are file names, separated by spaces. Wildcard characters may be used (see Using Wildcard Characters in File Names) and a name of the form a(m) represents member m in archive file a (see Archive Members as Targets). Usually there is only one target per rule, but occasionally there is a reason to have more (see Multiple Targets in a Rule). 

The command lines start with a tab character. The first command may appear on the line after the prerequisites, with a tab character, or may appear on the same line, with a semicolon. Either way, the effect is the same. There are other differences in the syntax of command lines. See Writing the Commands in Rules. 

Because dollar signs are used to start make variable references, if you really want a dollar sign in a target or prerequisite you must write two of them, `$$' (see How to Use Variables). If you have enabled secondary expansion (see Secondary Expansion) and you want a literal dollar sign in the prerequisites lise, you must actually write four dollar signs (`$$$$'). 

You may split a long line by inserting a backslash followed by a newline, but this is not required, as make places no limit on the length of a line in a makefile. 

A rule tells make two things: when the targets are out of date, and how to update them when necessary. 

The criterion for being out of date is specified in terms of the prerequisites, which consist of file names separated by spaces. (Wildcards and archive members (see Archives) are allowed here too.) A target is out of date if it does not exist or if it is older than any of the prerequisites (by comparison of last-modification times). The idea is that the contents of the target file are computed based on information in the prerequisites, so if any of the prerequisites changes, the contents of the existing target file are no longer necessarily valid. 

How to update is specified by commands. These are lines to be executed by the shell (normally `sh'), but with some extra features (see Writing the Commands in Rules). 



--------------------------------------------------------------------------------
Next: Wildcards, Previous: Rule Syntax, Up: Rules 
4.3 Types of Prerequisites
There are actually two different types of prerequisites understood by GNU make: normal prerequisites such as described in the previous section, and order-only prerequisites. A normal prerequisite makes two statements: first, it imposes an order of execution of build commands: any commands necessary to build any of a target's prerequisites will be fully executed before any commands necessary to build the target. Second, it imposes a dependency relationship: if any prerequisite is newer than the target, then the target is considered out-of-date and must be rebuilt. 

Normally, this is exactly what you want: if a target's prerequisite is updated, then the target should also be updated. 

Occasionally, however, you have a situation where you want to impose a specific ordering on the rules to be invoked without forcing the target to be updated if one of those rules is executed. In that case, you want to define order-only prerequisites. Order-only prerequisites can be specified by placing a pipe symbol (|) in the prerequisites list: any prerequisites to the left of the pipe symbol are normal; any prerequisites to the right are order-only: 

     targets : normal-prerequisites | order-only-prerequisites

The normal prerequisites section may of course be empty. Also, you may still declare multiple lines of prerequisites for the same target: they are appended appropriately. Note that if you declare the same file to be both a normal and an order-only prerequisite, the normal prerequisite takes precedence (since they are a strict superset of the behavior of an order-only prerequisite). 



--------------------------------------------------------------------------------
Next: Directory Search, Previous: Prerequisite Types, Up: Rules 
4.4 Using Wildcard Characters in File Names
A single file name can specify many files using wildcard characters. The wildcard characters in make are `*', `?' and `[...]', the same as in the Bourne shell. For example, *.c specifies a list of all the files (in the working directory) whose names end in `.c'. 

The character `~' at the beginning of a file name also has special significance. If alone, or followed by a slash, it represents your home directory. For example ~/bin expands to /home/you/bin. If the `~' is followed by a word, the string represents the home directory of the user named by that word. For example ~john/bin expands to /home/john/bin. On systems which don't have a home directory for each user (such as MS-DOS or MS-Windows), this functionality can be simulated by setting the environment variable HOME. 

Wildcard expansion is performed by make automatically in targets and in prerequisites. In commands the shell is responsible for wildcard expansion. In other contexts, wildcard expansion happens only if you request it explicitly with the wildcard function. 

The special significance of a wildcard character can be turned off by preceding it with a backslash. Thus, foo\*bar would refer to a specific file whose name consists of `foo', an asterisk, and `bar'. 

Wildcard Examples: Several examples 
Wildcard Pitfall: Problems to avoid. 
Wildcard Function: How to cause wildcard expansion where it does not normally take place. 


--------------------------------------------------------------------------------
Next: Wildcard Pitfall, Previous: Wildcards, Up: Wildcards 
4.4.1 Wildcard Examples
Wildcards can be used in the commands of a rule, where they are expanded by the shell. For example, here is a rule to delete all the object files: 

     clean:
             rm -f *.o

Wildcards are also useful in the prerequisites of a rule. With the following rule in the makefile, `make print' will print all the `.c' files that have changed since the last time you printed them: 

     print: *.c
             lpr -p $?
             touch print

This rule uses print as an empty target file; see Empty Target Files to Record Events. (The automatic variable `$?' is used to print only those files that have changed; see Automatic Variables.) 

Wildcard expansion does not happen when you define a variable. Thus, if you write this: 

     objects = *.o

then the value of the variable objects is the actual string `*.o'. However, if you use the value of objects in a target, prerequisite or command, wildcard expansion will take place at that time. To set objects to the expansion, instead use: 

     objects := $(wildcard *.o)

See Wildcard Function. 



--------------------------------------------------------------------------------
Next: Wildcard Function, Previous: Wildcard Examples, Up: Wildcards 
4.4.2 Pitfalls of Using Wildcards
Now here is an example of a naive way of using wildcard expansion, that does not do what you would intend. Suppose you would like to say that the executable file foo is made from all the object files in the directory, and you write this: 

     objects = *.o
     
     foo : $(objects)
             cc -o foo $(CFLAGS) $(objects)

The value of objects is the actual string `*.o'. Wildcard expansion happens in the rule for foo, so that each existing `.o' file becomes a prerequisite of foo and will be recompiled if necessary. 

But what if you delete all the `.o' files? When a wildcard matches no files, it is left as it is, so then foo will depend on the oddly-named file *.o. Since no such file is likely to exist, make will give you an error saying it cannot figure out how to make *.o. This is not what you want! 

Actually it is possible to obtain the desired result with wildcard expansion, but you need more sophisticated techniques, including the wildcard function and string substitution. See The Function wildcard. 

Microsoft operating systems (MS-DOS and MS-Windows) use backslashes to separate directories in pathnames, like so: 

       c:\foo\bar\baz.c

This is equivalent to the Unix-style c:/foo/bar/baz.c (the c: part is the so-called drive letter). When make runs on these systems, it supports backslashes as well as the Unix-style forward slashes in pathnames. However, this support does not include the wildcard expansion, where backslash is a quote character. Therefore, you must use Unix-style slashes in these cases. 



--------------------------------------------------------------------------------
Previous: Wildcard Pitfall, Up: Wildcards 
4.4.3 The Function wildcard
Wildcard expansion happens automatically in rules. But wildcard expansion does not normally take place when a variable is set, or inside the arguments of a function. If you want to do wildcard expansion in such places, you need to use the wildcard function, like this: 

     $(wildcard pattern...)

This string, used anywhere in a makefile, is replaced by a space-separated list of names of existing files that match one of the given file name patterns. If no existing file name matches a pattern, then that pattern is omitted from the output of the wildcard function. Note that this is different from how unmatched wildcards behave in rules, where they are used verbatim rather than ignored (see Wildcard Pitfall). 

One use of the wildcard function is to get a list of all the C source files in a directory, like this: 

     $(wildcard *.c)

We can change the list of C source files into a list of object files by replacing the `.c' suffix with `.o' in the result, like this: 

     $(patsubst %.c,%.o,$(wildcard *.c))

(Here we have used another function, patsubst. See Functions for String Substitution and Analysis.) 

Thus, a makefile to compile all C source files in the directory and then link them together could be written as follows: 

     objects := $(patsubst %.c,%.o,$(wildcard *.c))
     
     foo : $(objects)
             cc -o foo $(objects)

(This takes advantage of the implicit rule for compiling C programs, so there is no need to write explicit rules for compiling the files. See The Two Flavors of Variables, for an explanation of `:=', which is a variant of `='.) 



--------------------------------------------------------------------------------
Next: Phony Targets, Previous: Wildcards, Up: Rules 
4.5 Searching Directories for Prerequisites
For large systems, it is often desirable to put sources in a separate directory from the binaries. The directory search features of make facilitate this by searching several directories automatically to find a prerequisite. When you redistribute the files among directories, you do not need to change the individual rules, just the search paths. 

General Search: Specifying a search path that applies to every prerequisite. 
Selective Search: Specifying a search path for a specified class of names. 
Search Algorithm: When and how search paths are applied. 
Commands/Search: How to write shell commands that work together with search paths. 
Implicit/Search: How search paths affect implicit rules. 
Libraries/Search: Directory search for link libraries. 


--------------------------------------------------------------------------------
Next: Selective Search, Previous: Directory Search, Up: Directory Search 
4.5.1 VPATH: Search Path for All Prerequisites
The value of the make variable VPATH specifies a list of directories that make should search. Most often, the directories are expected to contain prerequisite files that are not in the current directory; however, make uses VPATH as a search list for both prerequisites and targets of rules. 

Thus, if a file that is listed as a target or prerequisite does not exist in the current directory, make searches the directories listed in VPATH for a file with that name. If a file is found in one of them, that file may become the prerequisite (see below). Rules may then specify the names of files in the prerequisite list as if they all existed in the current directory. See Writing Shell Commands with Directory Search. 

In the VPATH variable, directory names are separated by colons or blanks. The order in which directories are listed is the order followed by make in its search. (On MS-DOS and MS-Windows, semi-colons are used as separators of directory names in VPATH, since the colon can be used in the pathname itself, after the drive letter.) 

For example, 

     VPATH = src:../headers

specifies a path containing two directories, src and ../headers, which make searches in that order. 

With this value of VPATH, the following rule, 

     foo.o : foo.c

is interpreted as if it were written like this: 

     foo.o : src/foo.c

assuming the file foo.c does not exist in the current directory but is found in the directory src. 



--------------------------------------------------------------------------------
Next: Search Algorithm, Previous: General Search, Up: Directory Search 
4.5.2 The vpath Directive
Similar to the VPATH variable, but more selective, is the vpath directive (note lower case), which allows you to specify a search path for a particular class of file names: those that match a particular pattern. Thus you can supply certain search directories for one class of file names and other directories (or none) for other file names. 

There are three forms of the vpath directive: 

vpath pattern directories
Specify the search path directories for file names that match pattern. 
The search path, directories, is a list of directories to be searched, separated by colons (semi-colons on MS-DOS and MS-Windows) or blanks, just like the search path used in the VPATH variable. 


vpath pattern
Clear out the search path associated with pattern. 

vpath
Clear all search paths previously specified with vpath directives. 
A vpath pattern is a string containing a `%' character. The string must match the file name of a prerequisite that is being searched for, the `%' character matching any sequence of zero or more characters (as in pattern rules; see Defining and Redefining Pattern Rules). For example, %.h matches files that end in .h. (If there is no `%', the pattern must match the prerequisite exactly, which is not useful very often.) 

`%' characters in a vpath directive's pattern can be quoted with preceding backslashes (`\'). Backslashes that would otherwise quote `%' characters can be quoted with more backslashes. Backslashes that quote `%' characters or other backslashes are removed from the pattern before it is compared to file names. Backslashes that are not in danger of quoting `%' characters go unmolested. 

When a prerequisite fails to exist in the current directory, if the pattern in a vpath directive matches the name of the prerequisite file, then the directories in that directive are searched just like (and before) the directories in the VPATH variable. 

For example, 

     vpath %.h ../headers

tells make to look for any prerequisite whose name ends in .h in the directory ../headers if the file is not found in the current directory. 

If several vpath patterns match the prerequisite file's name, then make processes each matching vpath directive one by one, searching all the directories mentioned in each directive. make handles multiple vpath directives in the order in which they appear in the makefile; multiple directives with the same pattern are independent of each other. 

Thus, 

     vpath %.c foo
     vpath %   blish
     vpath %.c bar

will look for a file ending in `.c' in foo, then blish, then bar, while 

     vpath %.c foo:bar
     vpath %   blish

will look for a file ending in `.c' in foo, then bar, then blish. 



--------------------------------------------------------------------------------
Next: Commands/Search, Previous: Selective Search, Up: Directory Search 
4.5.3 How Directory Searches are Performed
When a prerequisite is found through directory search, regardless of type (general or selective), the pathname located may not be the one that make actually provides you in the prerequisite list. Sometimes the path discovered through directory search is thrown away. 

The algorithm make uses to decide whether to keep or abandon a path found via directory search is as follows: 

If a target file does not exist at the path specified in the makefile, directory search is performed. 
If the directory search is successful, that path is kept and this file is tentatively stored as the target. 
All prerequisites of this target are examined using this same method. 
After processing the prerequisites, the target may or may not need to be rebuilt: 
If the target does not need to be rebuilt, the path to the file found during directory search is used for any prerequisite lists which contain this target. In short, if make doesn't need to rebuild the target then you use the path found via directory search. 
If the target does need to be rebuilt (is out-of-date), the pathname found during directory search is thrown away, and the target is rebuilt using the file name specified in the makefile. In short, if make must rebuild, then the target is rebuilt locally, not in the directory found via directory search. 
This algorithm may seem complex, but in practice it is quite often exactly what you want. 

Other versions of make use a simpler algorithm: if the file does not exist, and it is found via directory search, then that pathname is always used whether or not the target needs to be built. Thus, if the target is rebuilt it is created at the pathname discovered during directory search. 

If, in fact, this is the behavior you want for some or all of your directories, you can use the GPATH variable to indicate this to make. 

GPATH has the same syntax and format as VPATH (that is, a space- or colon-delimited list of pathnames). If an out-of-date target is found by directory search in a directory that also appears in GPATH, then that pathname is not thrown away. The target is rebuilt using the expanded path. 



--------------------------------------------------------------------------------
Next: Implicit/Search, Previous: Search Algorithm, Up: Directory Search 
4.5.4 Writing Shell Commands with Directory Search
When a prerequisite is found in another directory through directory search, this cannot change the commands of the rule; they will execute as written. Therefore, you must write the commands with care so that they will look for the prerequisite in the directory where make finds it. 

This is done with the automatic variables such as `$^' (see Automatic Variables). For instance, the value of `$^' is a list of all the prerequisites of the rule, including the names of the directories in which they were found, and the value of `$@' is the target. Thus: 

     foo.o : foo.c
             cc -c $(CFLAGS) $^ -o $@

(The variable CFLAGS exists so you can specify flags for C compilation by implicit rules; we use it here for consistency so it will affect all C compilations uniformly; see Variables Used by Implicit Rules.) 

Often the prerequisites include header files as well, which you do not want to mention in the commands. The automatic variable `$<' is just the first prerequisite: 

     VPATH = src:../headers
     foo.o : foo.c defs.h hack.h
             cc -c $(CFLAGS) $< -o $@



--------------------------------------------------------------------------------
Next: Libraries/Search, Previous: Commands/Search, Up: Directory Search 
4.5.5 Directory Search and Implicit Rules
The search through the directories specified in VPATH or with vpath also happens during consideration of implicit rules (see Using Implicit Rules). 

For example, when a file foo.o has no explicit rule, make considers implicit rules, such as the built-in rule to compile foo.c if that file exists. If such a file is lacking in the current directory, the appropriate directories are searched for it. If foo.c exists (or is mentioned in the makefile) in any of the directories, the implicit rule for C compilation is applied. 

The commands of implicit rules normally use automatic variables as a matter of necessity; consequently they will use the file names found by directory search with no extra effort. 



--------------------------------------------------------------------------------
Previous: Implicit/Search, Up: Directory Search 
4.5.6 Directory Search for Link Libraries
Directory search applies in a special way to libraries used with the linker. This special feature comes into play when you write a prerequisite whose name is of the form `-lname'. (You can tell something strange is going on here because the prerequisite is normally the name of a file, and the file name of a library generally looks like libname.a, not like `-lname'.) 

When a prerequisite's name has the form `-lname', make handles it specially by searching for the file libname.so in the current directory, in directories specified by matching vpath search paths and the VPATH search path, and then in the directories /lib, /usr/lib, and prefix/lib (normally /usr/local/lib, but MS-DOS/MS-Windows versions of make behave as if prefix is defined to be the root of the DJGPP installation tree). 

If that file is not found, then the file libname.a is searched for, in the same directories as above. 

For example, if there is a /usr/lib/libcurses.a library on your system (and no /usr/lib/libcurses.so file), then 

     foo : foo.c -lcurses
             cc $^ -o $@

would cause the command `cc foo.c /usr/lib/libcurses.a -o foo' to be executed when foo is older than foo.c or than /usr/lib/libcurses.a. 

Although the default set of files to be searched for is libname.so and libname.a, this is customizable via the .LIBPATTERNS variable. Each word in the value of this variable is a pattern string. When a prerequisite like `-lname' is seen, make will replace the percent in each pattern in the list with name and perform the above directory searches using that library filename. If no library is found, the next word in the list will be used. 

The default value for .LIBPATTERNS is `lib%.so lib%.a', which provides the default behavior described above. 

You can turn off link library expansion completely by setting this variable to an empty value. 



--------------------------------------------------------------------------------
Next: Force Targets, Previous: Directory Search, Up: Rules 
4.6 Phony Targets
A phony target is one that is not really the name of a file. It is just a name for some commands to be executed when you make an explicit request. There are two reasons to use a phony target: to avoid a conflict with a file of the same name, and to improve performance. 

If you write a rule whose commands will not create the target file, the commands will be executed every time the target comes up for remaking. Here is an example: 

     clean:
             rm *.o temp

Because the rm command does not create a file named clean, probably no such file will ever exist. Therefore, the rm command will be executed every time you say `make clean'. The phony target will cease to work if anything ever does create a file named clean in this directory. Since it has no prerequisites, the file clean would inevitably be considered up to date, and its commands would not be executed. To avoid this problem, you can explicitly declare the target to be phony, using the special target .PHONY (see Special Built-in Target Names) as follows: 

     .PHONY : clean

Once this is done, `make clean' will run the commands regardless of whether there is a file named clean. 

Since it knows that phony targets do not name actual files that could be remade from other files, make skips the implicit rule search for phony targets (see Implicit Rules). This is why declaring a target phony is good for performance, even if you are not worried about the actual file existing. 

Thus, you first write the line that states that clean is a phony target, then you write the rule, like this: 

     .PHONY: clean
     clean:
             rm *.o temp

Another example of the usefulness of phony targets is in conjunction with recursive invocations of make (for more information, see Recursive Use of make). In this case the makefile will often contain a variable which lists a number of subdirectories to be built. One way to handle this is with one rule whose command is a shell loop over the subdirectories, like this: 

     SUBDIRS = foo bar baz
     
     subdirs:
             for dir in $(SUBDIRS); do \
               $(MAKE) -C $$dir; \
             done

There are a few problems with this method, however. First, any error detected in a submake is not noted by this rule, so it will continue to build the rest of the directories even when one fails. This can be overcome by adding shell commands to note the error and exit, but then it will do so even if make is invoked with the -k option, which is unfortunate. Second, and perhaps more importantly, you cannot take advantage of make's ability to build targets in parallel (see Parallel Execution), since there is only one rule. 

By declaring the subdirectories as phony targets (you must do this as the subdirectory obviously always exists; otherwise it won't be built) you can remove these problems: 

     SUBDIRS = foo bar baz
     
     .PHONY: subdirs $(SUBDIRS)
     
     subdirs: $(SUBDIRS)
     
     $(SUBDIRS):
             $(MAKE) -C $@
     
     foo: baz

Here we've also declared that the foo subdirectory cannot be built until after the baz subdirectory is complete; this kind of relationship declaration is particularly important when attempting parallel builds. 

A phony target should not be a prerequisite of a real target file; if it is, its commands are run every time make goes to update that file. As long as a phony target is never a prerequisite of a real target, the phony target commands will be executed only when the phony target is a specified goal (see Arguments to Specify the Goals). 

Phony targets can have prerequisites. When one directory contains multiple programs, it is most convenient to describe all of the programs in one makefile ./Makefile. Since the target remade by default will be the first one in the makefile, it is common to make this a phony target named `all' and give it, as prerequisites, all the individual programs. For example: 

     all : prog1 prog2 prog3
     .PHONY : all
     
     prog1 : prog1.o utils.o
             cc -o prog1 prog1.o utils.o
     
     prog2 : prog2.o
             cc -o prog2 prog2.o
     
     prog3 : prog3.o sort.o utils.o
             cc -o prog3 prog3.o sort.o utils.o

Now you can say just `make' to remake all three programs, or specify as arguments the ones to remake (as in `make prog1 prog3'). Phoniness is not inherited: the prerequisites of a phony target are not themselves phony, unless explicitly declared to be so. 

When one phony target is a prerequisite of another, it serves as a subroutine of the other. For example, here `make cleanall' will delete the object files, the difference files, and the file program: 

     .PHONY: cleanall cleanobj cleandiff
     
     cleanall : cleanobj cleandiff
             rm program
     
     cleanobj :
             rm *.o
     
     cleandiff :
             rm *.diff



--------------------------------------------------------------------------------
Next: Empty Targets, Previous: Phony Targets, Up: Rules 
4.7 Rules without Commands or Prerequisites
If a rule has no prerequisites or commands, and the target of the rule is a nonexistent file, then make imagines this target to have been updated whenever its rule is run. This implies that all targets depending on this one will always have their commands run. 

An example will illustrate this: 

     clean: FORCE
             rm $(objects)
     FORCE:

Here the target `FORCE' satisfies the special conditions, so the target clean that depends on it is forced to run its commands. There is nothing special about the name `FORCE', but that is one name commonly used this way. 

As you can see, using `FORCE' this way has the same results as using `.PHONY: clean'. 

Using `.PHONY' is more explicit and more efficient. However, other versions of make do not support `.PHONY'; thus `FORCE' appears in many makefiles. See Phony Targets. 



--------------------------------------------------------------------------------
Next: Special Targets, Previous: Force Targets, Up: Rules 
4.8 Empty Target Files to Record Events
The empty target is a variant of the phony target; it is used to hold commands for an action that you request explicitly from time to time. Unlike a phony target, this target file can really exist; but the file's contents do not matter, and usually are empty. 

The purpose of the empty target file is to record, with its last-modification time, when the rule's commands were last executed. It does so because one of the commands is a touch command to update the target file. 

The empty target file should have some prerequisites (otherwise it doesn't make sense). When you ask to remake the empty target, the commands are executed if any prerequisite is more recent than the target; in other words, if a prerequisite has changed since the last time you remade the target. Here is an example: 

     print: foo.c bar.c
             lpr -p $?
             touch print

With this rule, `make print' will execute the lpr command if either source file has changed since the last `make print'. The automatic variable `$?' is used to print only those files that have changed (see Automatic Variables). 



--------------------------------------------------------------------------------
Next: Multiple Targets, Previous: Empty Targets, Up: Rules 
4.9 Special Built-in Target Names
Certain names have special meanings if they appear as targets. 

.PHONY
The prerequisites of the special target .PHONY are considered to be phony targets. When it is time to consider such a target, make will run its commands unconditionally, regardless of whether a file with that name exists or what its last-modification time is. See Phony Targets. 



.SUFFIXES
The prerequisites of the special target .SUFFIXES are the list of suffixes to be used in checking for suffix rules. See Old-Fashioned Suffix Rules. 



.DEFAULT
The commands specified for .DEFAULT are used for any target for which no rules are found (either explicit rules or implicit rules). See Last Resort. If .DEFAULT commands are specified, every file mentioned as a prerequisite, but not as a target in a rule, will have these commands executed on its behalf. See Implicit Rule Search Algorithm. 



.PRECIOUS
The targets which .PRECIOUS depends on are given the following special treatment: if make is killed or interrupted during the execution of their commands, the target is not deleted. See Interrupting or Killing make. Also, if the target is an intermediate file, it will not be deleted after it is no longer needed, as is normally done. See Chains of Implicit Rules. In this latter respect it overlaps with the .SECONDARY special target. 
You can also list the target pattern of an implicit rule (such as `%.o') as a prerequisite file of the special target .PRECIOUS to preserve intermediate files created by rules whose target patterns match that file's name. 




.INTERMEDIATE
The targets which .INTERMEDIATE depends on are treated as intermediate files. See Chains of Implicit Rules. .INTERMEDIATE with no prerequisites has no effect. 



.SECONDARY
The targets which .SECONDARY depends on are treated as intermediate files, except that they are never automatically deleted. See Chains of Implicit Rules. 
.SECONDARY with no prerequisites causes all targets to be treated as secondary (i.e., no target is removed because it is considered intermediate). 




.SECONDEXPANSION
If .SECONDEXPANSION is mentioned as a target anywhere in the makefile, then all prerequisite lists defined after it appears will be expanded a second time after all makefiles have been read in. See Secondary Expansion. 
The prerequisites of the special target .SUFFIXES are the list of suffixes to be used in checking for suffix rules. See Old-Fashioned Suffix Rules. 




.DELETE_ON_ERROR
If .DELETE_ON_ERROR is mentioned as a target anywhere in the makefile, then make will delete the target of a rule if it has changed and its commands exit with a nonzero exit status, just as it does when it receives a signal. See Errors in Commands. 



.IGNORE
If you specify prerequisites for .IGNORE, then make will ignore errors in execution of the commands run for those particular files. The commands for .IGNORE are not meaningful. 
If mentioned as a target with no prerequisites, .IGNORE says to ignore errors in execution of commands for all files. This usage of `.IGNORE' is supported only for historical compatibility. Since this affects every command in the makefile, it is not very useful; we recommend you use the more selective ways to ignore errors in specific commands. See Errors in Commands. 




.LOW_RESOLUTION_TIME
If you specify prerequisites for .LOW_RESOLUTION_TIME, make assumes that these files are created by commands that generate low resolution time stamps. The commands for .LOW_RESOLUTION_TIME are not meaningful. 
The high resolution file time stamps of many modern hosts lessen the chance of make incorrectly concluding that a file is up to date. Unfortunately, these hosts provide no way to set a high resolution file time stamp, so commands like `cp -p' that explicitly set a file's time stamp must discard its subsecond part. If a file is created by such a command, you should list it as a prerequisite of .LOW_RESOLUTION_TIME so that make does not mistakenly conclude that the file is out of date. For example: 

          .LOW_RESOLUTION_TIME: dst
          dst: src
                  cp -p src dst
     
Since `cp -p' discards the subsecond part of src's time stamp, dst is typically slightly older than src even when it is up to date. The .LOW_RESOLUTION_TIME line causes make to consider dst to be up to date if its time stamp is at the start of the same second that src's time stamp is in. 

Due to a limitation of the archive format, archive member time stamps are always low resolution. You need not list archive members as prerequisites of .LOW_RESOLUTION_TIME, as make does this automatically. 




.SILENT
If you specify prerequisites for .SILENT, then make will not print the commands to remake those particular files before executing them. The commands for .SILENT are not meaningful. 
If mentioned as a target with no prerequisites, .SILENT says not to print any commands before executing them. This usage of `.SILENT' is supported only for historical compatibility. We recommend you use the more selective ways to silence specific commands. See Command Echoing. If you want to silence all commands for a particular run of make, use the `-s' or `--silent' option (see Options Summary). 




.EXPORT_ALL_VARIABLES
Simply by being mentioned as a target, this tells make to export all variables to child processes by default. See Communicating Variables to a Sub-make. 



.NOTPARALLEL
If .NOTPARALLEL is mentioned as a target, then this invocation of make will be run serially, even if the `-j' option is given. Any recursively invoked make command will still be run in parallel (unless its makefile contains this target). Any prerequisites on this target are ignored. 
Any defined implicit rule suffix also counts as a special target if it appears as a target, and so does the concatenation of two suffixes, such as `.c.o'. These targets are suffix rules, an obsolete way of defining implicit rules (but a way still widely used). In principle, any target name could be special in this way if you break it in two and add both pieces to the suffix list. In practice, suffixes normally begin with `.', so these special target names also begin with `.'. See Old-Fashioned Suffix Rules. 



--------------------------------------------------------------------------------
Next: Multiple Rules, Previous: Special Targets, Up: Rules 
4.10 Multiple Targets in a Rule
A rule with multiple targets is equivalent to writing many rules, each with one target, and all identical aside from that. The same commands apply to all the targets, but their effects may vary because you can substitute the actual target name into the command using `$@'. The rule contributes the same prerequisites to all the targets also. 

This is useful in two cases. 

You want just prerequisites, no commands. For example: 
          kbd.o command.o files.o: command.h
     
gives an additional prerequisite to each of the three object files mentioned. 

Similar commands work for all the targets. The commands do not need to be absolutely identical, since the automatic variable `$@' can be used to substitute the particular target to be remade into the commands (see Automatic Variables). For example: 
          bigoutput littleoutput : text.g
                  generate text.g -$(subst output,,$@) > $@
     
is equivalent to 

          bigoutput : text.g
                  generate text.g -big > bigoutput
          littleoutput : text.g
                  generate text.g -little > littleoutput
     
Here we assume the hypothetical program generate makes two types of output, one if given `-big' and one if given `-little'. See Functions for String Substitution and Analysis, for an explanation of the subst function. 

Suppose you would like to vary the prerequisites according to the target, much as the variable `$@' allows you to vary the commands. You cannot do this with multiple targets in an ordinary rule, but you can do it with a static pattern rule. See Static Pattern Rules. 



--------------------------------------------------------------------------------
Next: Static Pattern, Previous: Multiple Targets, Up: Rules 
4.11 Multiple Rules for One Target
One file can be the target of several rules. All the prerequisites mentioned in all the rules are merged into one list of prerequisites for the target. If the target is older than any prerequisite from any rule, the commands are executed. 

There can only be one set of commands to be executed for a file. If more than one rule gives commands for the same file, make uses the last set given and prints an error message. (As a special case, if the file's name begins with a dot, no error message is printed. This odd behavior is only for compatibility with other implementations of make... you should avoid using it). Occasionally it is useful to have the same target invoke multiple commands which are defined in different parts of your makefile; you can use double-colon rules (see Double-Colon) for this. 

An extra rule with just prerequisites can be used to give a few extra prerequisites to many files at once. For example, makefiles often have a variable, such as objects, containing a list of all the compiler output files in the system being made. An easy way to say that all of them must be recompiled if config.h changes is to write the following: 

     objects = foo.o bar.o
     foo.o : defs.h
     bar.o : defs.h test.h
     $(objects) : config.h

This could be inserted or taken out without changing the rules that really specify how to make the object files, making it a convenient form to use if you wish to add the additional prerequisite intermittently. 

Another wrinkle is that the additional prerequisites could be specified with a variable that you set with a command argument to make (see Overriding Variables). For example, 

     extradeps=
     $(objects) : $(extradeps)

means that the command `make extradeps=foo.h' will consider foo.h as a prerequisite of each object file, but plain `make' will not. 

If none of the explicit rules for a target has commands, then make searches for an applicable implicit rule to find some commands see Using Implicit Rules). 



--------------------------------------------------------------------------------
Next: Double-Colon, Previous: Multiple Rules, Up: Rules 
4.12 Static Pattern Rules
Static pattern rules are rules which specify multiple targets and construct the prerequisite names for each target based on the target name. They are more general than ordinary rules with multiple targets because the targets do not have to have identical prerequisites. Their prerequisites must be analogous, but not necessarily identical. 

Static Usage: The syntax of static pattern rules. 
Static versus Implicit: When are they better than implicit rules? 


--------------------------------------------------------------------------------
Next: Static versus Implicit, Previous: Static Pattern, Up: Static Pattern 
4.12.1 Syntax of Static Pattern Rules
Here is the syntax of a static pattern rule: 

     targets ...: target-pattern: prereq-patterns ...
             commands
             ...

The targets list specifies the targets that the rule applies to. The targets can contain wildcard characters, just like the targets of ordinary rules (see Using Wildcard Characters in File Names). 

The target-pattern and prereq-patterns say how to compute the prerequisites of each target. Each target is matched against the target-pattern to extract a part of the target name, called the stem. This stem is substituted into each of the prereq-patterns to make the prerequisite names (one from each prereq-pattern). 

Each pattern normally contains the character `%' just once. When the target-pattern matches a target, the `%' can match any part of the target name; this part is called the stem. The rest of the pattern must match exactly. For example, the target foo.o matches the pattern `%.o', with `foo' as the stem. The targets foo.c and foo.out do not match that pattern. 

The prerequisite names for each target are made by substituting the stem for the `%' in each prerequisite pattern. For example, if one prerequisite pattern is %.c, then substitution of the stem `foo' gives the prerequisite name foo.c. It is legitimate to write a prerequisite pattern that does not contain `%'; then this prerequisite is the same for all targets. 

`%' characters in pattern rules can be quoted with preceding backslashes (`\'). Backslashes that would otherwise quote `%' characters can be quoted with more backslashes. Backslashes that quote `%' characters or other backslashes are removed from the pattern before it is compared to file names or has a stem substituted into it. Backslashes that are not in danger of quoting `%' characters go unmolested. For example, the pattern the\%weird\\%pattern\\ has `the%weird\' preceding the operative `%' character, and `pattern\\' following it. The final two backslashes are left alone because they cannot affect any `%' character. 

Here is an example, which compiles each of foo.o and bar.o from the corresponding .c file: 

     objects = foo.o bar.o
     
     all: $(objects)
     
     $(objects): %.o: %.c
             $(CC) -c $(CFLAGS) $< -o $@

Here `$<' is the automatic variable that holds the name of the prerequisite and `$@' is the automatic variable that holds the name of the target; see Automatic Variables. 

Each target specified must match the target pattern; a warning is issued for each target that does not. If you have a list of files, only some of which will match the pattern, you can use the filter function to remove nonmatching file names (see Functions for String Substitution and Analysis): 

     files = foo.elc bar.o lose.o
     
     $(filter %.o,$(files)): %.o: %.c
             $(CC) -c $(CFLAGS) $< -o $@
     $(filter %.elc,$(files)): %.elc: %.el
             emacs -f batch-byte-compile $<

In this example the result of `$(filter %.o,$(files))' is bar.o lose.o, and the first static pattern rule causes each of these object files to be updated by compiling the corresponding C source file. The result of `$(filter %.elc,$(files))' is foo.elc, so that file is made from foo.el. 

Another example shows how to use $* in static pattern rules: 

     bigoutput littleoutput : %output : text.g
             generate text.g -$* > $@

When the generate command is run, $* will expand to the stem, either `big' or `little'. 



--------------------------------------------------------------------------------
Previous: Static Usage, Up: Static Pattern 
4.12.2 Static Pattern Rules versus Implicit Rules
A static pattern rule has much in common with an implicit rule defined as a pattern rule (see Defining and Redefining Pattern Rules). Both have a pattern for the target and patterns for constructing the names of prerequisites. The difference is in how make decides when the rule applies. 

An implicit rule can apply to any target that matches its pattern, but it does apply only when the target has no commands otherwise specified, and only when the prerequisites can be found. If more than one implicit rule appears applicable, only one applies; the choice depends on the order of rules. 

By contrast, a static pattern rule applies to the precise list of targets that you specify in the rule. It cannot apply to any other target and it invariably does apply to each of the targets specified. If two conflicting rules apply, and both have commands, that's an error. 

The static pattern rule can be better than an implicit rule for these reasons: 

You may wish to override the usual implicit rule for a few files whose names cannot be categorized syntactically but can be given in an explicit list. 
If you cannot be sure of the precise contents of the directories you are using, you may not be sure which other irrelevant files might lead make to use the wrong implicit rule. The choice might depend on the order in which the implicit rule search is done. With static pattern rules, there is no uncertainty: each rule applies to precisely the targets specified. 


--------------------------------------------------------------------------------
Next: Automatic Prerequisites, Previous: Static Pattern, Up: Rules 
4.13 Double-Colon Rules
Double-colon rules are rules written with `::' instead of `:' after the target names. They are handled differently from ordinary rules when the same target appears in more than one rule. 

When a target appears in multiple rules, all the rules must be the same type: all ordinary, or all double-colon. If they are double-colon, each of them is independent of the others. Each double-colon rule's commands are executed if the target is older than any prerequisites of that rule. If there are no prerequisites for that rule, its commands are always executed (even if the target already exists). This can result in executing none, any, or all of the double-colon rules. 

Double-colon rules with the same target are in fact completely separate from one another. Each double-colon rule is processed individually, just as rules with different targets are processed. 

The double-colon rules for a target are executed in the order they appear in the makefile. However, the cases where double-colon rules really make sense are those where the order of executing the commands would not matter. 

Double-colon rules are somewhat obscure and not often very useful; they provide a mechanism for cases in which the method used to update a target differs depending on which prerequisite files caused the update, and such cases are rare. 

Each double-colon rule should specify commands; if it does not, an implicit rule will be used if one applies. See Using Implicit Rules. 



--------------------------------------------------------------------------------
Previous: Double-Colon, Up: Rules 
4.14 Generating Prerequisites Automatically
In the makefile for a program, many of the rules you need to write often say only that some object file depends on some header file. For example, if main.c uses defs.h via an #include, you would write: 

     main.o: defs.h

You need this rule so that make knows that it must remake main.o whenever defs.h changes. You can see that for a large program you would have to write dozens of such rules in your makefile. And, you must always be very careful to update the makefile every time you add or remove an #include. To avoid this hassle, most modern C compilers can write these rules for you, by looking at the #include lines in the source files. Usually this is done with the `-M' option to the compiler. For example, the command: 

     cc -M main.c

generates the output: 

     main.o : main.c defs.h

Thus you no longer have to write all those rules yourself. The compiler will do it for you. 

Note that such a prerequisite constitutes mentioning main.o in a makefile, so it can never be considered an intermediate file by implicit rule search. This means that make won't ever remove the file after using it; see Chains of Implicit Rules. 

With old make programs, it was traditional practice to use this compiler feature to generate prerequisites on demand with a command like `make depend'. That command would create a file depend containing all the automatically-generated prerequisites; then the makefile could use include to read them in (see Include). 

In GNU make, the feature of remaking makefiles makes this practice obsolete you need never tell make explicitly to regenerate the prerequisites, because it always regenerates any makefile that is out of date. See Remaking Makefiles. 

The practice we recommend for automatic prerequisite generation is to have one makefile corresponding to each source file. For each source file name.c there is a makefile name.d which lists what files the object file name.o depends on. That way only the source files that have changed need to be rescanned to produce the new prerequisites. 

Here is the pattern rule to generate a file of prerequisites (i.e., a makefile) called name.d from a C source file called name.c: 

     %.d: %.c
             @set -e; rm -f $@; \
              $(CC) -M $(CPPFLAGS) $< > $@.$$$$; \
              sed 's,\($*\)\.o[ :]*,\1.o $@ : ,g' < $@.$$$$ > $@; \
              rm -f $@.$$$$

See Pattern Rules, for information on defining pattern rules. The `-e' flag to the shell causes it to exit immediately if the $(CC) command (or any other command) fails (exits with a nonzero status). With the GNU C compiler, you may wish to use the `-MM' flag instead of `-M'. This omits prerequisites on system header files. See Options Controlling the Preprocessor, for details. 

The purpose of the sed command is to translate (for example): 

     main.o : main.c defs.h

into: 

     main.o main.d : main.c defs.h

This makes each `.d' file depend on all the source and header files that the corresponding `.o' file depends on. make then knows it must regenerate the prerequisites whenever any of the source or header files changes. 

Once you've defined the rule to remake the `.d' files, you then use the include directive to read them all in. See Include. For example: 

     sources = foo.c bar.c
     
     include $(sources:.c=.d)

(This example uses a substitution variable reference to translate the list of source files `foo.c bar.c' into a list of prerequisite makefiles, `foo.d bar.d'. See Substitution Refs, for full information on substitution references.) Since the `.d' files are makefiles like any others, make will remake them as necessary with no further work from you. See Remaking Makefiles. 

Note that the `.d' files contain target definitions; you should be sure to place the include directive after the first, default goal in your makefiles or run the risk of having a random object file become the default goal. See How Make Works. 



--------------------------------------------------------------------------------
Next: Using Variables, Previous: Rules, Up: Top 
5 Writing the Commands in Rules
The commands of a rule consist of one or more shell command lines to be executed, one at a time, in the order they appear. Typically, the result of executing these commands is that the target of the rule is brought up to date. 

Users use many different shell programs, but commands in makefiles are always interpreted by /bin/sh unless the makefile specifies otherwise. See Command Execution. 

Command Syntax: Command syntax features and pitfalls. 
Echoing: How to control when commands are echoed. 
Execution: How commands are executed. 
Parallel: How commands can be executed in parallel. 
Errors: What happens after a command execution error. 
Interrupts: What happens when a command is interrupted. 
Recursion: Invoking make from makefiles. 
Sequences: Defining canned sequences of commands. 
Empty Commands: Defining useful, do-nothing commands. 


--------------------------------------------------------------------------------
Next: Echoing, Previous: Commands, Up: Commands 
5.1 Command Syntax
Makefiles have the unusual property that there are really two distinct syntaxes in one file. Most of the makefile uses make syntax (see Writing Makefiles). However, commands are meant to be interpreted by the shell and so they are written using shell syntax. The make program does not try to understand shell syntax: it performs only a very few specific translations on the content of the command before handing it to the shell. 

Each command line must start with a tab, except that the first command line may be attached to the target-and-prerequisites line with a semicolon in between. Any line in the makefile that begins with a tab and appears in a  rule context  (that is, after a rule has been started until another rule or variable definition) will be considered a command line for that rule. Blank lines and lines of just comments may appear among the command lines; they are ignored. 

Some consequences of these rules include: 

A blank line that begins with a tab is not blank: it's an empty command (see Empty Commands). 

A comment in a command line is not a make comment; it will be passed to the shell as-is. Whether the shell treats it as a comment or not depends on your shell. 
A variable definition in a  rule context  which is indented by a tab as the first character on the line, will be considered a command line, not a make variable definition, and passed to the shell. 
A conditional expression (ifdef, ifeq, etc. see Syntax of Conditionals) in a  rule context  which is indented by a tab as the first character on the line, will be considered a command line and be passed to the shell. 
Splitting Lines: Breaking long command lines for readability. 
Variables in Commands: Using make variables in commands. 


--------------------------------------------------------------------------------
Next: Variables in Commands, Previous: Command Syntax, Up: Command Syntax 
5.1.1 Splitting Command Lines
One of the few ways in which make does interpret command lines is checking for a backslash just before the newline. As in normal makefile syntax, a single command can be split into multiple lines in the makefile by placing a backslash before each newline. A sequence of lines like this is considered a single command, and one instance of the shell will be invoked to run it. 

However, in contrast to how they are treated in other places in a makefile, backslash-newline pairs are not removed from the command. Both the backslash and the newline characters are preserved and passed to the shell. How the backslash-newline is interpreted depends on your shell. If the first character of the next line after the backslash-newline is a tab, then that tab (and only that tab) is removed. Whitespace is never added to the command. 

For example, this makefile: 

     all :
             @echo no\
     space
             @echo no\
             space
             @echo one \
             space
             @echo one\
              space

consists of four separate shell commands where the output is: 

     nospace
     nospace
     one space
     one space

As a more complex example, this makefile: 

     all : ; @echo 'hello \
             world' ; echo "hello \
         world"

will run one shell with a command script of: 

     echo 'hello \
     world' ; echo "hello \
         world"

which, according to shell quoting rules, will yield the following output: 

     hello \
     world
     hello     world

Notice how the backslash/newline pair was removed inside the string quoted with double quotes ("..."), but not from the string quoted with single quotes ('...'). This is the way the default shell (/bin/sh) handles backslash/newline pairs. If you specify a different shell in your makefiles it may treat them differently. 

Sometimes you want to split a long line inside of single quotes, but you don't want the backslash-newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the command. In this situation the newline quoting rules for makefiles will be used, and the backslash-newline will be removed. If we rewrite our example above using this method: 

     HELLO = 'hello \
     world'
     
     all : ; @echo $(HELLO)

we will get output like this: 

     hello world

If you like, you can also use target-specific variables (see Target-specific Variable Values) to obtain a tighter correspondence between the variable and the command that uses it. 



--------------------------------------------------------------------------------
Previous: Splitting Lines, Up: Command Syntax 
5.1.2 Using Variables in Commands
The other way in which make processes commands is by expanding any variable references in them (see Basics of Variable References). This occurs after make has finished reading all the makefiles and the target is determined to be out of date; so, the commands for targets which are not rebuilt are never expanded. 

Variable and function references in commands have identical syntax and semantics to references elsewhere in the makefile. They also have the same quoting rules: if you want a dollar sign to appear in your command, you must double it (`$$'). For shells like the default shell, that use dollar signs to introduce variables, it's important to keep clear in your mind whether the variable you want to reference is a make variable (use a single dollar sign) or a shell variable (use two dollar signs). For example: 

     LIST = one two three
     all:
             for i in $(LIST); do \
                 echo $$i; \
             done

results in the following command being passed to the shell: 

     for i in one two three; do \
         echo $i; \
     done

which generates the expected result: 

     one
     two
     three



--------------------------------------------------------------------------------
Next: Execution, Previous: Command Syntax, Up: Commands 
5.2 Command Echoing
Normally make prints each command line before it is executed. We call this echoing because it gives the appearance that you are typing the commands yourself. 

When a line starts with `@', the echoing of that line is suppressed. The `@' is discarded before the command is passed to the shell. Typically you would use this for a command whose only effect is to print something, such as an echo command to indicate progress through the makefile: 

     @echo About to make distribution files

When make is given the flag `-n' or `--just-print' it only echoes commands, it won't execute them. See Summary of Options. In this case and only this case, even the commands starting with `@' are printed. This flag is useful for finding out which commands make thinks are necessary without actually doing them. 

The `-s' or `--silent' flag to make prevents all echoing, as if all commands started with `@'. A rule in the makefile for the special target .SILENT without prerequisites has the same effect (see Special Built-in Target Names). .SILENT is essentially obsolete since `@' is more flexible. 



--------------------------------------------------------------------------------
Next: Parallel, Previous: Echoing, Up: Commands 
5.3 Command Execution
When it is time to execute commands to update a target, they are executed by invoking a new subshell for each command line. (In practice, make may take shortcuts that do not affect the results.) 

Please note: this implies that setting shell variables and invoking shell commands such as cd that set a context local to each process will not affect the following command lines.2 If you want to use cd to affect the next statement, put both statements in a single command line. Then make will invoke one shell to run the entire line, and the shell will execute the statements in sequence. For example: 

     foo : bar/lose
             cd $(@D) && gobble $(@F) > ../$@

Here we use the shell AND operator (&&) so that if the cd command fails, the script will fail without trying to invoke the gobble command in the wrong directory, which could cause problems (in this case it would certainly cause ../foo to be truncated, at least). 

Choosing the Shell: How make chooses the shell used to run commands. 


--------------------------------------------------------------------------------
Previous: Execution, Up: Execution 
5.3.1 Choosing the Shell
The program used as the shell is taken from the variable SHELL. If this variable is not set in your makefile, the program /bin/sh is used as the shell. 

Unlike most variables, the variable SHELL is never set from the environment. This is because the SHELL environment variable is used to specify your personal choice of shell program for interactive use. It would be very bad for personal choices like this to affect the functioning of makefiles. See Variables from the Environment. 

Furthermore, when you do set SHELL in your makefile that value is not exported in the environment to commands that make invokes. Instead, the value inherited from the user's environment, if any, is exported. You can override this behavior by explicitly exporting SHELL (see Communicating Variables to a Sub-make), forcing it to be passed in the environment to commands. 

However, on MS-DOS and MS-Windows the value of SHELL in the environment is used, since on those systems most users do not set this variable, and therefore it is most likely set specifically to be used by make. On MS-DOS, if the setting of SHELL is not suitable for make, you can set the variable MAKESHELL to the shell that make should use; if set it will be used as the shell instead of the value of SHELL. 

Choosing a Shell in DOS and Windows
Choosing a shell in MS-DOS and MS-Windows is much more complex than on other systems. 

On MS-DOS, if SHELL is not set, the value of the variable COMSPEC (which is always set) is used instead. 

The processing of lines that set the variable SHELL in Makefiles is different on MS-DOS. The stock shell, command.com, is ridiculously limited in its functionality and many users of make tend to install a replacement shell. Therefore, on MS-DOS, make examines the value of SHELL, and changes its behavior based on whether it points to a Unix-style or DOS-style shell. This allows reasonable functionality even if SHELL points to command.com. 

If SHELL points to a Unix-style shell, make on MS-DOS additionally checks whether that shell can indeed be found; if not, it ignores the line that sets SHELL. In MS-DOS, GNU make searches for the shell in the following places: 

In the precise place pointed to by the value of SHELL. For example, if the makefile specifies `SHELL = /bin/sh', make will look in the directory /bin on the current drive. 
In the current directory. 
In each of the directories in the PATH variable, in order. 
In every directory it examines, make will first look for the specific file (sh in the example above). If this is not found, it will also look in that directory for that file with one of the known extensions which identify executable files. For example .exe, .com, .bat, .btm, .sh, and some others. 

If any of these attempts is successful, the value of SHELL will be set to the full pathname of the shell as found. However, if none of these is found, the value of SHELL will not be changed, and thus the line that sets it will be effectively ignored. This is so make will only support features specific to a Unix-style shell if such a shell is actually installed on the system where make runs. 

Note that this extended search for the shell is limited to the cases where SHELL is set from the Makefile; if it is set in the environment or command line, you are expected to set it to the full pathname of the shell, exactly as things are on Unix. 

The effect of the above DOS-specific processing is that a Makefile that contains `SHELL = /bin/sh' (as many Unix makefiles do), will work on MS-DOS unaltered if you have e.g. sh.exe installed in some directory along your PATH. 



--------------------------------------------------------------------------------
Next: Errors, Previous: Execution, Up: Commands 
5.4 Parallel Execution
GNU make knows how to execute several commands at once. Normally, make will execute only one command at a time, waiting for it to finish before executing the next. However, the `-j' or `--jobs' option tells make to execute many commands simultaneously. 

On MS-DOS, the `-j' option has no effect, since that system doesn't support multi-processing. 

If the `-j' option is followed by an integer, this is the number of commands to execute at once; this is called the number of job slots. If there is nothing looking like an integer after the `-j' option, there is no limit on the number of job slots. The default number of job slots is one, which means serial execution (one thing at a time). 

One unpleasant consequence of running several commands simultaneously is that output generated by the commands appears whenever each command sends it, so messages from different commands may be interspersed. 

Another problem is that two processes cannot both take input from the same device; so to make sure that only one command tries to take input from the terminal at once, make will invalidate the standard input streams of all but one running command. This means that attempting to read from standard input will usually be a fatal error (a `Broken pipe' signal) for most child processes if there are several. It is unpredictable which command will have a valid standard input stream (which will come from the terminal, or wherever you redirect the standard input of make). The first command run will always get it first, and the first command started after that one finishes will get it next, and so on. 

We will change how this aspect of make works if we find a better alternative. In the mean time, you should not rely on any command using standard input at all if you are using the parallel execution feature; but if you are not using this feature, then standard input works normally in all commands. 

Finally, handling recursive make invocations raises issues. For more information on this, see Communicating Options to a Sub-make. 

If a command fails (is killed by a signal or exits with a nonzero status), and errors are not ignored for that command (see Errors in Commands), the remaining command lines to remake the same target will not be run. If a command fails and the `-k' or `--keep-going' option was not given (see Summary of Options), make aborts execution. If make terminates for any reason (including a signal) with child processes running, it waits for them to finish before actually exiting. 

When the system is heavily loaded, you will probably want to run fewer jobs than when it is lightly loaded. You can use the `-l' option to tell make to limit the number of jobs to run at once, based on the load average. The `-l' or `--max-load' option is followed by a floating-point number. For example, 

     -l 2.5

will not let make start more than one job if the load average is above 2.5. The `-l' option with no following number removes the load limit, if one was given with a previous `-l' option. 

More precisely, when make goes to start up a job, and it already has at least one job running, it checks the current load average; if it is not lower than the limit given with `-l', make waits until the load average goes below that limit, or until all the other jobs finish. 

By default, there is no load limit. 



--------------------------------------------------------------------------------
Next: Interrupts, Previous: Parallel, Up: Commands 
5.5 Errors in Commands
After each shell command returns, make looks at its exit status. If the command completed successfully, the next command line is executed in a new shell; after the last command line is finished, the rule is finished. 

If there is an error (the exit status is nonzero), make gives up on the current rule, and perhaps on all rules. 

Sometimes the failure of a certain command does not indicate a problem. For example, you may use the mkdir command to ensure that a directory exists. If the directory already exists, mkdir will report an error, but you probably want make to continue regardless. 

To ignore errors in a command line, write a `-' at the beginning of the line's text (after the initial tab). The `-' is discarded before the command is passed to the shell for execution. 

For example, 

     clean:
             -rm -f *.o

This causes rm to continue even if it is unable to remove a file. 

When you run make with the `-i' or `--ignore-errors' flag, errors are ignored in all commands of all rules. A rule in the makefile for the special target .IGNORE has the same effect, if there are no prerequisites. These ways of ignoring errors are obsolete because `-' is more flexible. 

When errors are to be ignored, because of either a `-' or the `-i' flag, make treats an error return just like success, except that it prints out a message that tells you the status code the command exited with, and says that the error has been ignored. 

When an error happens that make has not been told to ignore, it implies that the current target cannot be correctly remade, and neither can any other that depends on it either directly or indirectly. No further commands will be executed for these targets, since their preconditions have not been achieved. 

Normally make gives up immediately in this circumstance, returning a nonzero status. However, if the `-k' or `--keep-going' flag is specified, make continues to consider the other prerequisites of the pending targets, remaking them if necessary, before it gives up and returns nonzero status. For example, after an error in compiling one object file, `make -k' will continue compiling other object files even though it already knows that linking them will be impossible. See Summary of Options. 

The usual behavior assumes that your purpose is to get the specified targets up to date; once make learns that this is impossible, it might as well report the failure immediately. The `-k' option says that the real purpose is to test as many of the changes made in the program as possible, perhaps to find several independent problems so that you can correct them all before the next attempt to compile. This is why Emacs' compile command passes the `-k' flag by default. Usually when a command fails, if it has changed the target file at all, the file is corrupted and cannot be used or at least it is not completely updated. Yet the file's time stamp says that it is now up to date, so the next time make runs, it will not try to update that file. The situation is just the same as when the command is killed by a signal; see Interrupts. So generally the right thing to do is to delete the target file if the command fails after beginning to change the file. make will do this if .DELETE_ON_ERROR appears as a target. This is almost always what you want make to do, but it is not historical practice; so for compatibility, you must explicitly request it. 



--------------------------------------------------------------------------------
Next: Recursion, Previous: Errors, Up: Commands 
5.6 Interrupting or Killing make
If make gets a fatal signal while a command is executing, it may delete the target file that the command was supposed to update. This is done if the target file's last-modification time has changed since make first checked it. 

The purpose of deleting the target is to make sure that it is remade from scratch when make is next run. Why is this? Suppose you type Ctrl-c while a compiler is running, and it has begun to write an object file foo.o. The Ctrl-c kills the compiler, resulting in an incomplete file whose last-modification time is newer than the source file foo.c. But make also receives the Ctrl-c signal and deletes this incomplete file. If make did not do this, the next invocation of make would think that foo.o did not require updating resulting in a strange error message from the linker when it tries to link an object file half of which is missing. 

You can prevent the deletion of a target file in this way by making the special target .PRECIOUS depend on it. Before remaking a target, make checks to see whether it appears on the prerequisites of .PRECIOUS, and thereby decides whether the target should be deleted if a signal happens. Some reasons why you might do this are that the target is updated in some atomic fashion, or exists only to record a modification-time (its contents do not matter), or must exist at all times to prevent other sorts of trouble. 



--------------------------------------------------------------------------------
Next: Sequences, Previous: Interrupts, Up: Commands 
5.7 Recursive Use of make
Recursive use of make means using make as a command in a makefile. This technique is useful when you want separate makefiles for various subsystems that compose a larger system. For example, suppose you have a subdirectory subdir which has its own makefile, and you would like the containing directory's makefile to run make on the subdirectory. You can do it by writing this: 

     subsystem:
             cd subdir && $(MAKE)

or, equivalently, this (see Summary of Options): 

     subsystem:
             $(MAKE) -C subdir

You can write recursive make commands just by copying this example, but there are many things to know about how they work and why, and about how the sub-make relates to the top-level make. You may also find it useful to declare targets that invoke recursive make commands as `.PHONY' (for more discussion on when this is useful, see Phony Targets). 

For your convenience, when GNU make starts (after it has processed any -C options) it sets the variable CURDIR to the pathname of the current working directory. This value is never touched by make again: in particular note that if you include files from other directories the value of CURDIR does not change. The value has the same precedence it would have if it were set in the makefile (by default, an environment variable CURDIR will not override this value). Note that setting this variable has no impact on the operation of make (it does not cause make to change its working directory, for example). 

MAKE Variable: The special effects of using `$(MAKE)'. 
Variables/Recursion: How to communicate variables to a sub-make. 
Options/Recursion: How to communicate options to a sub-make. 
-w Option: How the `-w' or `--print-directory' option helps debug use of recursive make commands. 


--------------------------------------------------------------------------------
Next: Variables/Recursion, Previous: Recursion, Up: Recursion 
5.7.1 How the MAKE Variable Works
Recursive make commands should always use the variable MAKE, not the explicit command name `make', as shown here: 

     subsystem:
             cd subdir && $(MAKE)

The value of this variable is the file name with which make was invoked. If this file name was /bin/make, then the command executed is `cd subdir && /bin/make'. If you use a special version of make to run the top-level makefile, the same special version will be executed for recursive invocations. As a special feature, using the variable MAKE in the commands of a rule alters the effects of the `-t' (`--touch'), `-n' (`--just-print'), or `-q' (`--question') option. Using the MAKE variable has the same effect as using a `+' character at the beginning of the command line. See Instead of Executing the Commands. This special feature is only enabled if the MAKE variable appears directly in the command script: it does not apply if the MAKE variable is referenced through expansion of another variable. In the latter case you must use the `+' token to get these special effects. 

Consider the command `make -t' in the above example. (The `-t' option marks targets as up to date without actually running any commands; see Instead of Execution.) Following the usual definition of `-t', a `make -t' command in the example would create a file named subsystem and do nothing else. What you really want it to do is run `cd subdir && make -t'; but that would require executing the command, and `-t' says not to execute commands. The special feature makes this do what you want: whenever a command line of a rule contains the variable MAKE, the flags `-t', `-n' and `-q' do not apply to that line. Command lines containing MAKE are executed normally despite the presence of a flag that causes most commands not to be run. The usual MAKEFLAGS mechanism passes the flags to the sub-make (see Communicating Options to a Sub-make), so your request to touch the files, or print the commands, is propagated to the subsystem. 



--------------------------------------------------------------------------------
Next: Options/Recursion, Previous: MAKE Variable, Up: Recursion 
5.7.2 Communicating Variables to a Sub-make
Variable values of the top-level make can be passed to the sub-make through the environment by explicit request. These variables are defined in the sub-make as defaults, but do not override what is specified in the makefile used by the sub-make makefile unless you use the `-e' switch (see Summary of Options). 

To pass down, or export, a variable, make adds the variable and its value to the environment for running each command. The sub-make, in turn, uses the environment to initialize its table of variable values. See Variables from the Environment. 

Except by explicit request, make exports a variable only if it is either defined in the environment initially or set on the command line, and if its name consists only of letters, numbers, and underscores. Some shells cannot cope with environment variable names consisting of characters other than letters, numbers, and underscores. 

The value of the make variable SHELL is not exported. Instead, the value of the SHELL variable from the invoking environment is passed to the sub-make. You can force make to export its value for SHELL by using the export directive, described below. See Choosing the Shell. 

The special variable MAKEFLAGS is always exported (unless you unexport it). MAKEFILES is exported if you set it to anything. 

make automatically passes down variable values that were defined on the command line, by putting them in the MAKEFLAGS variable. See Options/Recursion. 

Variables are not normally passed down if they were created by default by make (see Variables Used by Implicit Rules). The sub-make will define these for itself. 

If you want to export specific variables to a sub-make, use the export directive, like this: 

     export variable ...

If you want to prevent a variable from being exported, use the unexport directive, like this: 

     unexport variable ...

In both of these forms, the arguments to export and unexport are expanded, and so could be variables or functions which expand to a (list of) variable names to be (un)exported. 

As a convenience, you can define a variable and export it at the same time by doing: 

     export variable = value

has the same result as: 

     variable = value
     export variable

and 

     export variable := value

has the same result as: 

     variable := value
     export variable

Likewise, 

     export variable += value

is just like: 

     variable += value
     export variable

See Appending More Text to Variables. 

You may notice that the export and unexport directives work in make in the same way they work in the shell, sh. 

If you want all variables to be exported by default, you can use export by itself: 

     export

This tells make that variables which are not explicitly mentioned in an export or unexport directive should be exported. Any variable given in an unexport directive will still not be exported. If you use export by itself to export variables by default, variables whose names contain characters other than alphanumerics and underscores will not be exported unless specifically mentioned in an export directive. 

The behavior elicited by an export directive by itself was the default in older versions of GNU make. If your makefiles depend on this behavior and you want to be compatible with old versions of make, you can write a rule for the special target .EXPORT_ALL_VARIABLES instead of using the export directive. This will be ignored by old makes, while the export directive will cause a syntax error. Likewise, you can use unexport by itself to tell make not to export variables by default. Since this is the default behavior, you would only need to do this if export had been used by itself earlier (in an included makefile, perhaps). You cannot use export and unexport by themselves to have variables exported for some commands and not for others. The last export or unexport directive that appears by itself determines the behavior for the entire run of make. 

As a special feature, the variable MAKELEVEL is changed when it is passed down from level to level. This variable's value is a string which is the depth of the level as a decimal number. The value is `0' for the top-level make; `1' for a sub-make, `2' for a sub-sub-make, and so on. The incrementation happens when make sets up the environment for a command. 

The main use of MAKELEVEL is to test it in a conditional directive (see Conditional Parts of Makefiles); this way you can write a makefile that behaves one way if run recursively and another way if run directly by you. 

You can use the variable MAKEFILES to cause all sub-make commands to use additional makefiles. The value of MAKEFILES is a whitespace-separated list of file names. This variable, if defined in the outer-level makefile, is passed down through the environment; then it serves as a list of extra makefiles for the sub-make to read before the usual or specified ones. See The Variable MAKEFILES. 



--------------------------------------------------------------------------------
Next: -w Option, Previous: Variables/Recursion, Up: Recursion 
5.7.3 Communicating Options to a Sub-make
Flags such as `-s' and `-k' are passed automatically to the sub-make through the variable MAKEFLAGS. This variable is set up automatically by make to contain the flag letters that make received. Thus, if you do `make -ks' then MAKEFLAGS gets the value `ks'. 

As a consequence, every sub-make gets a value for MAKEFLAGS in its environment. In response, it takes the flags from that value and processes them as if they had been given as arguments. See Summary of Options. 

Likewise variables defined on the command line are passed to the sub-make through MAKEFLAGS. Words in the value of MAKEFLAGS that contain `=', make treats as variable definitions just as if they appeared on the command line. See Overriding Variables. 

The options `-C', `-f', `-o', and `-W' are not put into MAKEFLAGS; these options are not passed down. 

The `-j' option is a special case (see Parallel Execution). If you set it to some numeric value `N' and your operating system supports it (most any UNIX system will; others typically won't), the parent make and all the sub-makes will communicate to ensure that there are only `N' jobs running at the same time between them all. Note that any job that is marked recursive (see Instead of Executing the Commands) doesn't count against the total jobs (otherwise we could get `N' sub-makes running and have no slots left over for any real work!) 

If your operating system doesn't support the above communication, then `-j 1' is always put into MAKEFLAGS instead of the value you specified. This is because if the `-j' option were passed down to sub-makes, you would get many more jobs running in parallel than you asked for. If you give `-j' with no numeric argument, meaning to run as many jobs as possible in parallel, this is passed down, since multiple infinities are no more than one. 

If you do not want to pass the other flags down, you must change the value of MAKEFLAGS, like this: 

     subsystem:
             cd subdir && $(MAKE) MAKEFLAGS=

The command line variable definitions really appear in the variable MAKEOVERRIDES, and MAKEFLAGS contains a reference to this variable. If you do want to pass flags down normally, but don't want to pass down the command line variable definitions, you can reset MAKEOVERRIDES to empty, like this: 

     MAKEOVERRIDES =

This is not usually useful to do. However, some systems have a small fixed limit on the size of the environment, and putting so much information into the value of MAKEFLAGS can exceed it. If you see the error message `Arg list too long', this may be the problem. (For strict compliance with POSIX.2, changing MAKEOVERRIDES does not affect MAKEFLAGS if the special target `.POSIX' appears in the makefile. You probably do not care about this.) 

A similar variable MFLAGS exists also, for historical compatibility. It has the same value as MAKEFLAGS except that it does not contain the command line variable definitions, and it always begins with a hyphen unless it is empty (MAKEFLAGS begins with a hyphen only when it begins with an option that has no single-letter version, such as `--warn-undefined-variables'). MFLAGS was traditionally used explicitly in the recursive make command, like this: 

     subsystem:
             cd subdir && $(MAKE) $(MFLAGS)

but now MAKEFLAGS makes this usage redundant. If you want your makefiles to be compatible with old make programs, use this technique; it will work fine with more modern make versions too. 

The MAKEFLAGS variable can also be useful if you want to have certain options, such as `-k' (see Summary of Options), set each time you run make. You simply put a value for MAKEFLAGS in your environment. You can also set MAKEFLAGS in a makefile, to specify additional flags that should also be in effect for that makefile. (Note that you cannot use MFLAGS this way. That variable is set only for compatibility; make does not interpret a value you set for it in any way.) 

When make interprets the value of MAKEFLAGS (either from the environment or from a makefile), it first prepends a hyphen if the value does not already begin with one. Then it chops the value into words separated by blanks, and parses these words as if they were options given on the command line (except that `-C', `-f', `-h', `-o', `-W', and their long-named versions are ignored; and there is no error for an invalid option). 

If you do put MAKEFLAGS in your environment, you should be sure not to include any options that will drastically affect the actions of make and undermine the purpose of makefiles and of make itself. For instance, the `-t', `-n', and `-q' options, if put in one of these variables, could have disastrous consequences and would certainly have at least surprising and probably annoying effects. 



--------------------------------------------------------------------------------
Previous: Options/Recursion, Up: Recursion 
5.7.4 The `--print-directory' Option
If you use several levels of recursive make invocations, the `-w' or `--print-directory' option can make the output a lot easier to understand by showing each directory as make starts processing it and as make finishes processing it. For example, if `make -w' is run in the directory /u/gnu/make, make will print a line of the form: 

     make: Entering directory `/u/gnu/make'.

before doing anything else, and a line of the form: 

     make: Leaving directory `/u/gnu/make'.

when processing is completed. 

Normally, you do not need to specify this option because `make' does it for you: `-w' is turned on automatically when you use the `-C' option, and in sub-makes. make will not automatically turn on `-w' if you also use `-s', which says to be silent, or if you use `--no-print-directory' to explicitly disable it. 



--------------------------------------------------------------------------------
Next: Empty Commands, Previous: Recursion, Up: Commands 
5.8 Defining Canned Command Sequences
When the same sequence of commands is useful in making various targets, you can define it as a canned sequence with the define directive, and refer to the canned sequence from the rules for those targets. The canned sequence is actually a variable, so the name must not conflict with other variable names. 

Here is an example of defining a canned sequence of commands: 

     define run-yacc
     yacc $(firstword $^)
     mv y.tab.c $@
     endef

Here run-yacc is the name of the variable being defined; endef marks the end of the definition; the lines in between are the commands. The define directive does not expand variable references and function calls in the canned sequence; the `$' characters, parentheses, variable names, and so on, all become part of the value of the variable you are defining. See Defining Variables Verbatim, for a complete explanation of define. 

The first command in this example runs Yacc on the first prerequisite of whichever rule uses the canned sequence. The output file from Yacc is always named y.tab.c. The second command moves the output to the rule's target file name. 

To use the canned sequence, substitute the variable into the commands of a rule. You can substitute it like any other variable (see Basics of Variable References). Because variables defined by define are recursively expanded variables, all the variable references you wrote inside the define are expanded now. For example: 

     foo.c : foo.y
             $(run-yacc)

`foo.y' will be substituted for the variable `$^' when it occurs in run-yacc's value, and `foo.c' for `$@'. 

This is a realistic example, but this particular one is not needed in practice because make has an implicit rule to figure out these commands based on the file names involved (see Using Implicit Rules). 

In command execution, each line of a canned sequence is treated just as if the line appeared on its own in the rule, preceded by a tab. In particular, make invokes a separate subshell for each line. You can use the special prefix characters that affect command lines (`@', `-', and `+') on each line of a canned sequence. See Writing the Commands in Rules. For example, using this canned sequence: 

     define frobnicate
     @echo "frobnicating target $@"
     frob-step-1 $< -o $@-step-1
     frob-step-2 $@-step-1 -o $@
     endef

make will not echo the first line, the echo command. But it will echo the following two command lines. 

On the other hand, prefix characters on the command line that refers to a canned sequence apply to every line in the sequence. So the rule: 

     frob.out: frob.in
             @$(frobnicate)

does not echo any commands. (See Command Echoing, for a full explanation of `@'.) 



--------------------------------------------------------------------------------
Previous: Sequences, Up: Commands 
5.9 Using Empty Commands
It is sometimes useful to define commands which do nothing. This is done simply by giving a command that consists of nothing but whitespace. For example: 

     target: ;

defines an empty command string for target. You could also use a line beginning with a tab character to define an empty command string, but this would be confusing because such a line looks empty. 

You may be wondering why you would want to define a command string that does nothing. The only reason this is useful is to prevent a target from getting implicit commands (from implicit rules or the .DEFAULT special target; see Implicit Rules and see Defining Last-Resort Default Rules). 

You may be inclined to define empty command strings for targets that are not actual files, but only exist so that their prerequisites can be remade. However, this is not the best way to do that, because the prerequisites may not be remade properly if the target file actually does exist. See Phony Targets, for a better way to do this. 



--------------------------------------------------------------------------------
Next: Conditionals, Previous: Commands, Up: Top 
6 How to Use Variables
A variable is a name defined in a makefile to represent a string of text, called the variable's value. These values are substituted by explicit request into targets, prerequisites, commands, and other parts of the makefile. (In some other versions of make, variables are called macros.) Variables and functions in all parts of a makefile are expanded when read, except for the shell commands in rules, the right-hand sides of variable definitions using `=', and the bodies of variable definitions using the define directive. 

Variables can represent lists of file names, options to pass to compilers, programs to run, directories to look in for source files, directories to write output in, or anything else you can imagine. 

A variable name may be any sequence of characters not containing `:', `#', `=', or leading or trailing whitespace. However, variable names containing characters other than letters, numbers, and underscores should be avoided, as they may be given special meanings in the future, and with some shells they cannot be passed through the environment to a sub-make (see Communicating Variables to a Sub-make). 

Variable names are case-sensitive. The names `foo', `FOO', and `Foo' all refer to different variables. 

It is traditional to use upper case letters in variable names, but we recommend using lower case letters for variable names that serve internal purposes in the makefile, and reserving upper case for parameters that control implicit rules or for parameters that the user should override with command options (see Overriding Variables). 

A few variables have names that are a single punctuation character or just a few characters. These are the automatic variables, and they have particular specialized uses. See Automatic Variables. 

Reference: How to use the value of a variable. 
Flavors: Variables come in two flavors. 
Advanced: Advanced features for referencing a variable. 
Values: All the ways variables get their values. 
Setting: How to set a variable in the makefile. 
Appending: How to append more text to the old value of a variable. 
Override Directive: How to set a variable in the makefile even if the user has set it with a command argument. 
Defining: An alternate way to set a variable to a verbatim string. 
Environment: Variable values can come from the environment. 
Target-specific: Variable values can be defined on a per-target basis. 
Pattern-specific: Target-specific variable values can be applied to a group of targets that match a pattern. 


--------------------------------------------------------------------------------
Next: Flavors, Previous: Using Variables, Up: Using Variables 
6.1 Basics of Variable References
To substitute a variable's value, write a dollar sign followed by the name of the variable in parentheses or braces: either `$(foo)' or `${foo}' is a valid reference to the variable foo. This special significance of `$' is why you must write `$$' to have the effect of a single dollar sign in a file name or command. 

Variable references can be used in any context: targets, prerequisites, commands, most directives, and new variable values. Here is an example of a common case, where a variable holds the names of all the object files in a program: 

     objects = program.o foo.o utils.o
     program : $(objects)
             cc -o program $(objects)
     
     $(objects) : defs.h

Variable references work by strict textual substitution. Thus, the rule 

     foo = c
     prog.o : prog.$(foo)
             $(foo)$(foo) -$(foo) prog.$(foo)

could be used to compile a C program prog.c. Since spaces before the variable value are ignored in variable assignments, the value of foo is precisely `c'. (Don't actually write your makefiles this way!) 

A dollar sign followed by a character other than a dollar sign, open-parenthesis or open-brace treats that single character as the variable name. Thus, you could reference the variable x with `$x'. However, this practice is strongly discouraged, except in the case of the automatic variables (see Automatic Variables). 



--------------------------------------------------------------------------------
Next: Advanced, Previous: Reference, Up: Using Variables 
6.2 The Two Flavors of Variables
There are two ways that a variable in GNU make can have a value; we call them the two flavors of variables. The two flavors are distinguished in how they are defined and in what they do when expanded. 

The first flavor of variable is a recursively expanded variable. Variables of this sort are defined by lines using `=' (see Setting Variables) or by the define directive (see Defining Variables Verbatim). The value you specify is installed verbatim; if it contains references to other variables, these references are expanded whenever this variable is substituted (in the course of expanding some other string). When this happens, it is called recursive expansion. 

For example, 

     foo = $(bar)
     bar = $(ugh)
     ugh = Huh?
     
     all:;echo $(foo)

will echo `Huh?': `$(foo)' expands to `$(bar)' which expands to `$(ugh)' which finally expands to `Huh?'. 

This flavor of variable is the only sort supported by other versions of make. It has its advantages and its disadvantages. An advantage (most would say) is that: 

     CFLAGS = $(include_dirs) -O
     include_dirs = -Ifoo -Ibar

will do what was intended: when `CFLAGS' is expanded in a command, it will expand to `-Ifoo -Ibar -O'. A major disadvantage is that you cannot append something on the end of a variable, as in 

     CFLAGS = $(CFLAGS) -O

because it will cause an infinite loop in the variable expansion. (Actually make detects the infinite loop and reports an error.) Another disadvantage is that any functions (see Functions for Transforming Text) referenced in the definition will be executed every time the variable is expanded. This makes make run slower; worse, it causes the wildcard and shell functions to give unpredictable results because you cannot easily control when they are called, or even how many times. 

To avoid all the problems and inconveniences of recursively expanded variables, there is another flavor: simply expanded variables. 

Simply expanded variables are defined by lines using `:=' (see Setting Variables). The value of a simply expanded variable is scanned once and for all, expanding any references to other variables and functions, when the variable is defined. The actual value of the simply expanded variable is the result of expanding the text that you write. It does not contain any references to other variables; it contains their values as of the time this variable was defined. Therefore, 

     x := foo
     y := $(x) bar
     x := later

is equivalent to 

     y := foo bar
     x := later

When a simply expanded variable is referenced, its value is substituted verbatim. 

Here is a somewhat more complicated example, illustrating the use of `:=' in conjunction with the shell function. (See The shell Function.) This example also shows use of the variable MAKELEVEL, which is changed when it is passed down from level to level. (See Communicating Variables to a Sub-make, for information about MAKELEVEL.) 


     ifeq (0,${MAKELEVEL})
     whoami    := $(shell whoami)
     host-type := $(shell arch)
     MAKE := ${MAKE} host-type=${host-type} whoami=${whoami}
     endif

An advantage of this use of `:=' is that a typical `descend into a directory' command then looks like this: 

     ${subdirs}:
             ${MAKE} -C $@ all

Simply expanded variables generally make complicated makefile programming more predictable because they work like variables in most programming languages. They allow you to redefine a variable using its own value (or its value processed in some way by one of the expansion functions) and to use the expansion functions much more efficiently (see Functions for Transforming Text). 

You can also use them to introduce controlled leading whitespace into variable values. Leading whitespace characters are discarded from your input before substitution of variable references and function calls; this means you can include leading spaces in a variable value by protecting them with variable references, like this: 

     nullstring :=
     space := $(nullstring) # end of the line

Here the value of the variable space is precisely one space. The comment `# end of the line' is included here just for clarity. Since trailing space characters are not stripped from variable values, just a space at the end of the line would have the same effect (but be rather hard to read). If you put whitespace at the end of a variable value, it is a good idea to put a comment like that at the end of the line to make your intent clear. Conversely, if you do not want any whitespace characters at the end of your variable value, you must remember not to put a random comment on the end of the line after some whitespace, such as this: 

     dir := /foo/bar    # directory to put the frobs in

Here the value of the variable dir is `/foo/bar    ' (with four trailing spaces), which was probably not the intention. (Imagine something like `$(dir)/file' with this definition!) 

There is another assignment operator for variables, `?='. This is called a conditional variable assignment operator, because it only has an effect if the variable is not yet defined. This statement: 

     FOO ?= bar

is exactly equivalent to this (see The origin Function): 

     ifeq ($(origin FOO), undefined)
       FOO = bar
     endif

Note that a variable set to an empty value is still defined, so `?=' will not set that variable. 



--------------------------------------------------------------------------------
Next: Values, Previous: Flavors, Up: Using Variables 
6.3 Advanced Features for Reference to Variables
This section describes some advanced features you can use to reference variables in more flexible ways. 

Substitution Refs: Referencing a variable with substitutions on the value. 
Computed Names: Computing the name of the variable to refer to. 


--------------------------------------------------------------------------------
Next: Computed Names, Previous: Advanced, Up: Advanced 
6.3.1 Substitution References
A substitution reference substitutes the value of a variable with alterations that you specify. It has the form `$(var:a=b)' (or `${var:a=b}') and its meaning is to take the value of the variable var, replace every a at the end of a word with b in that value, and substitute the resulting string. 

When we say  at the end of a word , we mean that a must appear either followed by whitespace or at the end of the value in order to be replaced; other occurrences of a in the value are unaltered. For example: 

     foo := a.o b.o c.o
     bar := $(foo:.o=.c)

sets `bar' to `a.c b.c c.c'. See Setting Variables. 

A substitution reference is actually an abbreviation for use of the patsubst expansion function (see Functions for String Substitution and Analysis). We provide substitution references as well as patsubst for compatibility with other implementations of make. 

Another type of substitution reference lets you use the full power of the patsubst function. It has the same form `$(var:a=b)' described above, except that now a must contain a single `%' character. This case is equivalent to `$(patsubst a,b,$(var))'. See Functions for String Substitution and Analysis, for a description of the patsubst function. 

For example:
     
     foo := a.o b.o c.o
     bar := $(foo:%.o=%.c)

sets `bar' to `a.c b.c c.c'. 



--------------------------------------------------------------------------------
Previous: Substitution Refs, Up: Advanced 
6.3.2 Computed Variable Names
Computed variable names are a complicated concept needed only for sophisticated makefile programming. For most purposes you need not consider them, except to know that making a variable with a dollar sign in its name might have strange results. However, if you are the type that wants to understand everything, or you are actually interested in what they do, read on. 

Variables may be referenced inside the name of a variable. This is called a computed variable name or a nested variable reference. For example, 

     x = y
     y = z
     a := $($(x))

defines a as `z': the `$(x)' inside `$($(x))' expands to `y', so `$($(x))' expands to `$(y)' which in turn expands to `z'. Here the name of the variable to reference is not stated explicitly; it is computed by expansion of `$(x)'. The reference `$(x)' here is nested within the outer variable reference. 

The previous example shows two levels of nesting, but any number of levels is possible. For example, here are three levels: 

     x = y
     y = z
     z = u
     a := $($($(x)))

Here the innermost `$(x)' expands to `y', so `$($(x))' expands to `$(y)' which in turn expands to `z'; now we have `$(z)', which becomes `u'. 

References to recursively-expanded variables within a variable name are reexpanded in the usual fashion. For example: 

     x = $(y)
     y = z
     z = Hello
     a := $($(x))

defines a as `Hello': `$($(x))' becomes `$($(y))' which becomes `$(z)' which becomes `Hello'. 

Nested variable references can also contain modified references and function invocations (see Functions for Transforming Text), just like any other reference. For example, using the subst function (see Functions for String Substitution and Analysis): 

     x = variable1
     variable2 := Hello
     y = $(subst 1,2,$(x))
     z = y
     a := $($($(z)))

eventually defines a as `Hello'. It is doubtful that anyone would ever want to write a nested reference as convoluted as this one, but it works: `$($($(z)))' expands to `$($(y))' which becomes `$($(subst 1,2,$(x)))'. This gets the value `variable1' from x and changes it by substitution to `variable2', so that the entire string becomes `$(variable2)', a simple variable reference whose value is `Hello'. 

A computed variable name need not consist entirely of a single variable reference. It can contain several variable references, as well as some invariant text. For example, 

     a_dirs := dira dirb
     1_dirs := dir1 dir2
     
     a_files := filea fileb
     1_files := file1 file2
     
     ifeq "$(use_a)" "yes"
     a1 := a
     else
     a1 := 1
     endif
     
     ifeq "$(use_dirs)" "yes"
     df := dirs
     else
     df := files
     endif
     
     dirs := $($(a1)_$(df))

will give dirs the same value as a_dirs, 1_dirs, a_files or 1_files depending on the settings of use_a and use_dirs. 

Computed variable names can also be used in substitution references: 

     a_objects := a.o b.o c.o
     1_objects := 1.o 2.o 3.o
     
     sources := $($(a1)_objects:.o=.c)

defines sources as either `a.c b.c c.c' or `1.c 2.c 3.c', depending on the value of a1. 

The only restriction on this sort of use of nested variable references is that they cannot specify part of the name of a function to be called. This is because the test for a recognized function name is done before the expansion of nested references. For example, 

     ifdef do_sort
     func := sort
     else
     func := strip
     endif
     
     bar := a d b g q c
     
     foo := $($(func) $(bar))

attempts to give `foo' the value of the variable `sort a d b g q c' or `strip a d b g q c', rather than giving `a d b g q c' as the argument to either the sort or the strip function. This restriction could be removed in the future if that change is shown to be a good idea. 

You can also use computed variable names in the left-hand side of a variable assignment, or in a define directive, as in: 

     dir = foo
     $(dir)_sources := $(wildcard $(dir)/*.c)
     define $(dir)_print
     lpr $($(dir)_sources)
     endef

This example defines the variables `dir', `foo_sources', and `foo_print'. 

Note that nested variable references are quite different from recursively expanded variables (see The Two Flavors of Variables), though both are used together in complex ways when doing makefile programming. 



--------------------------------------------------------------------------------
Next: Setting, Previous: Advanced, Up: Using Variables 
6.4 How Variables Get Their Values
Variables can get values in several different ways: 

You can specify an overriding value when you run make. See Overriding Variables. 
You can specify a value in the makefile, either with an assignment (see Setting Variables) or with a verbatim definition (see Defining Variables Verbatim). 
Variables in the environment become make variables. See Variables from the Environment. 
Several automatic variables are given new values for each rule. Each of these has a single conventional use. See Automatic Variables. 
Several variables have constant initial values. See Variables Used by Implicit Rules. 


--------------------------------------------------------------------------------
Next: Appending, Previous: Values, Up: Using Variables 
6.5 Setting Variables
To set a variable from the makefile, write a line starting with the variable name followed by `=' or `:='. Whatever follows the `=' or `:=' on the line becomes the value. For example, 

     objects = main.o foo.o bar.o utils.o

defines a variable named objects. Whitespace around the variable name and immediately after the `=' is ignored. 

Variables defined with `=' are recursively expanded variables. Variables defined with `:=' are simply expanded variables; these definitions can contain variable references which will be expanded before the definition is made. See The Two Flavors of Variables. 

The variable name may contain function and variable references, which are expanded when the line is read to find the actual variable name to use. 

There is no limit on the length of the value of a variable except the amount of swapping space on the computer. When a variable definition is long, it is a good idea to break it into several lines by inserting backslash-newline at convenient places in the definition. This will not affect the functioning of make, but it will make the makefile easier to read. 

Most variable names are considered to have the empty string as a value if you have never set them. Several variables have built-in initial values that are not empty, but you can set them in the usual ways (see Variables Used by Implicit Rules). Several special variables are set automatically to a new value for each rule; these are called the automatic variables (see Automatic Variables). 

If you'd like a variable to be set to a value only if it's not already set, then you can use the shorthand operator `?=' instead of `='. These two settings of the variable `FOO' are identical (see The origin Function): 

     FOO ?= bar

and 

     ifeq ($(origin FOO), undefined)
     FOO = bar
     endif



--------------------------------------------------------------------------------
Next: Override Directive, Previous: Setting, Up: Using Variables 
6.6 Appending More Text to Variables
Often it is useful to add more text to the value of a variable already defined. You do this with a line containing `+=', like this: 

     objects += another.o

This takes the value of the variable objects, and adds the text `another.o' to it (preceded by a single space). Thus: 

     objects = main.o foo.o bar.o utils.o
     objects += another.o

sets objects to `main.o foo.o bar.o utils.o another.o'. 

Using `+=' is similar to: 

     objects = main.o foo.o bar.o utils.o
     objects := $(objects) another.o

but differs in ways that become important when you use more complex values. 

When the variable in question has not been defined before, `+=' acts just like normal `=': it defines a recursively-expanded variable. However, when there is a previous definition, exactly what `+=' does depends on what flavor of variable you defined originally. See The Two Flavors of Variables, for an explanation of the two flavors of variables. 

When you add to a variable's value with `+=', make acts essentially as if you had included the extra text in the initial definition of the variable. If you defined it first with `:=', making it a simply-expanded variable, `+=' adds to that simply-expanded definition, and expands the new text before appending it to the old value just as `:=' does (see Setting Variables, for a full explanation of `:='). In fact, 

     variable := value
     variable += more

is exactly equivalent to: 


     variable := value
     variable := $(variable) more

On the other hand, when you use `+=' with a variable that you defined first to be recursively-expanded using plain `=', make does something a bit different. Recall that when you define a recursively-expanded variable, make does not expand the value you set for variable and function references immediately. Instead it stores the text verbatim, and saves these variable and function references to be expanded later, when you refer to the new variable (see The Two Flavors of Variables). When you use `+=' on a recursively-expanded variable, it is this unexpanded text to which make appends the new text you specify. 

     variable = value
     variable += more

is roughly equivalent to: 

     temp = value
     variable = $(temp) more

except that of course it never defines a variable called temp. The importance of this comes when the variable's old value contains variable references. Take this common example: 

     CFLAGS = $(includes) -O
     ...
     CFLAGS += -pg # enable profiling

The first line defines the CFLAGS variable with a reference to another variable, includes. (CFLAGS is used by the rules for C compilation; see Catalogue of Implicit Rules.) Using `=' for the definition makes CFLAGS a recursively-expanded variable, meaning `$(includes) -O' is not expanded when make processes the definition of CFLAGS. Thus, includes need not be defined yet for its value to take effect. It only has to be defined before any reference to CFLAGS. If we tried to append to the value of CFLAGS without using `+=', we might do it like this: 

     CFLAGS := $(CFLAGS) -pg # enable profiling

This is pretty close, but not quite what we want. Using `:=' redefines CFLAGS as a simply-expanded variable; this means make expands the text `$(CFLAGS) -pg' before setting the variable. If includes is not yet defined, we get ` -O -pg', and a later definition of includes will have no effect. Conversely, by using `+=' we set CFLAGS to the unexpanded value `$(includes) -O -pg'. Thus we preserve the reference to includes, so if that variable gets defined at any later point, a reference like `$(CFLAGS)' still uses its value. 



--------------------------------------------------------------------------------
Next: Defining, Previous: Appending, Up: Using Variables 
6.7 The override Directive
If a variable has been set with a command argument (see Overriding Variables), then ordinary assignments in the makefile are ignored. If you want to set the variable in the makefile even though it was set with a command argument, you can use an override directive, which is a line that looks like this: 

     override variable = value

or 

     override variable := value

To append more text to a variable defined on the command line, use: 

     override variable += more text

See Appending More Text to Variables. 

The override directive was not invented for escalation in the war between makefiles and command arguments. It was invented so you can alter and add to values that the user specifies with command arguments. 

For example, suppose you always want the `-g' switch when you run the C compiler, but you would like to allow the user to specify the other switches with a command argument just as usual. You could use this override directive: 

     override CFLAGS += -g

You can also use override directives with define directives. This is done as you might expect: 

     override define foo
     bar
     endef

See Defining Variables Verbatim. 



--------------------------------------------------------------------------------
Next: Environment, Previous: Override Directive, Up: Using Variables 
6.8 Defining Variables Verbatim
Another way to set the value of a variable is to use the define directive. This directive has an unusual syntax which allows newline characters to be included in the value, which is convenient for defining both canned sequences of commands (see Defining Canned Command Sequences), and also sections of makefile syntax to use with eval (see Eval Function). 

The define directive is followed on the same line by the name of the variable and nothing more. The value to give the variable appears on the following lines. The end of the value is marked by a line containing just the word endef. Aside from this difference in syntax, define works just like `=': it creates a recursively-expanded variable (see The Two Flavors of Variables). The variable name may contain function and variable references, which are expanded when the directive is read to find the actual variable name to use. 

You may nest define directives: make will keep track of nested directives and report an error if they are not all properly closed with endef. Note that lines beginning with tab characters are considered part of a command script, so any define or endef strings appearing on such a line will not be considered make operators. 

     define two-lines
     echo foo
     echo $(bar)
     endef

The value in an ordinary assignment cannot contain a newline; but the newlines that separate the lines of the value in a define become part of the variable's value (except for the final newline which precedes the endef and is not considered part of the value). 

When used in a command script, the previous example is functionally equivalent to this: 

     two-lines = echo foo; echo $(bar)

since two commands separated by semicolon behave much like two separate shell commands. However, note that using two separate lines means make will invoke the shell twice, running an independent subshell for each line. See Command Execution. 

If you want variable definitions made with define to take precedence over command-line variable definitions, you can use the override directive together with define: 

     override define two-lines
     foo
     $(bar)
     endef

See The override Directive. 



--------------------------------------------------------------------------------
Next: Target-specific, Previous: Defining, Up: Using Variables 
6.9 Variables from the Environment
Variables in make can come from the environment in which make is run. Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment. (If the `-e' flag is specified, then values from the environment override assignments in the makefile. See Summary of Options. But this is not recommended practice.) 

Thus, by setting the variable CFLAGS in your environment, you can cause all C compilations in most makefiles to use the compiler switches you prefer. This is safe for variables with standard or conventional meanings because you know that no makefile will use them for other things. (Note this is not totally reliable; some makefiles set CFLAGS explicitly and therefore are not affected by the value in the environment.) 

When make runs a command script, variables defined in the makefile are placed into the environment of that command. This allows you to pass values to sub-make invocations (see Recursive Use of make). By default, only variables that came from the environment or the command line are passed to recursive invocations. You can use the export directive to pass other variables. See Communicating Variables to a Sub-make, for full details. 

Other use of variables from the environment is not recommended. It is not wise for makefiles to depend for their functioning on environment variables set up outside their control, since this would cause different users to get different results from the same makefile. This is against the whole purpose of most makefiles. 

Such problems would be especially likely with the variable SHELL, which is normally present in the environment to specify the user's choice of interactive shell. It would be very undesirable for this choice to affect make; so, make handles the SHELL environment variable in a special way; see Choosing the Shell. 

6.10 Target-specific Variable Values
Variable values in make are usually global; that is, they are the same regardless of where they are evaluated (unless they're reset, of course). One exception to that is automatic variables (see Automatic Variables). 

The other exception is target-specific variable values. This feature allows you to define different values for the same variable, based on the target that make is currently building. As with automatic variables, these values are only available within the context of a target's command script (and in other target-specific assignments). 

Set a target-specific variable value like this: 

     target ... : variable-assignment

or like this: 

     target ... : override variable-assignment

or like this: 

     target ... : export variable-assignment

Multiple target values create a target-specific variable value for each member of the target list individually. 

The variable-assignment can be any valid form of assignment; recursive (`='), static (`:='), appending (`+='), or conditional (`?='). All variables that appear within the variable-assignment are evaluated within the context of the target: thus, any previously-defined target-specific variable values will be in effect. Note that this variable is actually distinct from any  global  value: the two variables do not have to have the same flavor (recursive vs. static). 

Target-specific variables have the same priority as any other makefile variable. Variables provided on the command-line (and in the environment if the `-e' option is in force) will take precedence. Specifying the override directive will allow the target-specific variable value to be preferred. 

There is one more special feature of target-specific variables: when you define a target-specific variable that variable value is also in effect for all prerequisites of this target, and all their prerequisites, etc. (unless those prerequisites override that variable with their own target-specific variable value). So, for example, a statement like this: 

     prog : CFLAGS = -g
     prog : prog.o foo.o bar.o

will set CFLAGS to `-g' in the command script for prog, but it will also set CFLAGS to `-g' in the command scripts that create prog.o, foo.o, and bar.o, and any command scripts which create their prerequisites. 

Be aware that a given prerequisite will only be built once per invocation of make, at most. If the same file is a prerequisite of multiple targets, and each of those targets has a different value for the same target-specific variable, then the first target to be built will cause that prerequisite to be built and the prerequisite will inherit the target-specific value from the first target. It will ignore the target-specific values from any other targets. 



--------------------------------------------------------------------------------
Previous: Target-specific, Up: Using Variables 
6.11 Pattern-specific Variable Values
In addition to target-specific variable values (see Target-specific Variable Values), GNU make supports pattern-specific variable values. In this form, the variable is defined for any target that matches the pattern specified. If a target matches more than one pattern, all the matching pattern-specific variables are interpreted in the order in which they were defined in the makefile, and collected together into one set. Variables defined in this way are searched after any target-specific variables defined explicitly for that target, and before target-specific variables defined for the parent target. 

Set a pattern-specific variable value like this: 

     pattern ... : variable-assignment

or like this: 

     pattern ... : override variable-assignment

where pattern is a %-pattern. As with target-specific variable values, multiple pattern values create a pattern-specific variable value for each pattern individually. The variable-assignment can be any valid form of assignment. Any command-line variable setting will take precedence, unless override is specified. 

For example: 

     %.o : CFLAGS = -O

will assign CFLAGS the value of `-O' for all targets matching the pattern %.o. 



--------------------------------------------------------------------------------
Next: Functions, Previous: Using Variables, Up: Top 
7 Conditional Parts of Makefiles
A conditional causes part of a makefile to be obeyed or ignored depending on the values of variables. Conditionals can compare the value of one variable to another, or the value of a variable to a constant string. Conditionals control what make actually  sees  in the makefile, so they cannot be used to control shell commands at the time of execution. 

Conditional Example: Example of a conditional 
Conditional Syntax: The syntax of conditionals. 
Testing Flags: Conditionals that test flags. 


--------------------------------------------------------------------------------
Next: Conditional Syntax, Previous: Conditionals, Up: Conditionals 
7.1 Example of a Conditional
The following example of a conditional tells make to use one set of libraries if the CC variable is `gcc', and a different set of libraries otherwise. It works by controlling which of two command lines will be used as the command for a rule. The result is that `CC=gcc' as an argument to make changes not only which compiler is used but also which libraries are linked. 

     libs_for_gcc = -lgnu
     normal_libs =
     
     foo: $(objects)
     ifeq ($(CC),gcc)
             $(CC) -o foo $(objects) $(libs_for_gcc)
     else
             $(CC) -o foo $(objects) $(normal_libs)
     endif

This conditional uses three directives: one ifeq, one else and one endif. 

The ifeq directive begins the conditional, and specifies the condition. It contains two arguments, separated by a comma and surrounded by parentheses. Variable substitution is performed on both arguments and then they are compared. The lines of the makefile following the ifeq are obeyed if the two arguments match; otherwise they are ignored. 

The else directive causes the following lines to be obeyed if the previous conditional failed. In the example above, this means that the second alternative linking command is used whenever the first alternative is not used. It is optional to have an else in a conditional. 

The endif directive ends the conditional. Every conditional must end with an endif. Unconditional makefile text follows. 

As this example illustrates, conditionals work at the textual level: the lines of the conditional are treated as part of the makefile, or ignored, according to the condition. This is why the larger syntactic units of the makefile, such as rules, may cross the beginning or the end of the conditional. 

When the variable CC has the value `gcc', the above example has this effect: 

     foo: $(objects)
             $(CC) -o foo $(objects) $(libs_for_gcc)

When the variable CC has any other value, the effect is this: 

     foo: $(objects)
             $(CC) -o foo $(objects) $(normal_libs)

Equivalent results can be obtained in another way by conditionalizing a variable assignment and then using the variable unconditionally: 

     libs_for_gcc = -lgnu
     normal_libs =
     
     ifeq ($(CC),gcc)
       libs=$(libs_for_gcc)
     else
       libs=$(normal_libs)
     endif
     
     foo: $(objects)
             $(CC) -o foo $(objects) $(libs)



--------------------------------------------------------------------------------
Next: Testing Flags, Previous: Conditional Example, Up: Conditionals 
7.2 Syntax of Conditionals
The syntax of a simple conditional with no else is as follows: 

     conditional-directive
     text-if-true
     endif

The text-if-true may be any lines of text, to be considered as part of the makefile if the condition is true. If the condition is false, no text is used instead. 

The syntax of a complex conditional is as follows: 

     conditional-directive
     text-if-true
     else
     text-if-false
     endif

or: 

     conditional-directive
     text-if-one-is-true
     else conditional-directive
     text-if-true
     else
     text-if-false
     endif

There can be as many  else conditional-directive  clauses as necessary. Once a given condition is true, text-if-true is used and no other clause is used; if no condition is true then text-if-false is used. The text-if-true and text-if-false can be any number of lines of text. 

The syntax of the conditional-directive is the same whether the conditional is simple or complex; after an else or not. There are four different directives that test different conditions. Here is a table of them: 

ifeq (arg1, arg2)
ifeq 'arg1' 'arg2'
ifeq "arg1" "arg2"
ifeq "arg1" 'arg2'
ifeq 'arg1' "arg2"
Expand all variable references in arg1 and arg2 and compare them. If they are identical, the text-if-true is effective; otherwise, the text-if-false, if any, is effective. 
Often you want to test if a variable has a non-empty value. When the value results from complex expansions of variables and functions, expansions you would consider empty may actually contain whitespace characters and thus are not seen as empty. However, you can use the strip function (see Text Functions) to avoid interpreting whitespace as a non-empty value. For example: 

          ifeq ($(strip $(foo)),)
          text-if-empty
          endif
     
will evaluate text-if-empty even if the expansion of $(foo) contains whitespace characters. 


ifneq (arg1, arg2)
ifneq 'arg1' 'arg2'
ifneq "arg1" "arg2"
ifneq "arg1" 'arg2'
ifneq 'arg1' "arg2"
Expand all variable references in arg1 and arg2 and compare them. If they are different, the text-if-true is effective; otherwise, the text-if-false, if any, is effective. 

ifdef variable-name
The ifdef form takes the name of a variable as its argument, not a reference to a variable. The value of that variable has a non-empty value, the text-if-true is effective; otherwise, the text-if-false, if any, is effective. Variables that have never been defined have an empty value. The text variable-name is expanded, so it could be a variable or function that expands to the name of a variable. For example: 
          bar = true
          foo = bar
          ifdef $(foo)
          frobozz = yes
          endif
     
The variable reference $(foo) is expanded, yielding bar, which is considered to be the name of a variable. The variable bar is not expanded, but its value is examined to determine if it is non-empty. 

Note that ifdef only tests whether a variable has a value. It does not expand the variable to see if that value is nonempty. Consequently, tests using ifdef return true for all definitions except those like foo =. To test for an empty value, use ifeq ($(foo),). For example, 

          bar =
          foo = $(bar)
          ifdef foo
          frobozz = yes
          else
          frobozz = no
          endif
     
sets `frobozz' to `yes', while: 

          foo =
          ifdef foo
          frobozz = yes
          else
          frobozz = no
          endif
     
sets `frobozz' to `no'. 


ifndef variable-name
If the variable variable-name has an empty value, the text-if-true is effective; otherwise, the text-if-false, if any, is effective. The rules for expansion and testing of variable-name are identical to the ifdef directive. 
Extra spaces are allowed and ignored at the beginning of the conditional directive line, but a tab is not allowed. (If the line begins with a tab, it will be considered a command for a rule.) Aside from this, extra spaces or tabs may be inserted with no effect anywhere except within the directive name or within an argument. A comment starting with `#' may appear at the end of the line. 

The other two directives that play a part in a conditional are else and endif. Each of these directives is written as one word, with no arguments. Extra spaces are allowed and ignored at the beginning of the line, and spaces or tabs at the end. A comment starting with `#' may appear at the end of the line. 

Conditionals affect which lines of the makefile make uses. If the condition is true, make reads the lines of the text-if-true as part of the makefile; if the condition is false, make ignores those lines completely. It follows that syntactic units of the makefile, such as rules, may safely be split across the beginning or the end of the conditional. 

make evaluates conditionals when it reads a makefile. Consequently, you cannot use automatic variables in the tests of conditionals because they are not defined until commands are run (see Automatic Variables). 

To prevent intolerable confusion, it is not permitted to start a conditional in one makefile and end it in another. However, you may write an include directive within a conditional, provided you do not attempt to terminate the conditional inside the included file. 



--------------------------------------------------------------------------------
Previous: Conditional Syntax, Up: Conditionals 
7.3 Conditionals that Test Flags
You can write a conditional that tests make command flags such as `-t' by using the variable MAKEFLAGS together with the findstring function (see Functions for String Substitution and Analysis). This is useful when touch is not enough to make a file appear up to date. 

The findstring function determines whether one string appears as a substring of another. If you want to test for the `-t' flag, use `t' as the first string and the value of MAKEFLAGS as the other. 

For example, here is how to arrange to use `ranlib -t' to finish marking an archive file up to date: 

     archive.a: ...
     ifneq (,$(findstring t,$(MAKEFLAGS)))
             +touch archive.a
             +ranlib -t archive.a
     else
             ranlib archive.a
     endif

The `+' prefix marks those command lines as  recursive  so that they will be executed despite use of the `-t' flag. See Recursive Use of make. 



--------------------------------------------------------------------------------
Next: Running, Previous: Conditionals, Up: Top 
8 Functions for Transforming Text
Functions allow you to do text processing in the makefile to compute the files to operate on or the commands to use. You use a function in a function call, where you give the name of the function and some text (the arguments) for the function to operate on. The result of the function's processing is substituted into the makefile at the point of the call, just as a variable might be substituted. 

Syntax of Functions: How to write a function call. 
Text Functions: General-purpose text manipulation functions. 
File Name Functions: Functions for manipulating file names. 
Conditional Functions: Functions that implement conditions. 
Foreach Function: Repeat some text with controlled variation. 
Call Function: Expand a user-defined function. 
Value Function: Return the un-expanded value of a variable. 
Eval Function: Evaluate the arguments as makefile syntax. 
Origin Function: Find where a variable got its value. 
Flavor Function: Find out the flavor of a variable. 
Shell Function: Substitute the output of a shell command. 
Make Control Functions: Functions that control how make runs. 


--------------------------------------------------------------------------------
Next: Text Functions, Previous: Functions, Up: Functions 
8.1 Function Call Syntax
A function call resembles a variable reference. It looks like this: 

     $(function arguments)

or like this: 

     ${function arguments}

Here function is a function name; one of a short list of names that are part of make. You can also essentially create your own functions by using the call builtin function. 

The arguments are the arguments of the function. They are separated from the function name by one or more spaces or tabs, and if there is more than one argument, then they are separated by commas. Such whitespace and commas are not part of an argument's value. The delimiters which you use to surround the function call, whether parentheses or braces, can appear in an argument only in matching pairs; the other kind of delimiters may appear singly. If the arguments themselves contain other function calls or variable references, it is wisest to use the same kind of delimiters for all the references; write `$(subst a,b,$(x))', not `$(subst a,b,${x})'. This is because it is clearer, and because only one type of delimiter is matched to find the end of the reference. 

The text written for each argument is processed by substitution of variables and function calls to produce the argument value, which is the text on which the function acts. The substitution is done in the order in which the arguments appear. 

Commas and unmatched parentheses or braces cannot appear in the text of an argument as written; leading spaces cannot appear in the text of the first argument as written. These characters can be put into the argument value by variable substitution. First define variables comma and space whose values are isolated comma and space characters, then substitute these variables where such characters are wanted, like this: 

     comma:= ,
     empty:=
     space:= $(empty) $(empty)
     foo:= a b c
     bar:= $(subst $(space),$(comma),$(foo))
     # bar is now `a,b,c'.

Here the subst function replaces each space with a comma, through the value of foo, and substitutes the result. 



--------------------------------------------------------------------------------
Next: File Name Functions, Previous: Syntax of Functions, Up: Functions 
8.2 Functions for String Substitution and Analysis
Here are some functions that operate on strings: 

$(subst from,to,text)
Performs a textual replacement on the text text: each occurrence of from is replaced by to. The result is substituted for the function call. For example, 
          $(subst ee,EE,feet on the street)
     
substitutes the string `fEEt on the strEEt'. 


$(patsubst pattern,replacement,text)
Finds whitespace-separated words in text that match pattern and replaces them with replacement. Here pattern may contain a `%' which acts as a wildcard, matching any number of any characters within a word. If replacement also contains a `%', the `%' is replaced by the text that matched the `%' in pattern. Only the first `%' in the pattern and replacement is treated this way; any subsequent `%' is unchanged. 
`%' characters in patsubst function invocations can be quoted with preceding backslashes (`\'). Backslashes that would otherwise quote `%' characters can be quoted with more backslashes. Backslashes that quote `%' characters or other backslashes are removed from the pattern before it is compared file names or has a stem substituted into it. Backslashes that are not in danger of quoting `%' characters go unmolested. For example, the pattern the\%weird\\%pattern\\ has `the%weird\' preceding the operative `%' character, and `pattern\\' following it. The final two backslashes are left alone because they cannot affect any `%' character. 

Whitespace between words is folded into single space characters; leading and trailing whitespace is discarded. 

For example, 

          $(patsubst %.c,%.o,x.c.c bar.c)
     
produces the value `x.c.o bar.o'. 

Substitution references (see Substitution References) are a simpler way to get the effect of the patsubst function: 

          $(var:pattern=replacement)
     
is equivalent to 

          $(patsubst pattern,replacement,$(var))
     
The second shorthand simplifies one of the most common uses of patsubst: replacing the suffix at the end of file names. 

          $(var:suffix=replacement)
     
is equivalent to 

          $(patsubst %suffix,%replacement,$(var))
     
For example, you might have a list of object files: 

          objects = foo.o bar.o baz.o
     
To get the list of corresponding source files, you could simply write: 

          $(objects:.o=.c)
     
instead of using the general form: 

          $(patsubst %.o,%.c,$(objects))
     


$(strip string)
Removes leading and trailing whitespace from string and replaces each internal sequence of one or more whitespace characters with a single space. Thus, `$(strip a b c )' results in `a b c'. 
The function strip can be very useful when used in conjunction with conditionals. When comparing something with the empty string `' using ifeq or ifneq, you usually want a string of just whitespace to match the empty string (see Conditionals). 

Thus, the following may fail to have the desired results: 

          .PHONY: all
          ifneq   "$(needs_made)" ""
          all: $(needs_made)
          else
          all:;@echo 'Nothing to make!'
          endif
     
Replacing the variable reference `$(needs_made)' with the function call `$(strip $(needs_made))' in the ifneq directive would make it more robust. 


$(findstring find,in)
Searches in for an occurrence of find. If it occurs, the value is find; otherwise, the value is empty. You can use this function in a conditional to test for the presence of a specific substring in a given string. Thus, the two examples, 
          $(findstring a,a b c)
          $(findstring a,b c)
     
produce the values `a' and `' (the empty string), respectively. See Testing Flags, for a practical application of findstring. 




$(filter pattern...,text)
Returns all whitespace-separated words in text that do match any of the pattern words, removing any words that do not match. The patterns are written using `%', just like the patterns used in the patsubst function above. 
The filter function can be used to separate out different types of strings (such as file names) in a variable. For example: 

          sources := foo.c bar.c baz.s ugh.h
          foo: $(sources)
                  cc $(filter %.c %.s,$(sources)) -o foo
     
says that foo depends of foo.c, bar.c, baz.s and ugh.h but only foo.c, bar.c and baz.s should be specified in the command to the compiler. 


$(filter-out pattern...,text)
Returns all whitespace-separated words in text that do not match any of the pattern words, removing the words that do match one or more. This is the exact opposite of the filter function. 
For example, given: 

          objects=main1.o foo.o main2.o bar.o
          mains=main1.o main2.o
     
the following generates a list which contains all the object files not in `mains': 

          $(filter-out $(mains),$(objects))
     



$(sort list)
Sorts the words of list in lexical order, removing duplicate words. The output is a list of words separated by single spaces. Thus, 
          $(sort foo bar lose)
     
returns the value `bar foo lose'. 

Incidentally, since sort removes duplicate words, you can use it for this purpose even if you don't care about the sort order. 


$(word n,text)
Returns the nth word of text. The legitimate values of n start from 1. If n is bigger than the number of words in text, the value is empty. For example, 
          $(word 2, foo bar baz)
     
returns `bar'. 


$(wordlist s,e,text)
Returns the list of words in text starting with word s and ending with word e (inclusive). The legitimate values of s start from 1; e may start from 0. If s is bigger than the number of words in text, the value is empty. If e is bigger than the number of words in text, words up to the end of text are returned. If s is greater than e, nothing is returned. For example, 
          $(wordlist 2, 3, foo bar baz)
     
returns `bar baz'. 


$(words text)
Returns the number of words in text. Thus, the last word of text is $(word $(words text),text). 

$(firstword names...)
The argument names is regarded as a series of names, separated by whitespace. The value is the first name in the series. The rest of the names are ignored. 
For example, 

          $(firstword foo bar)
     
produces the result `foo'. Although $(firstword text) is the same as $(word 1,text), the firstword function is retained for its simplicity. 


$(lastword names...)
The argument names is regarded as a series of names, separated by whitespace. The value is the last name in the series. 
For example, 

          $(lastword foo bar)
     
produces the result `bar'. Although $(lastword text) is the same as $(word $(words text),text), the lastword function was added for its simplicity and better performance. 

Here is a realistic example of the use of subst and patsubst. Suppose that a makefile uses the VPATH variable to specify a list of directories that make should search for prerequisite files (see VPATH Search Path for All Prerequisites). This example shows how to tell the C compiler to search for header files in the same list of directories. 

The value of VPATH is a list of directories separated by colons, such as `src:../headers'. First, the subst function is used to change the colons to spaces: 

     $(subst :, ,$(VPATH))

This produces `src ../headers'. Then patsubst is used to turn each directory name into a `-I' flag. These can be added to the value of the variable CFLAGS, which is passed automatically to the C compiler, like this: 

     override CFLAGS += $(patsubst %,-I%,$(subst :, ,$(VPATH)))

The effect is to append the text `-Isrc -I../headers' to the previously given value of CFLAGS. The override directive is used so that the new value is assigned even if the previous value of CFLAGS was specified with a command argument (see The override Directive). 



--------------------------------------------------------------------------------
Next: Conditional Functions, Previous: Text Functions, Up: Functions 
8.3 Functions for File Names
Several of the built-in expansion functions relate specifically to taking apart file names or lists of file names. 

Each of the following functions performs a specific transformation on a file name. The argument of the function is regarded as a series of file names, separated by whitespace. (Leading and trailing whitespace is ignored.) Each file name in the series is transformed in the same way and the results are concatenated with single spaces between them. 

$(dir names...)
Extracts the directory-part of each file name in names. The directory-part of the file name is everything up through (and including) the last slash in it. If the file name contains no slash, the directory part is the string `./'. For example, 
          $(dir src/foo.c hacks)
     
produces the result `src/ ./'. 


$(notdir names...)
Extracts all but the directory-part of each file name in names. If the file name contains no slash, it is left unchanged. Otherwise, everything through the last slash is removed from it. 
A file name that ends with a slash becomes an empty string. This is unfortunate, because it means that the result does not always have the same number of whitespace-separated file names as the argument had; but we do not see any other valid alternative. 

For example, 

          $(notdir src/foo.c hacks)
     
produces the result `foo.c hacks'. 


$(suffix names...)
Extracts the suffix of each file name in names. If the file name contains a period, the suffix is everything starting with the last period. Otherwise, the suffix is the empty string. This frequently means that the result will be empty when names is not, and if names contains multiple file names, the result may contain fewer file names. 
For example, 

          $(suffix src/foo.c src-1.0/bar.c hacks)
     
produces the result `.c .c'. 


$(basename names...)
Extracts all but the suffix of each file name in names. If the file name contains a period, the basename is everything starting up to (and not including) the last period. Periods in the directory part are ignored. If there is no period, the basename is the entire file name. For example, 
          $(basename src/foo.c src-1.0/bar hacks)
     
produces the result `src/foo src-1.0/bar hacks'. 


$(addsuffix suffix,names...)
The argument names is regarded as a series of names, separated by whitespace; suffix is used as a unit. The value of suffix is appended to the end of each individual name and the resulting larger names are concatenated with single spaces between them. For example, 
          $(addsuffix .c,foo bar)
     
produces the result `foo.c bar.c'. 


$(addprefix prefix,names...)
The argument names is regarded as a series of names, separated by whitespace; prefix is used as a unit. The value of prefix is prepended to the front of each individual name and the resulting larger names are concatenated with single spaces between them. For example, 
          $(addprefix src/,foo bar)
     
produces the result `src/foo src/bar'. 


$(join list1,list2)
Concatenates the two arguments word by word: the two first words (one from each argument) concatenated form the first word of the result, the two second words form the second word of the result, and so on. So the nth word of the result comes from the nth word of each argument. If one argument has more words that the other, the extra words are copied unchanged into the result. 
For example, `$(join a b,.c .o)' produces `a.c b.o'. 

Whitespace between the words in the lists is not preserved; it is replaced with a single space. 

This function can merge the results of the dir and notdir functions, to produce the original list of files which was given to those two functions. 


$(wildcard pattern)
The argument pattern is a file name pattern, typically containing wildcard characters (as in shell file name patterns). The result of wildcard is a space-separated list of the names of existing files that match the pattern. See Using Wildcard Characters in File Names. 

$(realpath names...)
For each file name in names return the canonical absolute name. A canonical name does not contain any . or .. components, nor any repeated path separators (/) or symlinks. In case of a failure the empty string is returned. Consult the realpath(3) documentation for a list of possible failure causes. 

$(abspath names...)
For each file name in names return an absolute name that does not contain any . or .. components, nor any repeated path separators (/). Note that, in contrast to realpath function, abspath does not resolve symlinks and does not require the file names to refer to an existing file or directory. Use the wildcard function to test for existence. 


--------------------------------------------------------------------------------
Next: Foreach Function, Previous: File Name Functions, Up: Functions 
8.4 Functions for Conditionals
There are three functions that provide conditional expansion. A key aspect of these functions is that not all of the arguments are expanded initially. Only those arguments which need to be expanded, will be expanded. 

$(if condition,then-part[,else-part])
The if function provides support for conditional expansion in a functional context (as opposed to the GNU make makefile conditionals such as ifeq (see Syntax of Conditionals). 
The first argument, condition, first has all preceding and trailing whitespace stripped, then is expanded. If it expands to any non-empty string, then the condition is considered to be true. If it expands to an empty string, the condition is considered to be false. 

If the condition is true then the second argument, then-part, is evaluated and this is used as the result of the evaluation of the entire if function. 

If the condition is false then the third argument, else-part, is evaluated and this is the result of the if function. If there is no third argument, the if function evaluates to nothing (the empty string). 

Note that only one of the then-part or the else-part will be evaluated, never both. Thus, either can contain side-effects (such as shell function calls, etc.) 


$(or condition1[,condition2[,condition3...]])
The or function provides a  short-circuiting  OR operation. Each argument is expanded, in order. If an argument expands to a non-empty string the processing stops and the result of the expansion is that string. If, after all arguments are expanded, all of them are false (empty), then the result of the expansion is the empty string. 

$(and condition1[,condition2[,condition3...]])
The and function provides a  short-circuiting  AND operation. Each argument is expanded, in order. If an argument expands to an empty string the processing stops and the result of the expansion is the empty string. If all arguments expand to a non-empty string then the result of the expansion is the expansion of the last argument. 


--------------------------------------------------------------------------------
Next: Call Function, Previous: Conditional Functions, Up: Functions 
8.5 The foreach Function
The foreach function is very different from other functions. It causes one piece of text to be used repeatedly, each time with a different substitution performed on it. It resembles the for command in the shell sh and the foreach command in the C-shell csh. 

The syntax of the foreach function is: 

     $(foreach var,list,text)

The first two arguments, var and list, are expanded before anything else is done; note that the last argument, text, is not expanded at the same time. Then for each word of the expanded value of list, the variable named by the expanded value of var is set to that word, and text is expanded. Presumably text contains references to that variable, so its expansion will be different each time. 

The result is that text is expanded as many times as there are whitespace-separated words in list. The multiple expansions of text are concatenated, with spaces between them, to make the result of foreach. 

This simple example sets the variable `files' to the list of all files in the directories in the list `dirs': 

     dirs := a b c d
     files := $(foreach dir,$(dirs),$(wildcard $(dir)/*))

Here text is `$(wildcard $(dir)/*)'. The first repetition finds the value `a' for dir, so it produces the same result as `$(wildcard a/*)'; the second repetition produces the result of `$(wildcard b/*)'; and the third, that of `$(wildcard c/*)'. 

This example has the same result (except for setting `dirs') as the following example: 

     files := $(wildcard a/* b/* c/* d/*)

When text is complicated, you can improve readability by giving it a name, with an additional variable: 

     find_files = $(wildcard $(dir)/*)
     dirs := a b c d
     files := $(foreach dir,$(dirs),$(find_files))

Here we use the variable find_files this way. We use plain `=' to define a recursively-expanding variable, so that its value contains an actual function call to be reexpanded under the control of foreach; a simply-expanded variable would not do, since wildcard would be called only once at the time of defining find_files. 

The foreach function has no permanent effect on the variable var; its value and flavor after the foreach function call are the same as they were beforehand. The other values which are taken from list are in effect only temporarily, during the execution of foreach. The variable var is a simply-expanded variable during the execution of foreach. If var was undefined before the foreach function call, it is undefined after the call. See The Two Flavors of Variables. 

You must take care when using complex variable expressions that result in variable names because many strange things are valid variable names, but are probably not what you intended. For example, 

     files := $(foreach Esta escrito en espanol!,b c ch,$(find_files))

might be useful if the value of find_files references the variable whose name is `Esta escrito en espanol!' (es un nombre bastante largo, no?), but it is more likely to be a mistake. 



--------------------------------------------------------------------------------
Next: Value Function, Previous: Foreach Function, Up: Functions 
8.6 The call Function
The call function is unique in that it can be used to create new parameterized functions. You can write a complex expression as the value of a variable, then use call to expand it with different values. 

The syntax of the call function is: 

     $(call variable,param,param,...)

When make expands this function, it assigns each param to temporary variables $(1), $(2), etc. The variable $(0) will contain variable. There is no maximum number of parameter arguments. There is no minimum, either, but it doesn't make sense to use call with no parameters. 

Then variable is expanded as a make variable in the context of these temporary assignments. Thus, any reference to $(1) in the value of variable will resolve to the first param in the invocation of call. 

Note that variable is the name of a variable, not a reference to that variable. Therefore you would not normally use a `$' or parentheses when writing it. (You can, however, use a variable reference in the name if you want the name not to be a constant.) 

If variable is the name of a builtin function, the builtin function is always invoked (even if a make variable by that name also exists). 

The call function expands the param arguments before assigning them to temporary variables. This means that variable values containing references to builtin functions that have special expansion rules, like foreach or if, may not work as you expect. 

Some examples may make this clearer. 

This macro simply reverses its arguments: 

     reverse = $(2) $(1)
     
     foo = $(call reverse,a,b)

Here foo will contain `b a'. 

This one is slightly more interesting: it defines a macro to search for the first instance of a program in PATH: 

     pathsearch = $(firstword $(wildcard $(addsuffix /$(1),$(subst :, ,$(PATH)))))
     
     LS := $(call pathsearch,ls)

Now the variable LS contains /bin/ls or similar. 

The call function can be nested. Each recursive invocation gets its own local values for $(1), etc. that mask the values of higher-level call. For example, here is an implementation of a map function: 

     map = $(foreach a,$(2),$(call $(1),$(a)))

Now you can map a function that normally takes only one argument, such as origin, to multiple values in one step: 

     o = $(call map,origin,o map MAKE)

and end up with o containing something like `file file default'. 

A final caution: be careful when adding whitespace to the arguments to call. As with other functions, any whitespace contained in the second and subsequent arguments is kept; this can cause strange effects. It's generally safest to remove all extraneous whitespace when providing parameters to call. 



--------------------------------------------------------------------------------
Next: Eval Function, Previous: Call Function, Up: Functions 
8.7 The value Function
The value function provides a way for you to use the value of a variable without having it expanded. Please note that this does not undo expansions which have already occurred; for example if you create a simply expanded variable its value is expanded during the definition; in that case the value function will return the same result as using the variable directly. 

The syntax of the value function is: 

     $(value variable)

Note that variable is the name of a variable; not a reference to that variable. Therefore you would not normally use a `$' or parentheses when writing it. (You can, however, use a variable reference in the name if you want the name not to be a constant.) 

The result of this function is a string containing the value of variable, without any expansion occurring. For example, in this makefile: 

     FOO = $PATH
     
     all:
             @echo $(FOO)
             @echo $(value FOO)

The first output line would be ATH, since the  $P  would be expanded as a make variable, while the second output line would be the current value of your $PATH environment variable, since the value function avoided the expansion. 

The value function is most often used in conjunction with the eval function (see Eval Function). 



--------------------------------------------------------------------------------
Next: Origin Function, Previous: Value Function, Up: Functions 
8.8 The eval Function
The eval function is very special: it allows you to define new makefile constructs that are not constant; which are the result of evaluating other variables and functions. The argument to the eval function is expanded, then the results of that expansion are parsed as makefile syntax. The expanded results can define new make variables, targets, implicit or explicit rules, etc. 

The result of the eval function is always the empty string; thus, it can be placed virtually anywhere in a makefile without causing syntax errors. 

It's important to realize that the eval argument is expanded twice; first by the eval function, then the results of that expansion are expanded again when they are parsed as makefile syntax. This means you may need to provide extra levels of escaping for  $  characters when using eval. The value function (see Value Function) can sometimes be useful in these situations, to circumvent unwanted expansions. 

Here is an example of how eval can be used; this example combines a number of concepts and other functions. Although it might seem overly complex to use eval in this example, rather than just writing out the rules, consider two things: first, the template definition (in PROGRAM_template) could need to be much more complex than it is here; and second, you might put the complex,  generic  part of this example into another makefile, then include it in all the individual makefiles. Now your individual makefiles are quite straightforward. 

     PROGRAMS    = server client
     
     server_OBJS = server.o server_priv.o server_access.o
     server_LIBS = priv protocol
     
     client_OBJS = client.o client_api.o client_mem.o
     client_LIBS = protocol
     
     # Everything after this is generic
     
     .PHONY: all
     all: $(PROGRAMS)
     
     define PROGRAM_template
      $(1): $$($(1)_OBJS) $$($(1)_LIBS:%=-l%)
      ALL_OBJS   += $$($(1)_OBJS)
     endef
     
     $(foreach prog,$(PROGRAMS),$(eval $(call PROGRAM_template,$(prog))))
     
     $(PROGRAMS):
             $(LINK.o) $^ $(LDLIBS) -o $@
     
     clean:
             rm -f $(ALL_OBJS) $(PROGRAMS)



--------------------------------------------------------------------------------
Next: Flavor Function, Previous: Eval Function, Up: Functions 
8.9 The origin Function
The origin function is unlike most other functions in that it does not operate on the values of variables; it tells you something about a variable. Specifically, it tells you where it came from. 

The syntax of the origin function is: 

     $(origin variable)

Note that variable is the name of a variable to inquire about; not a reference to that variable. Therefore you would not normally use a `$' or parentheses when writing it. (You can, however, use a variable reference in the name if you want the name not to be a constant.) 

The result of this function is a string telling you how the variable variable was defined: 

`undefined'
if variable was never defined. 

`default'
if variable has a default definition, as is usual with CC and so on. See Variables Used by Implicit Rules. Note that if you have redefined a default variable, the origin function will return the origin of the later definition. 

`environment'
if variable was defined as an environment variable and the `-e' option is not turned on (see Summary of Options). 

`environment override'
if variable was defined as an environment variable and the `-e' option is turned on (see Summary of Options). 

`file'
if variable was defined in a makefile. 

`command line'
if variable was defined on the command line. 

`override'
if variable was defined with an override directive in a makefile (see The override Directive). 

`automatic'
if variable is an automatic variable defined for the execution of the commands for each rule (see Automatic Variables). 
This information is primarily useful (other than for your curiosity) to determine if you want to believe the value of a variable. For example, suppose you have a makefile foo that includes another makefile bar. You want a variable bletch to be defined in bar if you run the command `make -f bar', even if the environment contains a definition of bletch. However, if foo defined bletch before including bar, you do not want to override that definition. This could be done by using an override directive in foo, giving that definition precedence over the later definition in bar; unfortunately, the override directive would also override any command line definitions. So, bar could include: 

     ifdef bletch
     ifeq "$(origin bletch)" "environment"
     bletch = barf, gag, etc.
     endif
     endif

If bletch has been defined from the environment, this will redefine it. 

If you want to override a previous definition of bletch if it came from the environment, even under `-e', you could instead write: 

     ifneq "$(findstring environment,$(origin bletch))" ""
     bletch = barf, gag, etc.
     endif

Here the redefinition takes place if `$(origin bletch)' returns either `environment' or `environment override'. See Functions for String Substitution and Analysis. 



--------------------------------------------------------------------------------
Next: Shell Function, Previous: Origin Function, Up: Functions 
8.10 The flavor Function
The flavor function is unlike most other functions (and like origin function) in that it does not operate on the values of variables; it tells you something about a variable. Specifically, it tells you the flavor of a variable (see The Two Flavors of Variables). 

The syntax of the flavor function is: 

     $(flavor variable)

Note that variable is the name of a variable to inquire about; not a reference to that variable. Therefore you would not normally use a `$' or parentheses when writing it. (You can, however, use a variable reference in the name if you want the name not to be a constant.) 

The result of this function is a string that identifies the flavor of the variable variable: 

`undefined'
if variable was never defined. 

`recursive'
if variable is a recursively expanded variable. 

`simple'
if variable is a simply expanded variable. 


--------------------------------------------------------------------------------
Next: Make Control Functions, Previous: Flavor Function, Up: Functions 
8.11 The shell Function
The shell function is unlike any other function other than the wildcard function (see The Function wildcard) in that it communicates with the world outside of make. 

The shell function performs the same function that backquotes (``') perform in most shells: it does command expansion. This means that it takes as an argument a shell command and evaluates to the output of the command. The only processing make does on the result is to convert each newline (or carriage-return / newline pair) to a single space. If there is a trailing (carriage-return and) newline it will simply be removed. 

The commands run by calls to the shell function are run when the function calls are expanded (see How make Reads a Makefile). Because this function involves spawning a new shell, you should carefully consider the performance implications of using the shell function within recursively expanded variables vs. simply expanded variables (see The Two Flavors of Variables). 

Here are some examples of the use of the shell function: 

     contents := $(shell cat foo)

sets contents to the contents of the file foo, with a space (rather than a newline) separating each line. 

     files := $(shell echo *.c)

sets files to the expansion of `*.c'. Unless make is using a very strange shell, this has the same result as `$(wildcard *.c)' (as long as at least one `.c' file exists). 



--------------------------------------------------------------------------------
Previous: Shell Function, Up: Functions 
8.12 Functions That Control Make
These functions control the way make runs. Generally, they are used to provide information to the user of the makefile or to cause make to stop if some sort of environmental error is detected. 

$(error text...)
Generates a fatal error where the message is text. Note that the error is generated whenever this function is evaluated. So, if you put it inside a command script or on the right side of a recursive variable assignment, it won't be evaluated until later. The text will be expanded before the error is generated. 
For example, 

          ifdef ERROR1
          $(error error is $(ERROR1))
          endif
     
will generate a fatal error during the read of the makefile if the make variable ERROR1 is defined. Or, 

          ERR = $(error found an error!)
          
          .PHONY: err
          err: ; $(ERR)
     
will generate a fatal error while make is running, if the err target is invoked. 


$(warning text...)
This function works similarly to the error function, above, except that make doesn't exit. Instead, text is expanded and the resulting message is displayed, but processing of the makefile continues. 
The result of the expansion of this function is the empty string. 


$(info text...)
This function does nothing more than print its (expanded) argument(s) to standard output. No makefile name or line number is added. The result of the expansion of this function is the empty string. 


--------------------------------------------------------------------------------
Next: Implicit Rules, Previous: Functions, Up: Top 
9 How to Run make
A makefile that says how to recompile a program can be used in more than one way. The simplest use is to recompile every file that is out of date. Usually, makefiles are written so that if you run make with no arguments, it does just that. 

But you might want to update only some of the files; you might want to use a different compiler or different compiler options; you might want just to find out which files are out of date without changing them. 

By giving arguments when you run make, you can do any of these things and many others. 

The exit status of make is always one of three values: 

0
The exit status is zero if make is successful. 

2
The exit status is two if make encounters any errors. It will print messages describing the particular errors. 

1
The exit status is one if you use the `-q' flag and make determines that some target is not already up to date. See Instead of Executing the Commands. 
Makefile Arguments: How to specify which makefile to use. 
Goals: How to use goal arguments to specify which parts of the makefile to use. 
Instead of Execution: How to use mode flags to specify what kind of thing to do with the commands in the makefile other than simply execute them. 
Avoiding Compilation: How to avoid recompiling certain files. 
Overriding: How to override a variable to specify an alternate compiler and other things. 
Testing: How to proceed past some errors, to test compilation. 
Options Summary: Summary of Options 


--------------------------------------------------------------------------------
Next: Goals, Previous: Running, Up: Running 
9.1 Arguments to Specify the Makefile
The way to specify the name of the makefile is with the `-f' or `--file' option (`--makefile' also works). For example, `-f altmake' says to use the file altmake as the makefile. 

If you use the `-f' flag several times and follow each `-f' with an argument, all the specified files are used jointly as makefiles. 

If you do not use the `-f' or `--file' flag, the default is to try GNUmakefile, makefile, and Makefile, in that order, and use the first of these three which exists or can be made (see Writing Makefiles). 



--------------------------------------------------------------------------------
Next: Instead of Execution, Previous: Makefile Arguments, Up: Running 
9.2 Arguments to Specify the Goals
The goals are the targets that make should strive ultimately to update. Other targets are updated as well if they appear as prerequisites of goals, or prerequisites of prerequisites of goals, etc. 

By default, the goal is the first target in the makefile (not counting targets that start with a period). Therefore, makefiles are usually written so that the first target is for compiling the entire program or programs they describe. If the first rule in the makefile has several targets, only the first target in the rule becomes the default goal, not the whole list. You can manage the selection of the default goal from within your makefile using the .DEFAULT_GOAL variable (see Other Special Variables). 

You can also specify a different goal or goals with command-line arguments to make. Use the name of the goal as an argument. If you specify several goals, make processes each of them in turn, in the order you name them. 

Any target in the makefile may be specified as a goal (unless it starts with `-' or contains an `=', in which case it will be parsed as a switch or variable definition, respectively). Even targets not in the makefile may be specified, if make can find implicit rules that say how to make them. 

Make will set the special variable MAKECMDGOALS to the list of goals you specified on the command line. If no goals were given on the command line, this variable is empty. Note that this variable should be used only in special circumstances. 

An example of appropriate use is to avoid including .d files during clean rules (see Automatic Prerequisites), so make won't create them only to immediately remove them again: 

     sources = foo.c bar.c
     
     ifneq ($(MAKECMDGOALS),clean)
     include $(sources:.c=.d)
     endif

One use of specifying a goal is if you want to compile only a part of the program, or only one of several programs. Specify as a goal each file that you wish to remake. For example, consider a directory containing several programs, with a makefile that starts like this: 

     .PHONY: all
     all: size nm ld ar as

If you are working on the program size, you might want to say `make size' so that only the files of that program are recompiled. 

Another use of specifying a goal is to make files that are not normally made. For example, there may be a file of debugging output, or a version of the program that is compiled specially for testing, which has a rule in the makefile but is not a prerequisite of the default goal. 

Another use of specifying a goal is to run the commands associated with a phony target (see Phony Targets) or empty target (see Empty Target Files to Record Events). Many makefiles contain a phony target named clean which deletes everything except source files. Naturally, this is done only if you request it explicitly with `make clean'. Following is a list of typical phony and empty target names. See Standard Targets, for a detailed list of all the standard target names which GNU software packages use. 

all
Make all the top-level targets the makefile knows about. 

clean
Delete all files that are normally created by running make. 

mostlyclean
Like `clean', but may refrain from deleting a few files that people normally don't want to recompile. For example, the `mostlyclean' target for GCC does not delete libgcc.a, because recompiling it is rarely necessary and takes a lot of time. 

distclean
realclean
clobber
Any of these targets might be defined to delete more files than `clean' does. For example, this would delete configuration files or links that you would normally create as preparation for compilation, even if the makefile itself cannot create these files. 

install
Copy the executable file into a directory that users typically search for commands; copy any auxiliary files that the executable uses into the directories where it will look for them. 

print
Print listings of the source files that have changed. 

tar
Create a tar file of the source files. 

shar
Create a shell archive (shar file) of the source files. 

dist
Create a distribution file of the source files. This might be a tar file, or a shar file, or a compressed version of one of the above, or even more than one of the above. 

TAGS
Update a tags table for this program. 

check
test
Perform self tests on the program this makefile builds. 


--------------------------------------------------------------------------------
Next: Avoiding Compilation, Previous: Goals, Up: Running 
9.3 Instead of Executing the Commands
The makefile tells make how to tell whether a target is up to date, and how to update each target. But updating the targets is not always what you want. Certain options specify other activities for make. 

`-n'
`--just-print'
`--dry-run'
`--recon'
 No-op . The activity is to print what commands would be used to make the targets up to date, but not actually execute them. 

`-t'
`--touch'
 Touch . The activity is to mark the targets as up to date without actually changing them. In other words, make pretends to compile the targets but does not really change their contents. 

`-q'
`--question'
 Question . The activity is to find out silently whether the targets are up to date already; but execute no commands in either case. In other words, neither compilation nor output will occur. 

`-W file'
`--what-if=file'
`--assume-new=file'
`--new-file=file'
 What if . Each `-W' flag is followed by a file name. The given files' modification times are recorded by make as being the present time, although the actual modification times remain the same. You can use the `-W' flag in conjunction with the `-n' flag to see what would happen if you were to modify specific files. 
With the `-n' flag, make prints the commands that it would normally execute but does not execute them. 

With the `-t' flag, make ignores the commands in the rules and uses (in effect) the command touch for each target that needs to be remade. The touch command is also printed, unless `-s' or .SILENT is used. For speed, make does not actually invoke the program touch. It does the work directly. 

With the `-q' flag, make prints nothing and executes no commands, but the exit status code it returns is zero if and only if the targets to be considered are already up to date. If the exit status is one, then some updating needs to be done. If make encounters an error, the exit status is two, so you can distinguish an error from a target that is not up to date. 

It is an error to use more than one of these three flags in the same invocation of make. 

The `-n', `-t', and `-q' options do not affect command lines that begin with `+' characters or contain the strings `$(MAKE)' or `${MAKE}'. Note that only the line containing the `+' character or the strings `$(MAKE)' or `${MAKE}' is run regardless of these options. Other lines in the same rule are not run unless they too begin with `+' or contain `$(MAKE)' or `${MAKE}' (See How the MAKE Variable Works.) 

The `-W' flag provides two features: 

If you also use the `-n' or `-q' flag, you can see what make would do if you were to modify some files. 
Without the `-n' or `-q' flag, when make is actually executing commands, the `-W' flag can direct make to act as if some files had been modified, without actually modifying the files. 
Note that the options `-p' and `-v' allow you to obtain other information about make or about the makefiles in use (see Summary of Options). 



--------------------------------------------------------------------------------
Next: Overriding, Previous: Instead of Execution, Up: Running 
9.4 Avoiding Recompilation of Some Files
Sometimes you may have changed a source file but you do not want to recompile all the files that depend on it. For example, suppose you add a macro or a declaration to a header file that many other files depend on. Being conservative, make assumes that any change in the header file requires recompilation of all dependent files, but you know that they do not need to be recompiled and you would rather not waste the time waiting for them to compile. 

If you anticipate the problem before changing the header file, you can use the `-t' flag. This flag tells make not to run the commands in the rules, but rather to mark the target up to date by changing its last-modification date. You would follow this procedure: 

Use the command `make' to recompile the source files that really need recompilation, ensuring that the object files are up-to-date before you begin. 
Make the changes in the header files. 
Use the command `make -t' to mark all the object files as up to date. The next time you run make, the changes in the header files will not cause any recompilation. 
If you have already changed the header file at a time when some files do need recompilation, it is too late to do this. Instead, you can use the `-o file' flag, which marks a specified file as  old  (see Summary of Options). This means that the file itself will not be remade, and nothing else will be remade on its account. Follow this procedure: 

Recompile the source files that need compilation for reasons independent of the particular header file, with `make -o headerfile'. If several header files are involved, use a separate `-o' option for each header file. 
Touch all the object files with `make -t'. 


--------------------------------------------------------------------------------
Next: Testing, Previous: Avoiding Compilation, Up: Running 
9.5 Overriding Variables
An argument that contains `=' specifies the value of a variable: `v=x' sets the value of the variable v to x. If you specify a value in this way, all ordinary assignments of the same variable in the makefile are ignored; we say they have been overridden by the command line argument. 

The most common way to use this facility is to pass extra flags to compilers. For example, in a properly written makefile, the variable CFLAGS is included in each command that runs the C compiler, so a file foo.c would be compiled something like this: 

     cc -c $(CFLAGS) foo.c

Thus, whatever value you set for CFLAGS affects each compilation that occurs. The makefile probably specifies the usual value for CFLAGS, like this: 

     CFLAGS=-g

Each time you run make, you can override this value if you wish. For example, if you say `make CFLAGS='-g -O'', each C compilation will be done with `cc -c -g -O'. (This also illustrates how you can use quoting in the shell to enclose spaces and other special characters in the value of a variable when you override it.) 

The variable CFLAGS is only one of many standard variables that exist just so that you can change them this way. See Variables Used by Implicit Rules, for a complete list. 

You can also program the makefile to look at additional variables of your own, giving the user the ability to control other aspects of how the makefile works by changing the variables. 

When you override a variable with a command argument, you can define either a recursively-expanded variable or a simply-expanded variable. The examples shown above make a recursively-expanded variable; to make a simply-expanded variable, write `:=' instead of `='. But, unless you want to include a variable reference or function call in the value that you specify, it makes no difference which kind of variable you create. 

There is one way that the makefile can change a variable that you have overridden. This is to use the override directive, which is a line that looks like this: `override variable = value' (see The override Directive). 



--------------------------------------------------------------------------------
Next: Options Summary, Previous: Overriding, Up: Running 
9.6 Testing the Compilation of a Program
Normally, when an error happens in executing a shell command, make gives up immediately, returning a nonzero status. No further commands are executed for any target. The error implies that the goal cannot be correctly remade, and make reports this as soon as it knows. 

When you are compiling a program that you have just changed, this is not what you want. Instead, you would rather that make try compiling every file that can be tried, to show you as many compilation errors as possible. 

On these occasions, you should use the `-k' or `--keep-going' flag. This tells make to continue to consider the other prerequisites of the pending targets, remaking them if necessary, before it gives up and returns nonzero status. For example, after an error in compiling one object file, `make -k' will continue compiling other object files even though it already knows that linking them will be impossible. In addition to continuing after failed shell commands, `make -k' will continue as much as possible after discovering that it does not know how to make a target or prerequisite file. This will always cause an error message, but without `-k', it is a fatal error (see Summary of Options). 

The usual behavior of make assumes that your purpose is to get the goals up to date; once make learns that this is impossible, it might as well report the failure immediately. The `-k' flag says that the real purpose is to test as much as possible of the changes made in the program, perhaps to find several independent problems so that you can correct them all before the next attempt to compile. This is why Emacs' M-x compile command passes the `-k' flag by default. 



--------------------------------------------------------------------------------
Previous: Testing, Up: Running 
9.7 Summary of Options
Here is a table of all the options make understands: 

`-b'
`-m'
These options are ignored for compatibility with other versions of make. 

`-B'
`--always-make'
Consider all targets out-of-date. GNU make proceeds to consider targets and their prerequisites using the normal algorithms; however, all targets so considered are always remade regardless of the status of their prerequisites. To avoid infinite recursion, if MAKE_RESTARTS (see Other Special Variables) is set to a number greater than 0 this option is disabled when considering whether to remake makefiles (see How Makefiles Are Remade). 

`-C dir'
`--directory=dir'
Change to directory dir before reading the makefiles. If multiple `-C' options are specified, each is interpreted relative to the previous one: `-C / -C etc' is equivalent to `-C /etc'. This is typically used with recursive invocations of make (see Recursive Use of make). 

`-d'
Print debugging information in addition to normal processing. The debugging information says which files are being considered for remaking, which file-times are being compared and with what results, which files actually need to be remade, which implicit rules are considered and which are applied everything interesting about how make decides what to do. The -d option is equivalent to `--debug=a' (see below). 


`--debug[=options]'
Print debugging information in addition to normal processing. Various levels and types of output can be chosen. With no arguments, print the  basic  level of debugging. Possible arguments are below; only the first character is considered, and values must be comma- or space-separated. 

a (all)
All types of debugging output are enabled. This is equivalent to using `-d'. 

b (basic)
Basic debugging prints each target that was found to be out-of-date, and whether the build was successful or not. 

v (verbose)
A level above `basic'; includes messages about which makefiles were parsed, prerequisites that did not need to be rebuilt, etc. This option also enables `basic' messages. 

i (implicit)
Prints messages describing the implicit rule searches for each target. This option also enables `basic' messages. 

j (jobs)
Prints messages giving details on the invocation of specific subcommands. 

m (makefile)
By default, the above messages are not enabled while trying to remake the makefiles. This option enables messages while rebuilding makefiles, too. Note that the `all' option does enable this option. This option also enables `basic' messages. 


`-e'
`--environment-overrides'
Give variables taken from the environment precedence over variables from makefiles. See Variables from the Environment. 

`-f file'
`--file=file'
`--makefile=file'
Read the file named file as a makefile. See Writing Makefiles. 

`-h'
`--help'
Remind you of the options that make understands and then exit. 


`-i'
`--ignore-errors'
Ignore all errors in commands executed to remake files. See Errors in Commands. 

`-I dir'
`--include-dir=dir'
Specifies a directory dir to search for included makefiles. See Including Other Makefiles. If several `-I' options are used to specify several directories, the directories are searched in the order specified. 

`-j [jobs]'
`--jobs[=jobs]'
Specifies the number of jobs (commands) to run simultaneously. With no argument, make runs as many jobs simultaneously as possible. If there is more than one `-j' option, the last one is effective. See Parallel Execution, for more information on how commands are run. Note that this option is ignored on MS-DOS. 

`-k'
`--keep-going'
Continue as much as possible after an error. While the target that failed, and those that depend on it, cannot be remade, the other prerequisites of these targets can be processed all the same. See Testing the Compilation of a Program. 

`-l [load]'
`--load-average[=load]'
`--max-load[=load]'
Specifies that no new jobs (commands) should be started if there are other jobs running and the load average is at least load (a floating-point number). With no argument, removes a previous load limit. See Parallel Execution. 

`-L'
`--check-symlink-times'
On systems that support symbolic links, this option causes make to consider the timestamps on any symbolic links in addition to the timestamp on the file referenced by those links. When this option is provided, the most recent timestamp among the file and the symbolic links is taken as the modification time for this target file. 

`-n'
`--just-print'
`--dry-run'
`--recon'
Print the commands that would be executed, but do not execute them. See Instead of Executing the Commands. 


`-o file'
`--old-file=file'
`--assume-old=file'
Do not remake the file file even if it is older than its prerequisites, and do not remake anything on account of changes in file. Essentially the file is treated as very old and its rules are ignored. See Avoiding Recompilation of Some Files. 

`-p'
`--print-data-base'
Print the data base (rules and variable values) that results from reading the makefiles; then execute as usual or as otherwise specified. This also prints the version information given by the `-v' switch (see below). To print the data base without trying to remake any files, use `make -qp'. To print the data base of predefined rules and variables, use `make -p -f /dev/null'. The data base output contains filename and linenumber information for command and variable definitions, so it can be a useful debugging tool in complex environments. 

`-q'
`--question'
 Question mode . Do not run any commands, or print anything; just return an exit status that is zero if the specified targets are already up to date, one if any remaking is required, or two if an error is encountered. See Instead of Executing the Commands. 

`-r'
`--no-builtin-rules'
Eliminate use of the built-in implicit rules (see Using Implicit Rules). You can still define your own by writing pattern rules (see Defining and Redefining Pattern Rules). The `-r' option also clears out the default list of suffixes for suffix rules (see Old-Fashioned Suffix Rules). But you can still define your own suffixes with a rule for .SUFFIXES, and then define your own suffix rules. Note that only rules are affected by the -r option; default variables remain in effect (see Variables Used by Implicit Rules); see the `-R' option below. 

`-R'
`--no-builtin-variables'
Eliminate use of the built-in rule-specific variables (see Variables Used by Implicit Rules). You can still define your own, of course. The `-R' option also automatically enables the `-r' option (see above), since it doesn't make sense to have implicit rules without any definitions for the variables that they use. 

`-s'
`--silent'
`--quiet'
Silent operation; do not print the commands as they are executed. See Command Echoing. 


`-S'
`--no-keep-going'
`--stop'
Cancel the effect of the `-k' option. This is never necessary except in a recursive make where `-k' might be inherited from the top-level make via MAKEFLAGS (see Recursive Use of make) or if you set `-k' in MAKEFLAGS in your environment. 


`-t'
`--touch'
Touch files (mark them up to date without really changing them) instead of running their commands. This is used to pretend that the commands were done, in order to fool future invocations of make. See Instead of Executing the Commands. 


`-v'
`--version'
Print the version of the make program plus a copyright, a list of authors, and a notice that there is no warranty; then exit. 

`-w'
`--print-directory'
Print a message containing the working directory both before and after executing the makefile. This may be useful for tracking down errors from complicated nests of recursive make commands. See Recursive Use of make. (In practice, you rarely need to specify this option since `make' does it for you; see The `--print-directory' Option.) 
`--no-print-directory'
Disable printing of the working directory under -w. This option is useful when -w is turned on automatically, but you do not want to see the extra messages. See The `--print-directory' Option. 

`-W file'
`--what-if=file'
`--new-file=file'
`--assume-new=file'
Pretend that the target file has just been modified. When used with the `-n' flag, this shows you what would happen if you were to modify that file. Without `-n', it is almost the same as running a touch command on the given file before running make, except that the modification time is changed only in the imagination of make. See Instead of Executing the Commands. 

`--warn-undefined-variables'
Issue a warning message whenever make sees a reference to an undefined variable. This can be helpful when you are trying to debug makefiles which use variables in complex ways. 


--------------------------------------------------------------------------------
Next: Archives, Previous: Running, Up: Top 
10 Using Implicit Rules
Certain standard ways of remaking target files are used very often. For example, one customary way to make an object file is from a C source file using the C compiler, cc. 

Implicit rules tell make how to use customary techniques so that you do not have to specify them in detail when you want to use them. For example, there is an implicit rule for C compilation. File names determine which implicit rules are run. For example, C compilation typically takes a .c file and makes a .o file. So make applies the implicit rule for C compilation when it sees this combination of file name endings. 

A chain of implicit rules can apply in sequence; for example, make will remake a .o file from a .y file by way of a .c file. 

The built-in implicit rules use several variables in their commands so that, by changing the values of the variables, you can change the way the implicit rule works. For example, the variable CFLAGS controls the flags given to the C compiler by the implicit rule for C compilation. 

You can define your own implicit rules by writing pattern rules. 

Suffix rules are a more limited way to define implicit rules. Pattern rules are more general and clearer, but suffix rules are retained for compatibility. 

Using Implicit: How to use an existing implicit rule to get the commands for updating a file. 
Catalogue of Rules: A list of built-in implicit rules. 
Implicit Variables: How to change what predefined rules do. 
Chained Rules: How to use a chain of implicit rules. 
Pattern Rules: How to define new implicit rules. 
Last Resort: How to define commands for rules which cannot find any. 
Suffix Rules: The old-fashioned style of implicit rule. 
Implicit Rule Search: The precise algorithm for applying implicit rules. 


--------------------------------------------------------------------------------
Next: Catalogue of Rules, Previous: Implicit Rules, Up: Implicit Rules 
10.1 Using Implicit Rules
To allow make to find a customary method for updating a target file, all you have to do is refrain from specifying commands yourself. Either write a rule with no command lines, or don't write a rule at all. Then make will figure out which implicit rule to use based on which kind of source file exists or can be made. 

For example, suppose the makefile looks like this: 

     foo : foo.o bar.o
             cc -o foo foo.o bar.o $(CFLAGS) $(LDFLAGS)

Because you mention foo.o but do not give a rule for it, make will automatically look for an implicit rule that tells how to update it. This happens whether or not the file foo.o currently exists. 

If an implicit rule is found, it can supply both commands and one or more prerequisites (the source files). You would want to write a rule for foo.o with no command lines if you need to specify additional prerequisites, such as header files, that the implicit rule cannot supply. 

Each implicit rule has a target pattern and prerequisite patterns. There may be many implicit rules with the same target pattern. For example, numerous rules make `.o' files: one, from a `.c' file with the C compiler; another, from a `.p' file with the Pascal compiler; and so on. The rule that actually applies is the one whose prerequisites exist or can be made. So, if you have a file foo.c, make will run the C compiler; otherwise, if you have a file foo.p, make will run the Pascal compiler; and so on. 

Of course, when you write the makefile, you know which implicit rule you want make to use, and you know it will choose that one because you know which possible prerequisite files are supposed to exist. See Catalogue of Implicit Rules, for a catalogue of all the predefined implicit rules. 

Above, we said an implicit rule applies if the required prerequisites  exist or can be made . A file  can be made  if it is mentioned explicitly in the makefile as a target or a prerequisite, or if an implicit rule can be recursively found for how to make it. When an implicit prerequisite is the result of another implicit rule, we say that chaining is occurring. See Chains of Implicit Rules. 

In general, make searches for an implicit rule for each target, and for each double-colon rule, that has no commands. A file that is mentioned only as a prerequisite is considered a target whose rule specifies nothing, so implicit rule search happens for it. See Implicit Rule Search Algorithm, for the details of how the search is done. 

Note that explicit prerequisites do not influence implicit rule search. For example, consider this explicit rule: 

     foo.o: foo.p

The prerequisite on foo.p does not necessarily mean that make will remake foo.o according to the implicit rule to make an object file, a .o file, from a Pascal source file, a .p file. For example, if foo.c also exists, the implicit rule to make an object file from a C source file is used instead, because it appears before the Pascal rule in the list of predefined implicit rules (see Catalogue of Implicit Rules). 

If you do not want an implicit rule to be used for a target that has no commands, you can give that target empty commands by writing a semicolon (see Defining Empty Commands). 



--------------------------------------------------------------------------------
Next: Implicit Variables, Previous: Using Implicit, Up: Implicit Rules 
10.2 Catalogue of Implicit Rules
Here is a catalogue of predefined implicit rules which are always available unless the makefile explicitly overrides or cancels them. See Canceling Implicit Rules, for information on canceling or overriding an implicit rule. The `-r' or `--no-builtin-rules' option cancels all predefined rules. 

This manual only documents the default rules available on POSIX-based operating systems. Other operating systems, such as VMS, Windows, OS/2, etc. may have different sets of default rules. To see the full list of default rules and variables available in your version of GNU make, run `make -p' in a directory with no makefile. 

Not all of these rules will always be defined, even when the `-r' option is not given. Many of the predefined implicit rules are implemented in make as suffix rules, so which ones will be defined depends on the suffix list (the list of prerequisites of the special target .SUFFIXES). The default suffix list is: .out, .a, .ln, .o, .c, .cc, .C, .cpp, .p, .f, .F, .r, .y, .l, .s, .S, .mod, .sym, .def, .h, .info, .dvi, .tex, .texinfo, .texi, .txinfo, .w, .ch .web, .sh, .elc, .el. All of the implicit rules described below whose prerequisites have one of these suffixes are actually suffix rules. If you modify the suffix list, the only predefined suffix rules in effect will be those named by one or two of the suffixes that are on the list you specify; rules whose suffixes fail to be on the list are disabled. See Old-Fashioned Suffix Rules, for full details on suffix rules. 

Compiling C programs
n.o is made automatically from n.c with a command of the form `$(CC) -c $(CPPFLAGS) $(CFLAGS)'. 

Compiling C++ programs
n.o is made automatically from n.cc, n.cpp, or n.C with a command of the form `$(CXX) -c $(CPPFLAGS) $(CXXFLAGS)'. We encourage you to use the suffix `.cc' for C++ source files instead of `.C'. 

Compiling Pascal programs
n.o is made automatically from n.p with the command `$(PC) -c $(PFLAGS)'. 

Compiling Fortran and Ratfor programs
n.o is made automatically from n.r, n.F or n.f by running the Fortran compiler. The precise command used is as follows: 
`.f'
`$(FC) -c $(FFLAGS)'. 

`.F'
`$(FC) -c $(FFLAGS) $(CPPFLAGS)'. 

`.r'
`$(FC) -c $(FFLAGS) $(RFLAGS)'. 


Preprocessing Fortran and Ratfor programs
n.f is made automatically from n.r or n.F. This rule runs just the preprocessor to convert a Ratfor or preprocessable Fortran program into a strict Fortran program. The precise command used is as follows: 
`.F'
`$(FC) -F $(CPPFLAGS) $(FFLAGS)'. 

`.r'
`$(FC) -F $(FFLAGS) $(RFLAGS)'. 


Compiling Modula-2 programs
n.sym is made from n.def with a command of the form `$(M2C) $(M2FLAGS) $(DEFFLAGS)'. n.o is made from n.mod; the form is: `$(M2C) $(M2FLAGS) $(MODFLAGS)'. 

Assembling and preprocessing assembler programs
n.o is made automatically from n.s by running the assembler, as. The precise command is `$(AS) $(ASFLAGS)'. 
n.s is made automatically from n.S by running the C preprocessor, cpp. The precise command is `$(CPP) $(CPPFLAGS)'. 


Linking a single object file
n is made automatically from n.o by running the linker (usually called ld) via the C compiler. The precise command used is `$(CC) $(LDFLAGS) n.o $(LOADLIBES) $(LDLIBS)'. 
This rule does the right thing for a simple program with only one source file. It will also do the right thing if there are multiple object files (presumably coming from various other source files), one of which has a name matching that of the executable file. Thus, 

          x: y.o z.o
     
when x.c, y.c and z.c all exist will execute: 

          cc -c x.c -o x.o
          cc -c y.c -o y.o
          cc -c z.c -o z.o
          cc x.o y.o z.o -o x
          rm -f x.o
          rm -f y.o
          rm -f z.o
     
In more complicated cases, such as when there is no object file whose name derives from the executable file name, you must write an explicit command for linking. 

Each kind of file automatically made into `.o' object files will be automatically linked by using the compiler (`$(CC)', `$(FC)' or `$(PC)'; the C compiler `$(CC)' is used to assemble `.s' files) without the `-c' option. This could be done by using the `.o' object files as intermediates, but it is faster to do the compiling and linking in one step, so that's how it's done. 


Yacc for C programs
n.c is made automatically from n.y by running Yacc with the command `$(YACC) $(YFLAGS)'. 

Lex for C programs
n.c is made automatically from n.l by running Lex. The actual command is `$(LEX) $(LFLAGS)'. 

Lex for Ratfor programs
n.r is made automatically from n.l by running Lex. The actual command is `$(LEX) $(LFLAGS)'. 
The convention of using the same suffix `.l' for all Lex files regardless of whether they produce C code or Ratfor code makes it impossible for make to determine automatically which of the two languages you are using in any particular case. If make is called upon to remake an object file from a `.l' file, it must guess which compiler to use. It will guess the C compiler, because that is more common. If you are using Ratfor, make sure make knows this by mentioning n.r in the makefile. Or, if you are using Ratfor exclusively, with no C files, remove `.c' from the list of implicit rule suffixes with: 

          .SUFFIXES:
          .SUFFIXES: .o .r .f .l ...
     


Making Lint Libraries from C, Yacc, or Lex programs
n.ln is made from n.c by running lint. The precise command is `$(LINT) $(LINTFLAGS) $(CPPFLAGS) -i'. The same command is used on the C code produced from n.y or n.l. 

TeX and Web
n.dvi is made from n.tex with the command `$(TEX)'. n.tex is made from n.web with `$(WEAVE)', or from n.w (and from n.ch if it exists or can be made) with `$(CWEAVE)'. n.p is made from n.web with `$(TANGLE)' and n.c is made from n.w (and from n.ch if it exists or can be made) with `$(CTANGLE)'. 

Texinfo and Info
n.dvi is made from n.texinfo, n.texi, or n.txinfo, with the command `$(TEXI2DVI) $(TEXI2DVI_FLAGS)'. n.info is made from n.texinfo, n.texi, or n.txinfo, with the command `$(MAKEINFO) $(MAKEINFO_FLAGS)'. 

RCS
Any file n is extracted if necessary from an RCS file named either n,v or RCS/n,v. The precise command used is `$(CO) $(COFLAGS)'. n will not be extracted from RCS if it already exists, even if the RCS file is newer. The rules for RCS are terminal (see Match-Anything Pattern Rules), so RCS files cannot be generated from another source; they must actually exist. 

SCCS
Any file n is extracted if necessary from an SCCS file named either s.n or SCCS/s.n. The precise command used is `$(GET) $(GFLAGS)'. The rules for SCCS are terminal (see Match-Anything Pattern Rules), so SCCS files cannot be generated from another source; they must actually exist. 
For the benefit of SCCS, a file n is copied from n.sh and made executable (by everyone). This is for shell scripts that are checked into SCCS. Since RCS preserves the execution permission of a file, you do not need to use this feature with RCS. 

We recommend that you avoid using of SCCS. RCS is widely held to be superior, and is also free. By choosing free software in place of comparable (or inferior) proprietary software, you support the free software movement. 

Usually, you want to change only the variables listed in the table above, which are documented in the following section. 

However, the commands in built-in implicit rules actually use variables such as COMPILE.c, LINK.p, and PREPROCESS.S, whose values contain the commands listed above. 

make follows the convention that the rule to compile a .x source file uses the variable COMPILE.x. Similarly, the rule to produce an executable from a .x file uses LINK.x; and the rule to preprocess a .x file uses PREPROCESS.x. 

Every rule that produces an object file uses the variable OUTPUT_OPTION. make defines this variable either to contain `-o $@', or to be empty, depending on a compile-time option. You need the `-o' option to ensure that the output goes into the right file when the source file is in a different directory, as when using VPATH (see Directory Search). However, compilers on some systems do not accept a `-o' switch for object files. If you use such a system, and use VPATH, some compilations will put their output in the wrong place. A possible workaround for this problem is to give OUTPUT_OPTION the value `; mv $*.o $@'. 



--------------------------------------------------------------------------------
Next: Chained Rules, Previous: Catalogue of Rules, Up: Implicit Rules 
10.3 Variables Used by Implicit Rules
The commands in built-in implicit rules make liberal use of certain predefined variables. You can alter the values of these variables in the makefile, with arguments to make, or in the environment to alter how the implicit rules work without redefining the rules themselves. You can cancel all variables used by implicit rules with the `-R' or `--no-builtin-variables' option. 

For example, the command used to compile a C source file actually says `$(CC) -c $(CFLAGS) $(CPPFLAGS)'. The default values of the variables used are `cc' and nothing, resulting in the command `cc -c'. By redefining `CC' to `ncc', you could cause `ncc' to be used for all C compilations performed by the implicit rule. By redefining `CFLAGS' to be `-g', you could pass the `-g' option to each compilation. All implicit rules that do C compilation use `$(CC)' to get the program name for the compiler and all include `$(CFLAGS)' among the arguments given to the compiler. 

The variables used in implicit rules fall into two classes: those that are names of programs (like CC) and those that contain arguments for the programs (like CFLAGS). (The  name of a program  may also contain some command arguments, but it must start with an actual executable program name.) If a variable value contains more than one argument, separate them with spaces. 

The following tables describe of some of the more commonly-used predefined variables. This list is not exhaustive, and the default values shown here may not be what are selected by make for your environment. To see the complete list of predefined variables for your instance of GNU make you can run `make -p' in a directory with no makefiles. 

Here is a table of some of the more common variables used as names of programs in built-in rules: makefiles. 

AR
Archive-maintaining program; default `ar'. 

AS
Program for compiling assembly files; default `as'. 

CC
Program for compiling C programs; default `cc'. 

CO
Program for checking out files from RCS; default `co'. 

CXX
Program for compiling C++ programs; default `g++'. 

CO
Program for extracting a file from RCS; default `co'. 

CPP
Program for running the C preprocessor, with results to standard output; default `$(CC) -E'. 

FC
Program for compiling or preprocessing Fortran and Ratfor programs; default `f77'. 

GET
Program for extracting a file from SCCS; default `get'. 

LEX
Program to use to turn Lex grammars into source code; default `lex'. 

YACC
Program to use to turn Yacc grammars into source code; default `yacc'. 

LINT
Program to use to run lint on source code; default `lint'. 

M2C
Program to use to compile Modula-2 source code; default `m2c'. 

PC
Program for compiling Pascal programs; default `pc'. 

MAKEINFO
Program to convert a Texinfo source file into an Info file; default `makeinfo'. 

TEX
Program to make TeX dvi files from TeX source; default `tex'. 

TEXI2DVI
Program to make TeX dvi files from Texinfo source; default `texi2dvi'. 

WEAVE
Program to translate Web into TeX; default `weave'. 

CWEAVE
Program to translate C Web into TeX; default `cweave'. 

TANGLE
Program to translate Web into Pascal; default `tangle'. 

CTANGLE
Program to translate C Web into C; default `ctangle'. 

RM
Command to remove a file; default `rm -f'. 
Here is a table of variables whose values are additional arguments for the programs above. The default values for all of these is the empty string, unless otherwise noted. 

ARFLAGS
Flags to give the archive-maintaining program; default `rv'. 

ASFLAGS
Extra flags to give to the assembler (when explicitly invoked on a `.s' or `.S' file). 

CFLAGS
Extra flags to give to the C compiler. 

CXXFLAGS
Extra flags to give to the C++ compiler. 

COFLAGS
Extra flags to give to the RCS co program. 

CPPFLAGS
Extra flags to give to the C preprocessor and programs that use it (the C and Fortran compilers). 

FFLAGS
Extra flags to give to the Fortran compiler. 

GFLAGS
Extra flags to give to the SCCS get program. 

LDFLAGS
Extra flags to give to compilers when they are supposed to invoke the linker, `ld'. 

LFLAGS
Extra flags to give to Lex. 

YFLAGS
Extra flags to give to Yacc. 

PFLAGS
Extra flags to give to the Pascal compiler. 

RFLAGS
Extra flags to give to the Fortran compiler for Ratfor programs. 

LINTFLAGS
Extra flags to give to lint. 


--------------------------------------------------------------------------------
Next: Pattern Rules, Previous: Implicit Variables, Up: Implicit Rules 
10.4 Chains of Implicit Rules
Sometimes a file can be made by a sequence of implicit rules. For example, a file n.o could be made from n.y by running first Yacc and then cc. Such a sequence is called a chain. 

If the file n.c exists, or is mentioned in the makefile, no special searching is required: make finds that the object file can be made by C compilation from n.c; later on, when considering how to make n.c, the rule for running Yacc is used. Ultimately both n.c and n.o are updated. 

However, even if n.c does not exist and is not mentioned, make knows how to envision it as the missing link between n.o and n.y! In this case, n.c is called an intermediate file. Once make has decided to use the intermediate file, it is entered in the data base as if it had been mentioned in the makefile, along with the implicit rule that says how to create it. 

Intermediate files are remade using their rules just like all other files. But intermediate files are treated differently in two ways. 

The first difference is what happens if the intermediate file does not exist. If an ordinary file b does not exist, and make considers a target that depends on b, it invariably creates b and then updates the target from b. But if b is an intermediate file, then make can leave well enough alone. It won't bother updating b, or the ultimate target, unless some prerequisite of b is newer than that target or there is some other reason to update that target. 

The second difference is that if make does create b in order to update something else, it deletes b later on after it is no longer needed. Therefore, an intermediate file which did not exist before make also does not exist after make. make reports the deletion to you by printing a `rm -f' command showing which file it is deleting. 

Ordinarily, a file cannot be intermediate if it is mentioned in the makefile as a target or prerequisite. However, you can explicitly mark a file as intermediate by listing it as a prerequisite of the special target .INTERMEDIATE. This takes effect even if the file is mentioned explicitly in some other way. 

You can prevent automatic deletion of an intermediate file by marking it as a secondary file. To do this, list it as a prerequisite of the special target .SECONDARY. When a file is secondary, make will not create the file merely because it does not already exist, but make does not automatically delete the file. Marking a file as secondary also marks it as intermediate. 

You can list the target pattern of an implicit rule (such as `%.o') as a prerequisite of the special target .PRECIOUS to preserve intermediate files made by implicit rules whose target patterns match that file's name; see Interrupts. A chain can involve more than two implicit rules. For example, it is possible to make a file foo from RCS/foo.y,v by running RCS, Yacc and cc. Then both foo.y and foo.c are intermediate files that are deleted at the end. 

No single implicit rule can appear more than once in a chain. This means that make will not even consider such a ridiculous thing as making foo from foo.o.o by running the linker twice. This constraint has the added benefit of preventing any infinite loop in the search for an implicit rule chain. 

There are some special implicit rules to optimize certain cases that would otherwise be handled by rule chains. For example, making foo from foo.c could be handled by compiling and linking with separate chained rules, using foo.o as an intermediate file. But what actually happens is that a special rule for this case does the compilation and linking with a single cc command. The optimized rule is used in preference to the step-by-step chain because it comes earlier in the ordering of rules. 



--------------------------------------------------------------------------------
Next: Last Resort, Previous: Chained Rules, Up: Implicit Rules 
10.5 Defining and Redefining Pattern Rules
You define an implicit rule by writing a pattern rule. A pattern rule looks like an ordinary rule, except that its target contains the character `%' (exactly one of them). The target is considered a pattern for matching file names; the `%' can match any nonempty substring, while other characters match only themselves. The prerequisites likewise use `%' to show how their names relate to the target name. 

Thus, a pattern rule `%.o : %.c' says how to make any file stem.o from another file stem.c. 

Note that expansion using `%' in pattern rules occurs after any variable or function expansions, which take place when the makefile is read. See How to Use Variables, and Functions for Transforming Text. 

Pattern Intro: An introduction to pattern rules. 
Pattern Examples: Examples of pattern rules. 
Automatic Variables: How to use automatic variables in the commands of implicit rules. 
Pattern Match: How patterns match. 
Match-Anything Rules: Precautions you should take prior to defining rules that can match any target file whatever. 
Canceling Rules: How to override or cancel built-in rules. 


--------------------------------------------------------------------------------
Next: Pattern Examples, Previous: Pattern Rules, Up: Pattern Rules 
10.5.1 Introduction to Pattern Rules
A pattern rule contains the character `%' (exactly one of them) in the target; otherwise, it looks exactly like an ordinary rule. The target is a pattern for matching file names; the `%' matches any nonempty substring, while other characters match only themselves. For example, `%.c' as a pattern matches any file name that ends in `.c'. `s.%.c' as a pattern matches any file name that starts with `s.', ends in `.c' and is at least five characters long. (There must be at least one character to match the `%'.) The substring that the `%' matches is called the stem. 

`%' in a prerequisite of a pattern rule stands for the same stem that was matched by the `%' in the target. In order for the pattern rule to apply, its target pattern must match the file name under consideration and all of its prerequisites (after pattern substitution) must name files that exist or can be made. These files become prerequisites of the target. Thus, a rule of the form 

     %.o : %.c ; command...

specifies how to make a file n.o, with another file n.c as its prerequisite, provided that n.c exists or can be made. 

There may also be prerequisites that do not use `%'; such a prerequisite attaches to every file made by this pattern rule. These unvarying prerequisites are useful occasionally. 

A pattern rule need not have any prerequisites that contain `%', or in fact any prerequisites at all. Such a rule is effectively a general wildcard. It provides a way to make any file that matches the target pattern. See Last Resort. 

Pattern rules may have more than one target. Unlike normal rules, this does not act as many different rules with the same prerequisites and commands. If a pattern rule has multiple targets, make knows that the rule's commands are responsible for making all of the targets. The commands are executed only once to make all the targets. When searching for a pattern rule to match a target, the target patterns of a rule other than the one that matches the target in need of a rule are incidental: make worries only about giving commands and prerequisites to the file presently in question. However, when this file's commands are run, the other targets are marked as having been updated themselves. The order in which pattern rules appear in the makefile is important since this is the order in which they are considered. Of equally applicable rules, only the first one found is used. The rules you write take precedence over those that are built in. Note however, that a rule whose prerequisites actually exist or are mentioned always takes priority over a rule with prerequisites that must be made by chaining other implicit rules. 



--------------------------------------------------------------------------------
Next: Automatic Variables, Previous: Pattern Intro, Up: Pattern Rules 
10.5.2 Pattern Rule Examples
Here are some examples of pattern rules actually predefined in make. First, the rule that compiles `.c' files into `.o' files: 

     %.o : %.c
             $(CC) -c $(CFLAGS) $(CPPFLAGS) $< -o $@

defines a rule that can make any file x.o from x.c. The command uses the automatic variables `$@' and `$<' to substitute the names of the target file and the source file in each case where the rule applies (see Automatic Variables). 

Here is a second built-in rule: 

     % :: RCS/%,v
             $(CO) $(COFLAGS) $<

defines a rule that can make any file x whatsoever from a corresponding file x,v in the subdirectory RCS. Since the target is `%', this rule will apply to any file whatever, provided the appropriate prerequisite file exists. The double colon makes the rule terminal, which means that its prerequisite may not be an intermediate file (see Match-Anything Pattern Rules). 

This pattern rule has two targets: 

     %.tab.c %.tab.h: %.y
             bison -d $<

This tells make that the command `bison -d x.y' will make both x.tab.c and x.tab.h. If the file foo depends on the files parse.tab.o and scan.o and the file scan.o depends on the file parse.tab.h, when parse.y is changed, the command `bison -d parse.y' will be executed only once, and the prerequisites of both parse.tab.o and scan.o will be satisfied. (Presumably the file parse.tab.o will be recompiled from parse.tab.c and the file scan.o from scan.c, while foo is linked from parse.tab.o, scan.o, and its other prerequisites, and it will execute happily ever after.) 



--------------------------------------------------------------------------------
Next: Pattern Match, Previous: Pattern Examples, Up: Pattern Rules 
10.5.3 Automatic Variables
Suppose you are writing a pattern rule to compile a `.c' file into a `.o' file: how do you write the `cc' command so that it operates on the right source file name? You cannot write the name in the command, because the name is different each time the implicit rule is applied. 

What you do is use a special feature of make, the automatic variables. These variables have values computed afresh for each rule that is executed, based on the target and prerequisites of the rule. In this example, you would use `$@' for the object file name and `$<' for the source file name. 

It's very important that you recognize the limited scope in which automatic variable values are available: they only have values within the command script. In particular, you cannot use them anywhere within the target list of a rule; they have no value there and will expand to the empty string. Also, they cannot be accessed directly within the prerequisite list of a rule. A common mistake is attempting to use $@ within the prerequisites list; this will not work. However, there is a special feature of GNU make, secondary expansion (see Secondary Expansion), which will allow automatic variable values to be used in prerequisite lists. 

Here is a table of automatic variables: 

$@
The file name of the target of the rule. If the target is an archive member, then `$@' is the name of the archive file. In a pattern rule that has multiple targets (see Introduction to Pattern Rules), `$@' is the name of whichever target caused the rule's commands to be run. 



$%
The target member name, when the target is an archive member. See Archives. For example, if the target is foo.a(bar.o) then `$%' is bar.o and `$@' is foo.a. `$%' is empty when the target is not an archive member. 



$<
The name of the first prerequisite. If the target got its commands from an implicit rule, this will be the first prerequisite added by the implicit rule (see Implicit Rules). 



$?
The names of all the prerequisites that are newer than the target, with spaces between them. For prerequisites which are archive members, only the member named is used (see Archives). 

$^
The names of all the prerequisites, with spaces between them. For prerequisites which are archive members, only the member named is used (see Archives). A target has only one prerequisite on each other file it depends on, no matter how many times each file is listed as a prerequisite. So if you list a prerequisite more than once for a target, the value of $^ contains just one copy of the name. This list does not contain any of the order-only prerequisites; for those see the `$|' variable, below. 

$+
This is like `$^', but prerequisites listed more than once are duplicated in the order they were listed in the makefile. This is primarily useful for use in linking commands where it is meaningful to repeat library file names in a particular order. 



$|
The names of all the order-only prerequisites, with spaces between them. 



$*
The stem with which an implicit rule matches (see How Patterns Match). If the target is dir/a.foo.b and the target pattern is a.%.b then the stem is dir/foo. The stem is useful for constructing names of related files. In a static pattern rule, the stem is part of the file name that matched the `%' in the target pattern. 
In an explicit rule, there is no stem; so `$*' cannot be determined in that way. Instead, if the target name ends with a recognized suffix (see Old-Fashioned Suffix Rules), `$*' is set to the target name minus the suffix. For example, if the target name is `foo.c', then `$*' is set to `foo', since `.c' is a suffix. GNU make does this bizarre thing only for compatibility with other implementations of make. You should generally avoid using `$*' except in implicit rules or static pattern rules. 

If the target name in an explicit rule does not end with a recognized suffix, `$*' is set to the empty string for that rule. 

`$?' is useful even in explicit rules when you wish to operate on only the prerequisites that have changed. For example, suppose that an archive named lib is supposed to contain copies of several object files. This rule copies just the changed object files into the archive: 

     lib: foo.o bar.o lose.o win.o
             ar r lib $?

Of the variables listed above, four have values that are single file names, and three have values that are lists of file names. These seven have variants that get just the file's directory name or just the file name within the directory. The variant variables' names are formed by appending `D' or `F', respectively. These variants are semi-obsolete in GNU make since the functions dir and notdir can be used to get a similar effect (see Functions for File Names). Note, however, that the `D' variants all omit the trailing slash which always appears in the output of the dir function. Here is a table of the variants: 

`$(@D)'
The directory part of the file name of the target, with the trailing slash removed. If the value of `$@' is dir/foo.o then `$(@D)' is dir. This value is . if `$@' does not contain a slash. 



`$(@F)'
The file-within-directory part of the file name of the target. If the value of `$@' is dir/foo.o then `$(@F)' is foo.o. `$(@F)' is equivalent to `$(notdir $@)'. 



`$(*D)'
`$(*F)'
The directory part and the file-within-directory part of the stem; dir and foo in this example. 



`$(%D)'
`$(%F)'
The directory part and the file-within-directory part of the target archive member name. This makes sense only for archive member targets of the form archive(member) and is useful only when member may contain a directory name. (See Archive Members as Targets.) 



`$(<D)'
`$(<F)'
The directory part and the file-within-directory part of the first prerequisite. 



`$(^D)'
`$(^F)'
Lists of the directory parts and the file-within-directory parts of all prerequisites. 



`$(+D)'
`$(+F)'
Lists of the directory parts and the file-within-directory parts of all prerequisites, including multiple instances of duplicated prerequisites. 



`$(?D)'
`$(?F)'
Lists of the directory parts and the file-within-directory parts of all prerequisites that are newer than the target. 
Note that we use a special stylistic convention when we talk about these automatic variables; we write  the value of `$<' , rather than  the variable <  as we would write for ordinary variables such as objects and CFLAGS. We think this convention looks more natural in this special case. Please do not assume it has a deep significance; `$<' refers to the variable named < just as `$(CFLAGS)' refers to the variable named CFLAGS. You could just as well use `$(<)' in place of `$<'. 



--------------------------------------------------------------------------------
Next: Match-Anything Rules, Previous: Automatic Variables, Up: Pattern Rules 
10.5.4 How Patterns Match
A target pattern is composed of a `%' between a prefix and a suffix, either or both of which may be empty. The pattern matches a file name only if the file name starts with the prefix and ends with the suffix, without overlap. The text between the prefix and the suffix is called the stem. Thus, when the pattern `%.o' matches the file name test.o, the stem is `test'. The pattern rule prerequisites are turned into actual file names by substituting the stem for the character `%'. Thus, if in the same example one of the prerequisites is written as `%.c', it expands to `test.c'. 

When the target pattern does not contain a slash (and it usually does not), directory names in the file names are removed from the file name before it is compared with the target prefix and suffix. After the comparison of the file name to the target pattern, the directory names, along with the slash that ends them, are added on to the prerequisite file names generated from the pattern rule's prerequisite patterns and the file name. The directories are ignored only for the purpose of finding an implicit rule to use, not in the application of that rule. Thus, `e%t' matches the file name src/eat, with `src/a' as the stem. When prerequisites are turned into file names, the directories from the stem are added at the front, while the rest of the stem is substituted for the `%'. The stem `src/a' with a prerequisite pattern `c%r' gives the file name src/car. 



--------------------------------------------------------------------------------
Next: Canceling Rules, Previous: Pattern Match, Up: Pattern Rules 
10.5.5 Match-Anything Pattern Rules
When a pattern rule's target is just `%', it matches any file name whatever. We call these rules match-anything rules. They are very useful, but it can take a lot of time for make to think about them, because it must consider every such rule for each file name listed either as a target or as a prerequisite. 

Suppose the makefile mentions foo.c. For this target, make would have to consider making it by linking an object file foo.c.o, or by C compilation-and-linking in one step from foo.c.c, or by Pascal compilation-and-linking from foo.c.p, and many other possibilities. 

We know these possibilities are ridiculous since foo.c is a C source file, not an executable. If make did consider these possibilities, it would ultimately reject them, because files such as foo.c.o and foo.c.p would not exist. But these possibilities are so numerous that make would run very slowly if it had to consider them. 

To gain speed, we have put various constraints on the way make considers match-anything rules. There are two different constraints that can be applied, and each time you define a match-anything rule you must choose one or the other for that rule. 

One choice is to mark the match-anything rule as terminal by defining it with a double colon. When a rule is terminal, it does not apply unless its prerequisites actually exist. Prerequisites that could be made with other implicit rules are not good enough. In other words, no further chaining is allowed beyond a terminal rule. 

For example, the built-in implicit rules for extracting sources from RCS and SCCS files are terminal; as a result, if the file foo.c,v does not exist, make will not even consider trying to make it as an intermediate file from foo.c,v.o or from RCS/SCCS/s.foo.c,v. RCS and SCCS files are generally ultimate source files, which should not be remade from any other files; therefore, make can save time by not looking for ways to remake them. 

If you do not mark the match-anything rule as terminal, then it is nonterminal. A nonterminal match-anything rule cannot apply to a file name that indicates a specific type of data. A file name indicates a specific type of data if some non-match-anything implicit rule target matches it. 

For example, the file name foo.c matches the target for the pattern rule `%.c : %.y' (the rule to run Yacc). Regardless of whether this rule is actually applicable (which happens only if there is a file foo.y), the fact that its target matches is enough to prevent consideration of any nonterminal match-anything rules for the file foo.c. Thus, make will not even consider trying to make foo.c as an executable file from foo.c.o, foo.c.c, foo.c.p, etc. 

The motivation for this constraint is that nonterminal match-anything rules are used for making files containing specific types of data (such as executable files) and a file name with a recognized suffix indicates some other specific type of data (such as a C source file). 

Special built-in dummy pattern rules are provided solely to recognize certain file names so that nonterminal match-anything rules will not be considered. These dummy rules have no prerequisites and no commands, and they are ignored for all other purposes. For example, the built-in implicit rule 

     %.p :

exists to make sure that Pascal source files such as foo.p match a specific target pattern and thereby prevent time from being wasted looking for foo.p.o or foo.p.c. 

Dummy pattern rules such as the one for `%.p' are made for every suffix listed as valid for use in suffix rules (see Old-Fashioned Suffix Rules). 



--------------------------------------------------------------------------------
Previous: Match-Anything Rules, Up: Pattern Rules 
10.5.6 Canceling Implicit Rules
You can override a built-in implicit rule (or one you have defined yourself) by defining a new pattern rule with the same target and prerequisites, but different commands. When the new rule is defined, the built-in one is replaced. The new rule's position in the sequence of implicit rules is determined by where you write the new rule. 

You can cancel a built-in implicit rule by defining a pattern rule with the same target and prerequisites, but no commands. For example, the following would cancel the rule that runs the assembler: 

     %.o : %.s



--------------------------------------------------------------------------------
Next: Suffix Rules, Previous: Pattern Rules, Up: Implicit Rules 
10.6 Defining Last-Resort Default Rules
You can define a last-resort implicit rule by writing a terminal match-anything pattern rule with no prerequisites (see Match-Anything Rules). This is just like any other pattern rule; the only thing special about it is that it will match any target. So such a rule's commands are used for all targets and prerequisites that have no commands of their own and for which no other implicit rule applies. 

For example, when testing a makefile, you might not care if the source files contain real data, only that they exist. Then you might do this: 

     %::
             touch $@

to cause all the source files needed (as prerequisites) to be created automatically. 

You can instead define commands to be used for targets for which there are no rules at all, even ones which don't specify commands. You do this by writing a rule for the target .DEFAULT. Such a rule's commands are used for all prerequisites which do not appear as targets in any explicit rule, and for which no implicit rule applies. Naturally, there is no .DEFAULT rule unless you write one. 

If you use .DEFAULT with no commands or prerequisites: 

     .DEFAULT:

the commands previously stored for .DEFAULT are cleared. Then make acts as if you had never defined .DEFAULT at all. 

If you do not want a target to get the commands from a match-anything pattern rule or .DEFAULT, but you also do not want any commands to be run for the target, you can give it empty commands (see Defining Empty Commands). 

You can use a last-resort rule to override part of another makefile. See Overriding Part of Another Makefile. 



--------------------------------------------------------------------------------
Next: Implicit Rule Search, Previous: Last Resort, Up: Implicit Rules 
10.7 Old-Fashioned Suffix Rules
Suffix rules are the old-fashioned way of defining implicit rules for make. Suffix rules are obsolete because pattern rules are more general and clearer. They are supported in GNU make for compatibility with old makefiles. They come in two kinds: double-suffix and single-suffix. 

A double-suffix rule is defined by a pair of suffixes: the target suffix and the source suffix. It matches any file whose name ends with the target suffix. The corresponding implicit prerequisite is made by replacing the target suffix with the source suffix in the file name. A two-suffix rule whose target and source suffixes are `.o' and `.c' is equivalent to the pattern rule `%.o : %.c'. 

A single-suffix rule is defined by a single suffix, which is the source suffix. It matches any file name, and the corresponding implicit prerequisite name is made by appending the source suffix. A single-suffix rule whose source suffix is `.c' is equivalent to the pattern rule `% : %.c'. 

Suffix rule definitions are recognized by comparing each rule's target against a defined list of known suffixes. When make sees a rule whose target is a known suffix, this rule is considered a single-suffix rule. When make sees a rule whose target is two known suffixes concatenated, this rule is taken as a double-suffix rule. 

For example, `.c' and `.o' are both on the default list of known suffixes. Therefore, if you define a rule whose target is `.c.o', make takes it to be a double-suffix rule with source suffix `.c' and target suffix `.o'. Here is the old-fashioned way to define the rule for compiling a C source file: 

     .c.o:
             $(CC) -c $(CFLAGS) $(CPPFLAGS) -o $@ $<

Suffix rules cannot have any prerequisites of their own. If they have any, they are treated as normal files with funny names, not as suffix rules. Thus, the rule: 

     .c.o: foo.h
             $(CC) -c $(CFLAGS) $(CPPFLAGS) -o $@ $<

tells how to make the file .c.o from the prerequisite file foo.h, and is not at all like the pattern rule: 

     %.o: %.c foo.h
             $(CC) -c $(CFLAGS) $(CPPFLAGS) -o $@ $<

which tells how to make `.o' files from `.c' files, and makes all `.o' files using this pattern rule also depend on foo.h. 

Suffix rules with no commands are also meaningless. They do not remove previous rules as do pattern rules with no commands (see Canceling Implicit Rules). They simply enter the suffix or pair of suffixes concatenated as a target in the data base. 

The known suffixes are simply the names of the prerequisites of the special target .SUFFIXES. You can add your own suffixes by writing a rule for .SUFFIXES that adds more prerequisites, as in: 

     .SUFFIXES: .hack .win

which adds `.hack' and `.win' to the end of the list of suffixes. 

If you wish to eliminate the default known suffixes instead of just adding to them, write a rule for .SUFFIXES with no prerequisites. By special dispensation, this eliminates all existing prerequisites of .SUFFIXES. You can then write another rule to add the suffixes you want. For example, 

     .SUFFIXES:            # Delete the default suffixes
     .SUFFIXES: .c .o .h   # Define our suffix list

The `-r' or `--no-builtin-rules' flag causes the default list of suffixes to be empty. 

The variable SUFFIXES is defined to the default list of suffixes before make reads any makefiles. You can change the list of suffixes with a rule for the special target .SUFFIXES, but that does not alter this variable. 



--------------------------------------------------------------------------------
Previous: Suffix Rules, Up: Implicit Rules 
10.8 Implicit Rule Search Algorithm
Here is the procedure make uses for searching for an implicit rule for a target t. This procedure is followed for each double-colon rule with no commands, for each target of ordinary rules none of which have commands, and for each prerequisite that is not the target of any rule. It is also followed recursively for prerequisites that come from implicit rules, in the search for a chain of rules. 

Suffix rules are not mentioned in this algorithm because suffix rules are converted to equivalent pattern rules once the makefiles have been read in. 

For an archive member target of the form `archive(member)', the following algorithm is run twice, first using the entire target name t, and second using `(member)' as the target t if the first run found no rule. 

Split t into a directory part, called d, and the rest, called n. For example, if t is `src/foo.o', then d is `src/' and n is `foo.o'. 
Make a list of all the pattern rules one of whose targets matches t or n. If the target pattern contains a slash, it is matched against t; otherwise, against n. 
If any rule in that list is not a match-anything rule, then remove all nonterminal match-anything rules from the list. 
Remove from the list all rules with no commands. 
For each pattern rule in the list: 
Find the stem s, which is the nonempty part of t or n matched by the `%' in the target pattern. 
Compute the prerequisite names by substituting s for `%'; if the target pattern does not contain a slash, append d to the front of each prerequisite name. 
Test whether all the prerequisites exist or ought to exist. (If a file name is mentioned in the makefile as a target or as an explicit prerequisite, then we say it ought to exist.) 
If all prerequisites exist or ought to exist, or there are no prerequisites, then this rule applies. 

If no pattern rule has been found so far, try harder. For each pattern rule in the list: 
If the rule is terminal, ignore it and go on to the next rule. 
Compute the prerequisite names as before. 
Test whether all the prerequisites exist or ought to exist. 
For each prerequisite that does not exist, follow this algorithm recursively to see if the prerequisite can be made by an implicit rule. 
If all prerequisites exist, ought to exist, or can be made by implicit rules, then this rule applies. 
If no implicit rule applies, the rule for .DEFAULT, if any, applies. In that case, give t the same commands that .DEFAULT has. Otherwise, there are no commands for t. 
Once a rule that applies has been found, for each target pattern of the rule other than the one that matched t or n, the `%' in the pattern is replaced with s and the resultant file name is stored until the commands to remake the target file t are executed. After these commands are executed, each of these stored file names are entered into the data base and marked as having been updated and having the same update status as the file t. 

When the commands of a pattern rule are executed for t, the automatic variables are set corresponding to the target and prerequisites. See Automatic Variables. 



--------------------------------------------------------------------------------
Next: Features, Previous: Implicit Rules, Up: Top 
11 Using make to Update Archive Files
Archive files are files containing named subfiles called members; they are maintained with the program ar and their main use is as subroutine libraries for linking. 

Archive Members: Archive members as targets. 
Archive Update: The implicit rule for archive member targets. 
Archive Pitfalls: Dangers to watch out for when using archives. 
Archive Suffix Rules: You can write a special kind of suffix rule for updating archives. 


--------------------------------------------------------------------------------
Next: Archive Update, Previous: Archives, Up: Archives 
11.1 Archive Members as Targets
An individual member of an archive file can be used as a target or prerequisite in make. You specify the member named member in archive file archive as follows: 

     archive(member)

This construct is available only in targets and prerequisites, not in commands! Most programs that you might use in commands do not support this syntax and cannot act directly on archive members. Only ar and other programs specifically designed to operate on archives can do so. Therefore, valid commands to update an archive member target probably must use ar. For example, this rule says to create a member hack.o in archive foolib by copying the file hack.o: 

     foolib(hack.o) : hack.o
             ar cr foolib hack.o

In fact, nearly all archive member targets are updated in just this way and there is an implicit rule to do it for you. Please note: The `c' flag to ar is required if the archive file does not already exist. 

To specify several members in the same archive, you can write all the member names together between the parentheses. For example: 

     foolib(hack.o kludge.o)

is equivalent to: 

     foolib(hack.o) foolib(kludge.o)

You can also use shell-style wildcards in an archive member reference. See Using Wildcard Characters in File Names. For example, `foolib(*.o)' expands to all existing members of the foolib archive whose names end in `.o'; perhaps `foolib(hack.o) foolib(kludge.o)'. 



--------------------------------------------------------------------------------
Next: Archive Pitfalls, Previous: Archive Members, Up: Archives 
11.2 Implicit Rule for Archive Member Targets
Recall that a target that looks like a(m) stands for the member named m in the archive file a. 

When make looks for an implicit rule for such a target, as a special feature it considers implicit rules that match (m), as well as those that match the actual target a(m). 

This causes one special rule whose target is (%) to match. This rule updates the target a(m) by copying the file m into the archive. For example, it will update the archive member target foo.a(bar.o) by copying the file bar.o into the archive foo.a as a member named bar.o. 

When this rule is chained with others, the result is very powerful. Thus, `make "foo.a(bar.o)"' (the quotes are needed to protect the `(' and `)' from being interpreted specially by the shell) in the presence of a file bar.c is enough to cause the following commands to be run, even without a makefile: 

     cc -c bar.c -o bar.o
     ar r foo.a bar.o
     rm -f bar.o

Here make has envisioned the file bar.o as an intermediate file. See Chains of Implicit Rules. 

Implicit rules such as this one are written using the automatic variable `$%'. See Automatic Variables. 

An archive member name in an archive cannot contain a directory name, but it may be useful in a makefile to pretend that it does. If you write an archive member target foo.a(dir/file.o), make will perform automatic updating with this command: 

     ar r foo.a dir/file.o

which has the effect of copying the file dir/file.o into a member named file.o. In connection with such usage, the automatic variables %D and %F may be useful. 

Archive Symbols: How to update archive symbol directories. 


--------------------------------------------------------------------------------
Previous: Archive Update, Up: Archive Update 
11.2.1 Updating Archive Symbol Directories
An archive file that is used as a library usually contains a special member named __.SYMDEF that contains a directory of the external symbol names defined by all the other members. After you update any other members, you need to update __.SYMDEF so that it will summarize the other members properly. This is done by running the ranlib program: 

     ranlib archivefile

Normally you would put this command in the rule for the archive file, and make all the members of the archive file prerequisites of that rule. For example, 

     libfoo.a: libfoo.a(x.o) libfoo.a(y.o) ...
             ranlib libfoo.a

The effect of this is to update archive members x.o, y.o, etc., and then update the symbol directory member __.SYMDEF by running ranlib. The rules for updating the members are not shown here; most likely you can omit them and use the implicit rule which copies files into the archive, as described in the preceding section. 

This is not necessary when using the GNU ar program, which updates the __.SYMDEF member automatically. 



--------------------------------------------------------------------------------
Next: Archive Suffix Rules, Previous: Archive Update, Up: Archives 
11.3 Dangers When Using Archives
It is important to be careful when using parallel execution (the -j switch; see Parallel Execution) and archives. If multiple ar commands run at the same time on the same archive file, they will not know about each other and can corrupt the file. 

Possibly a future version of make will provide a mechanism to circumvent this problem by serializing all commands that operate on the same archive file. But for the time being, you must either write your makefiles to avoid this problem in some other way, or not use -j. 



--------------------------------------------------------------------------------
Previous: Archive Pitfalls, Up: Archives 
11.4 Suffix Rules for Archive Files
You can write a special kind of suffix rule for dealing with archive files. See Suffix Rules, for a full explanation of suffix rules. Archive suffix rules are obsolete in GNU make, because pattern rules for archives are a more general mechanism (see Archive Update). But they are retained for compatibility with other makes. 

To write a suffix rule for archives, you simply write a suffix rule using the target suffix `.a' (the usual suffix for archive files). For example, here is the old-fashioned suffix rule to update a library archive from C source files: 

     .c.a:
             $(CC) $(CFLAGS) $(CPPFLAGS) -c $< -o $*.o
             $(AR) r $@ $*.o
             $(RM) $*.o

This works just as if you had written the pattern rule: 

     (%.o): %.c
             $(CC) $(CFLAGS) $(CPPFLAGS) -c $< -o $*.o
             $(AR) r $@ $*.o
             $(RM) $*.o

In fact, this is just what make does when it sees a suffix rule with `.a' as the target suffix. Any double-suffix rule `.x.a' is converted to a pattern rule with the target pattern `(%.o)' and a prerequisite pattern of `%.x'. 

Since you might want to use `.a' as the suffix for some other kind of file, make also converts archive suffix rules to pattern rules in the normal way (see Suffix Rules). Thus a double-suffix rule `.x.a' produces two pattern rules: `(%.o): %.x' and `%.a: %.x'. 



--------------------------------------------------------------------------------
Next: Missing, Previous: Archives, Up: Top 
12 Features of GNU make
Here is a summary of the features of GNU make, for comparison with and credit to other versions of make. We consider the features of make in 4.2 BSD systems as a baseline. If you are concerned with writing portable makefiles, you should not use the features of make listed here, nor the ones in Missing. 

Many features come from the version of make in System V. 

The VPATH variable and its special meaning. See Searching Directories for Prerequisites. This feature exists in System V make, but is undocumented. It is documented in 4.3 BSD make (which says it mimics System V's VPATH feature). 
Included makefiles. See Including Other Makefiles. Allowing multiple files to be included with a single directive is a GNU extension. 
Variables are read from and communicated via the environment. See Variables from the Environment. 
Options passed through the variable MAKEFLAGS to recursive invocations of make. See Communicating Options to a Sub-make. 
The automatic variable $% is set to the member name in an archive reference. See Automatic Variables. 
The automatic variables $@, $*, $<, $%, and $? have corresponding forms like $(@F) and $(@D). We have generalized this to $^ as an obvious extension. See Automatic Variables. 
Substitution variable references. See Basics of Variable References. 
The command-line options `-b' and `-m', accepted and ignored. In System V make, these options actually do something. 
Execution of recursive commands to run make via the variable MAKE even if `-n', `-q' or `-t' is specified. See Recursive Use of make. 
Support for suffix `.a' in suffix rules. See Archive Suffix Rules. This feature is obsolete in GNU make, because the general feature of rule chaining (see Chains of Implicit Rules) allows one pattern rule for installing members in an archive (see Archive Update) to be sufficient. 
The arrangement of lines and backslash-newline combinations in commands is retained when the commands are printed, so they appear as they do in the makefile, except for the stripping of initial whitespace. 
The following features were inspired by various other versions of make. In some cases it is unclear exactly which versions inspired which others. 

Pattern rules using `%'. This has been implemented in several versions of make. We're not sure who invented it first, but it's been spread around a bit. See Defining and Redefining Pattern Rules. 
Rule chaining and implicit intermediate files. This was implemented by Stu Feldman in his version of make for AT&T Eighth Edition Research Unix, and later by Andrew Hume of AT&T Bell Labs in his mk program (where he terms it  transitive closure ). We do not really know if we got this from either of them or thought it up ourselves at the same time. See Chains of Implicit Rules. 
The automatic variable $^ containing a list of all prerequisites of the current target. We did not invent this, but we have no idea who did. See Automatic Variables. The automatic variable $+ is a simple extension of $^. 
The  what if  flag (`-W' in GNU make) was (as far as we know) invented by Andrew Hume in mk. See Instead of Executing the Commands. 
The concept of doing several things at once (parallelism) exists in many incarnations of make and similar programs, though not in the System V or BSD implementations. See Command Execution. 
Modified variable references using pattern substitution come from SunOS 4. See Basics of Variable References. This functionality was provided in GNU make by the patsubst function before the alternate syntax was implemented for compatibility with SunOS 4. It is not altogether clear who inspired whom, since GNU make had patsubst before SunOS 4 was released. 
The special significance of `+' characters preceding command lines (see Instead of Executing the Commands) is mandated by IEEE Standard 1003.2-1992 (POSIX.2). 
The `+=' syntax to append to the value of a variable comes from SunOS 4 make. See Appending More Text to Variables. 
The syntax `archive(mem1 mem2...)' to list multiple members in a single archive file comes from SunOS 4 make. See Archive Members. 
The -include directive to include makefiles with no error for a nonexistent file comes from SunOS 4 make. (But note that SunOS 4 make does not allow multiple makefiles to be specified in one -include directive.) The same feature appears with the name sinclude in SGI make and perhaps others. 
The remaining features are inventions new in GNU make: 

Use the `-v' or `--version' option to print version and copyright information. 
Use the `-h' or `--help' option to summarize the options to make. 
Simply-expanded variables. See The Two Flavors of Variables. 
Pass command-line variable assignments automatically through the variable MAKE to recursive make invocations. See Recursive Use of make. 
Use the `-C' or `--directory' command option to change directory. See Summary of Options. 
Make verbatim variable definitions with define. See Defining Variables Verbatim. 
Declare phony targets with the special target .PHONY. 
Andrew Hume of AT&T Bell Labs implemented a similar feature with a different syntax in his mk program. This seems to be a case of parallel discovery. See Phony Targets. 

Manipulate text by calling functions. See Functions for Transforming Text. 
Use the `-o' or `--old-file' option to pretend a file's modification-time is old. See Avoiding Recompilation of Some Files. 
Conditional execution. 
This feature has been implemented numerous times in various versions of make; it seems a natural extension derived from the features of the C preprocessor and similar macro languages and is not a revolutionary concept. See Conditional Parts of Makefiles. 

Specify a search path for included makefiles. See Including Other Makefiles. 
Specify extra makefiles to read with an environment variable. See The Variable MAKEFILES. 
Strip leading sequences of `./' from file names, so that ./file and file are considered to be the same file. 
Use a special search method for library prerequisites written in the form `-lname'. See Directory Search for Link Libraries. 
Allow suffixes for suffix rules (see Old-Fashioned Suffix Rules) to contain any characters. In other versions of make, they must begin with `.' and not contain any `/' characters. 
Keep track of the current level of make recursion using the variable MAKELEVEL. See Recursive Use of make. 
Provide any goals given on the command line in the variable MAKECMDGOALS. See Arguments to Specify the Goals. 
Specify static pattern rules. See Static Pattern Rules. 
Provide selective vpath search. See Searching Directories for Prerequisites. 
Provide computed variable references. See Basics of Variable References. 
Update makefiles. See How Makefiles Are Remade. System V make has a very, very limited form of this functionality in that it will check out SCCS files for makefiles. 
Various new built-in implicit rules. See Catalogue of Implicit Rules. 
The built-in variable `MAKE_VERSION' gives the version number of make. 


--------------------------------------------------------------------------------
Next: Makefile Conventions, Previous: Features, Up: Top 
13 Incompatibilities and Missing Features
The make programs in various other systems support a few features that are not implemented in GNU make. The POSIX.2 standard (IEEE Standard 1003.2-1992) which specifies make does not require any of these features. 

A target of the form `file((entry))' stands for a member of archive file file. The member is chosen, not by name, but by being an object file which defines the linker symbol entry. 
This feature was not put into GNU make because of the nonmodularity of putting knowledge into make of the internal format of archive file symbol tables. See Updating Archive Symbol Directories. 

Suffixes (used in suffix rules) that end with the character `~' have a special meaning to System V make; they refer to the SCCS file that corresponds to the file one would get without the `~'. For example, the suffix rule `.c~.o' would make the file n.o from the SCCS file s.n.c. For complete coverage, a whole series of such suffix rules is required. See Old-Fashioned Suffix Rules. 
In GNU make, this entire series of cases is handled by two pattern rules for extraction from SCCS, in combination with the general feature of rule chaining. See Chains of Implicit Rules. 

In System V and 4.3 BSD make, files found by VPATH search (see Searching Directories for Prerequisites) have their names changed inside command strings. We feel it is much cleaner to always use automatic variables and thus make this feature obsolete. 
In some Unix makes, the automatic variable $* appearing in the prerequisites of a rule has the amazingly strange  feature  of expanding to the full name of the target of that rule. We cannot imagine what went on in the minds of Unix make developers to do this; it is utterly inconsistent with the normal definition of $*. 
In some Unix makes, implicit rule search (see Using Implicit Rules) is apparently done for all targets, not just those without commands. This means you can do: 
          foo.o:
                  cc -c foo.c
     
and Unix make will intuit that foo.o depends on foo.c. 

We feel that such usage is broken. The prerequisite properties of make are well-defined (for GNU make, at least), and doing such a thing simply does not fit the model. 

GNU make does not include any built-in implicit rules for compiling or preprocessing EFL programs. If we hear of anyone who is using EFL, we will gladly add them. 
It appears that in SVR4 make, a suffix rule can be specified with no commands, and it is treated as if it had empty commands (see Empty Commands). For example: 
          .c.a:
     
will override the built-in .c.a suffix rule. 

We feel that it is cleaner for a rule without commands to always simply add to the prerequisite list for the target. The above example can be easily rewritten to get the desired behavior in GNU make: 

          .c.a: ;
     
Some versions of make invoke the shell with the `-e' flag, except under `-k' (see Testing the Compilation of a Program). The `-e' flag tells the shell to exit as soon as any program it runs returns a nonzero status. We feel it is cleaner to write each shell command line to stand on its own and not require this special treatment. 


--------------------------------------------------------------------------------
Next: Quick Reference, Previous: Missing, Up: Top 
14 Makefile Conventions

This describes conventions for writing the Makefiles for GNU programs. Using Automake will help you write a Makefile that follows these conventions. 

Makefile Basics: General Conventions for Makefiles 
Utilities in Makefiles: Utilities in Makefiles 
Command Variables: Variables for Specifying Commands 
Directory Variables: Variables for Installation Directories 
Standard Targets: Standard Targets for Users 
Install Command Categories: Three categories of commands in the `install' rule: normal, pre-install and post-install. 


--------------------------------------------------------------------------------
Next: Utilities in Makefiles, Up: Makefile Conventions 
14.1 General Conventions for Makefiles
Every Makefile should contain this line: 

     SHELL = /bin/sh

to avoid trouble on systems where the SHELL variable might be inherited from the environment. (This is never a problem with GNU make.) 

Different make programs have incompatible suffix lists and implicit rules, and this sometimes creates confusion or misbehavior. So it is a good idea to set the suffix list explicitly using only the suffixes you need in the particular Makefile, like this: 

     .SUFFIXES:
     .SUFFIXES: .c .o

The first line clears out the suffix list, the second introduces all suffixes which may be subject to implicit rules in this Makefile. 

Don't assume that . is in the path for command execution. When you need to run programs that are a part of your package during the make, please make sure that it uses ./ if the program is built as part of the make or $(srcdir)/ if the file is an unchanging part of the source code. Without one of these prefixes, the current search path is used. 

The distinction between ./ (the build directory) and $(srcdir)/ (the source directory) is important because users can build in a separate directory using the `--srcdir' option to configure. A rule of the form: 

     foo.1 : foo.man sedscript
             sed -e sedscript foo.man > foo.1

will fail when the build directory is not the source directory, because foo.man and sedscript are in the source directory. 

When using GNU make, relying on `VPATH' to find the source file will work in the case where there is a single dependency file, since the make automatic variable `$<' will represent the source file wherever it is. (Many versions of make set `$<' only in implicit rules.) A Makefile target like 

     foo.o : bar.c
             $(CC) -I. -I$(srcdir) $(CFLAGS) -c bar.c -o foo.o

should instead be written as 

     foo.o : bar.c
             $(CC) -I. -I$(srcdir) $(CFLAGS) -c $< -o $@

in order to allow `VPATH' to work correctly. When the target has multiple dependencies, using an explicit `$(srcdir)' is the easiest way to make the rule work well. For example, the target above for foo.1 is best written as: 

     foo.1 : foo.man sedscript
             sed -e $(srcdir)/sedscript $(srcdir)/foo.man > $@

GNU distributions usually contain some files which are not source files for example, Info files, and the output from Autoconf, Automake, Bison or Flex. Since these files normally appear in the source directory, they should always appear in the source directory, not in the build directory. So Makefile rules to update them should put the updated files in the source directory. 

However, if a file does not appear in the distribution, then the Makefile should not put it in the source directory, because building a program in ordinary circumstances should not modify the source directory in any way. 

Try to make the build and installation targets, at least (and all their subtargets) work correctly with a parallel make. 



--------------------------------------------------------------------------------
Next: Command Variables, Previous: Makefile Basics, Up: Makefile Conventions 
14.2 Utilities in Makefiles
Write the Makefile commands (and any shell scripts, such as configure) to run in sh, not in csh. Don't use any special features of ksh or bash. 

The configure script and the Makefile rules for building and installation should not use any utilities directly except these: 

     cat cmp cp diff echo egrep expr false grep install-info
     ln ls mkdir mv pwd rm rmdir sed sleep sort tar test touch true

The compression program gzip can be used in the dist rule. 

Stick to the generally supported options for these programs. For example, don't use `mkdir -p', convenient as it may be, because most systems don't support it. 

It is a good idea to avoid creating symbolic links in makefiles, since a few systems don't support them. 

The Makefile rules for building and installation can also use compilers and related programs, but should do so via make variables so that the user can substitute alternatives. Here are some of the programs we mean: 

     ar bison cc flex install ld ldconfig lex
     make makeinfo ranlib texi2dvi yacc

Use the following make variables to run those programs: 

     $(AR) $(BISON) $(CC) $(FLEX) $(INSTALL) $(LD) $(LDCONFIG) $(LEX)
     $(MAKE) $(MAKEINFO) $(RANLIB) $(TEXI2DVI) $(YACC)

When you use ranlib or ldconfig, you should make sure nothing bad happens if the system does not have the program in question. Arrange to ignore an error from that command, and print a message before the command to tell the user that failure of this command does not mean a problem. (The Autoconf `AC_PROG_RANLIB' macro can help with this.) 

If you use symbolic links, you should implement a fallback for systems that don't have symbolic links. 

Additional utilities that can be used via Make variables are: 

     chgrp chmod chown mknod

It is ok to use other utilities in Makefile portions (or scripts) intended only for particular systems where you know those utilities exist. 



--------------------------------------------------------------------------------
Next: Directory Variables, Previous: Utilities in Makefiles, Up: Makefile Conventions 
14.3 Variables for Specifying Commands
Makefiles should provide variables for overriding certain commands, options, and so on. 

In particular, you should run most utility programs via variables. Thus, if you use Bison, have a variable named BISON whose default value is set with `BISON = bison', and refer to it with $(BISON) whenever you need to use Bison. 

File management utilities such as ln, rm, mv, and so on, need not be referred to through variables in this way, since users don't need to replace them with other programs. 

Each program-name variable should come with an options variable that is used to supply options to the program. Append `FLAGS' to the program-name variable name to get the options variable name for example, BISONFLAGS. (The names CFLAGS for the C compiler, YFLAGS for yacc, and LFLAGS for lex, are exceptions to this rule, but we keep them because they are standard.) Use CPPFLAGS in any compilation command that runs the preprocessor, and use LDFLAGS in any compilation command that does linking as well as in any direct use of ld. 

If there are C compiler options that must be used for proper compilation of certain files, do not include them in CFLAGS. Users expect to be able to specify CFLAGS freely themselves. Instead, arrange to pass the necessary options to the C compiler independently of CFLAGS, by writing them explicitly in the compilation commands or by defining an implicit rule, like this: 

     CFLAGS = -g
     ALL_CFLAGS = -I. $(CFLAGS)
     .c.o:
             $(CC) -c $(CPPFLAGS) $(ALL_CFLAGS) $<

Do include the `-g' option in CFLAGS, because that is not required for proper compilation. You can consider it a default that is only recommended. If the package is set up so that it is compiled with GCC by default, then you might as well include `-O' in the default value of CFLAGS as well. 

Put CFLAGS last in the compilation command, after other variables containing compiler options, so the user can use CFLAGS to override the others. 

CFLAGS should be used in every invocation of the C compiler, both those which do compilation and those which do linking. 

Every Makefile should define the variable INSTALL, which is the basic command for installing a file into the system. 

Every Makefile should also define the variables INSTALL_PROGRAM and INSTALL_DATA. (The default for INSTALL_PROGRAM should be $(INSTALL); the default for INSTALL_DATA should be ${INSTALL} -m 644.) Then it should use those variables as the commands for actual installation, for executables and nonexecutables respectively. Use these variables as follows: 

     $(INSTALL_PROGRAM) foo $(bindir)/foo
     $(INSTALL_DATA) libfoo.a $(libdir)/libfoo.a

Optionally, you may prepend the value of DESTDIR to the target filename. Doing this allows the installer to create a snapshot of the installation to be copied onto the real target filesystem later. Do not set the value of DESTDIR in your Makefile, and do not include it in any installed files. With support for DESTDIR, the above examples become: 

     $(INSTALL_PROGRAM) foo $(DESTDIR)$(bindir)/foo
     $(INSTALL_DATA) libfoo.a $(DESTDIR)$(libdir)/libfoo.a

Always use a file name, not a directory name, as the second argument of the installation commands. Use a separate command for each file to be installed. 



--------------------------------------------------------------------------------
Next: Standard Targets, Previous: Command Variables, Up: Makefile Conventions 
14.4 Variables for Installation Directories
Installation directories should always be named by variables, so it is easy to install in a nonstandard place. The standard names for these variables and the values they should have in GNU packages are described below. They are based on a standard filesystem layout; variants of it are used in GNU/Linux and other modern operating systems. 

Installers are expected to override these values when calling make (e.g., make prefix=/usr install or configure (e.g., configure --prefix=/usr). GNU packages should not try to guess which value should be appropriate for these variables on the system they are being installed onto: use the default settings specified here so that all GNU packages behave identically, allowing the installer to achieve any desired layout. 

These two variables set the root for the installation. All the other installation directories should be subdirectories of one of these two, and nothing should be directly installed into these two directories. 

prefix
A prefix used in constructing the default values of the variables listed below. The default value of prefix should be /usr/local. When building the complete GNU system, the prefix will be empty and /usr will be a symbolic link to /. (If you are using Autoconf, write it as `@prefix@'.) 
Running `make install' with a different value of prefix from the one used to build the program should not recompile the program. 


exec_prefix
A prefix used in constructing the default values of some of the variables listed below. The default value of exec_prefix should be $(prefix). (If you are using Autoconf, write it as `@exec_prefix@'.) 
Generally, $(exec_prefix) is used for directories that contain machine-specific files (such as executables and subroutine libraries), while $(prefix) is used directly for other directories. 

Running `make install' with a different value of exec_prefix from the one used to build the program should not recompile the program. 

Executable programs are installed in one of the following directories. 

bindir
The directory for installing executable programs that users can run. This should normally be /usr/local/bin, but write it as $(exec_prefix)/bin. (If you are using Autoconf, write it as `@bindir@'.) 

sbindir
The directory for installing executable programs that can be run from the shell, but are only generally useful to system administrators. This should normally be /usr/local/sbin, but write it as $(exec_prefix)/sbin. (If you are using Autoconf, write it as `@sbindir@'.) 

libexecdir
The directory for installing executable programs to be run by other programs rather than by users. This directory should normally be /usr/local/libexec, but write it as $(exec_prefix)/libexec. (If you are using Autoconf, write it as `@libexecdir@'.) 
The definition of `libexecdir' is the same for all packages, so you should install your data in a subdirectory thereof. Most packages install their data under $(libexecdir)/package-name/, possibly within additional subdirectories thereof, such as $(libexecdir)/package-name/machine/version. 

Data files used by the program during its execution are divided into categories in two ways. 

Some files are normally modified by programs; others are never normally modified (though users may edit some of these). 
Some files are architecture-independent and can be shared by all machines at a site; some are architecture-dependent and can be shared only by machines of the same kind and operating system; others may never be shared between two machines. 
This makes for six different possibilities. However, we want to discourage the use of architecture-dependent files, aside from object files and libraries. It is much cleaner to make other data files architecture-independent, and it is generally not hard. 

Here are the variables Makefiles should use to specify directories to put these various kinds of files in: 

`datarootdir'
The root of the directory tree for read-only architecture-independent data files. This should normally be /usr/local/share, but write it as $(prefix)/share. (If you are using Autoconf, write it as `@datarootdir@'.) `datadir''s default value is based on this variable; so are `infodir', `mandir', and others. 

`datadir'
The directory for installing idiosyncratic read-only architecture-independent data files for this program. This is usually the same place as `datarootdir', but we use the two separate variables so that you can move these program-specific files without altering the location for Info files, man pages, etc. 
This should normally be /usr/local/share, but write it as $(datarootdir). (If you are using Autoconf, write it as `@datadir@'.) 

The definition of `datadir' is the same for all packages, so you should install your data in a subdirectory thereof. Most packages install their data under $(datadir)/package-name/. 


`sysconfdir'
The directory for installing read-only data files that pertain to a single machine that is to say, files for configuring a host. Mailer and network configuration files, /etc/passwd, and so forth belong here. All the files in this directory should be ordinary ASCII text files. This directory should normally be /usr/local/etc, but write it as $(prefix)/etc. (If you are using Autoconf, write it as `@sysconfdir@'.) 
Do not install executables here in this directory (they probably belong in $(libexecdir) or $(sbindir)). Also do not install files that are modified in the normal course of their use (programs whose purpose is to change the configuration of the system excluded). Those probably belong in $(localstatedir). 


`sharedstatedir'
The directory for installing architecture-independent data files which the programs modify while they run. This should normally be /usr/local/com, but write it as $(prefix)/com. (If you are using Autoconf, write it as `@sharedstatedir@'.) 

`localstatedir'
The directory for installing data files which the programs modify while they run, and that pertain to one specific machine. Users should never need to modify files in this directory to configure the package's operation; put such configuration information in separate files that go in $(datadir) or $(sysconfdir). $(localstatedir) should normally be /usr/local/var, but write it as $(prefix)/var. (If you are using Autoconf, write it as `@localstatedir@'.) 
These variables specify the directory for installing certain specific types of files, if your program has them. Every GNU package should have Info files, so every program needs `infodir', but not all need `libdir' or `lispdir'. 

`includedir'
The directory for installing header files to be included by user programs with the C `#include' preprocessor directive. This should normally be /usr/local/include, but write it as $(prefix)/include. (If you are using Autoconf, write it as `@includedir@'.) 
Most compilers other than GCC do not look for header files in directory /usr/local/include. So installing the header files this way is only useful with GCC. Sometimes this is not a problem because some libraries are only really intended to work with GCC. But some libraries are intended to work with other compilers. They should install their header files in two places, one specified by includedir and one specified by oldincludedir. 


`oldincludedir'
The directory for installing `#include' header files for use with compilers other than GCC. This should normally be /usr/include. (If you are using Autoconf, you can write it as `@oldincludedir@'.) 
The Makefile commands should check whether the value of oldincludedir is empty. If it is, they should not try to use it; they should cancel the second installation of the header files. 

A package should not replace an existing header in this directory unless the header came from the same package. Thus, if your Foo package provides a header file foo.h, then it should install the header file in the oldincludedir directory if either (1) there is no foo.h there or (2) the foo.h that exists came from the Foo package. 

To tell whether foo.h came from the Foo package, put a magic string in the file part of a comment and grep for that string. 


`docdir'
The directory for installing documentation files (other than Info) for this package. By default, it should be /usr/local/share/doc/yourpkg, but it should be written as $(datarootdir)/doc/yourpkg. (If you are using Autoconf, write it as `@docdir@'.) The yourpkg subdirectory, which may include a version number, prevents collisions among files with common names, such as README. 

`infodir'
The directory for installing the Info files for this package. By default, it should be /usr/local/share/info, but it should be written as $(datarootdir)/info. (If you are using Autoconf, write it as `@infodir@'.) infodir is separate from docdir for compatibility with existing practice. 

`htmldir'
`dvidir'
`pdfdir'
`psdir'
Directories for installing documentation files in the particular format. (It is not required to support documentation in all these formats.) They should all be set to $(docdir) by default. (If you are using Autoconf, write them as `@htmldir@', `@dvidir@', etc.) Packages which supply several translations of their documentation should install them in `$(htmldir)/'ll, `$(pdfdir)/'ll, etc. where ll is a locale abbreviation such as `en' or `pt_BR'. 

`libdir'
The directory for object files and libraries of object code. Do not install executables here, they probably ought to go in $(libexecdir) instead. The value of libdir should normally be /usr/local/lib, but write it as $(exec_prefix)/lib. (If you are using Autoconf, write it as `@libdir@'.) 

`lispdir'
The directory for installing any Emacs Lisp files in this package. By default, it should be /usr/local/share/emacs/site-lisp, but it should be written as $(datarootdir)/emacs/site-lisp. 
If you are using Autoconf, write the default as `@lispdir@'. In order to make `@lispdir@' work, you need the following lines in your configure.in file: 

          lispdir='${datarootdir}/emacs/site-lisp'
          AC_SUBST(lispdir)
     


`localedir'
The directory for installing locale-specific message catalogs for this package. By default, it should be /usr/local/share/locale, but it should be written as $(datarootdir)/locale. (If you are using Autoconf, write it as `@localedir@'.) This directory usually has a subdirectory per locale. 
Unix-style man pages are installed in one of the following: 

`mandir'
The top-level directory for installing the man pages (if any) for this package. It will normally be /usr/local/share/man, but you should write it as $(datarootdir)/man. (If you are using Autoconf, write it as `@mandir@'.) 

`man1dir'
The directory for installing section 1 man pages. Write it as $(mandir)/man1. 

`man2dir'
The directory for installing section 2 man pages. Write it as $(mandir)/man2 

`...'
Don't make the primary documentation for any GNU software be a man page. Write a manual in Texinfo instead. Man pages are just for the sake of people running GNU software on Unix, which is a secondary application only. 

`manext'
The file name extension for the installed man page. This should contain a period followed by the appropriate digit; it should normally be `.1'. 

`man1ext'
The file name extension for installed section 1 man pages. 

`man2ext'
The file name extension for installed section 2 man pages. 

`...'
Use these names instead of `manext' if the package needs to install man pages in more than one section of the manual. 
And finally, you should set the following variable: 

`srcdir'
The directory for the sources being compiled. The value of this variable is normally inserted by the configure shell script. (If you are using Autconf, use `srcdir = @srcdir@'.) 
For example: 

     
     
     # Common prefix for installation directories.
     # NOTE: This directory must exist when you start the install.
     prefix = /usr/local
     datarootdir = $(prefix)/share
     datadir = $(datarootdir)
     exec_prefix = $(prefix)
     # Where to put the executable for the command `gcc'.
     bindir = $(exec_prefix)/bin
     # Where to put the directories used by the compiler.
     libexecdir = $(exec_prefix)/libexec
     # Where to put the Info files.
     infodir = $(datarootdir)/info

If your program installs a large number of files into one of the standard user-specified directories, it might be useful to group them into a subdirectory particular to that program. If you do this, you should write the install rule to create these subdirectories. 

Do not expect the user to include the subdirectory name in the value of any of the variables listed above. The idea of having a uniform set of variable names for installation directories is to enable the user to specify the exact same values for several different GNU packages. In order for this to be useful, all the packages must be designed so that they will work sensibly when the user does so. 



--------------------------------------------------------------------------------
Next: Install Command Categories, Previous: Directory Variables, Up: Makefile Conventions 
14.5 Standard Targets for Users
All GNU programs should have the following targets in their Makefiles: 

`all'
Compile the entire program. This should be the default target. This target need not rebuild any documentation files; Info files should normally be included in the distribution, and DVI files should be made only when explicitly asked for. 
By default, the Make rules should compile and link with `-g', so that executable programs have debugging symbols. Users who don't mind being helpless can strip the executables later if they wish. 


`install'
Compile the program and copy the executables, libraries, and so on to the file names where they should reside for actual use. If there is a simple test to verify that a program is properly installed, this target should run that test. 
Do not strip executables when installing them. Devil-may-care users can use the install-strip target to do that. 

If possible, write the install target rule so that it does not modify anything in the directory where the program was built, provided `make all' has just been done. This is convenient for building the program under one user name and installing it under another. 

The commands should create all the directories in which files are to be installed, if they don't already exist. This includes the directories specified as the values of the variables prefix and exec_prefix, as well as all subdirectories that are needed. One way to do this is by means of an installdirs target as described below. 

Use `-' before any command for installing a man page, so that make will ignore any errors. This is in case there are systems that don't have the Unix man page documentation system installed. 

The way to install Info files is to copy them into $(infodir) with $(INSTALL_DATA) (see Command Variables), and then run the install-info program if it is present. install-info is a program that edits the Info dir file to add or update the menu entry for the given Info file; it is part of the Texinfo package. Here is a sample rule to install an Info file: 

          $(DESTDIR)$(infodir)/foo.info: foo.info
                  $(POST_INSTALL)
          # There may be a newer info file in . than in srcdir.
                  -if test -f foo.info; then d=.; \
                   else d=$(srcdir); fi; \
                  $(INSTALL_DATA) $$d/foo.info $(DESTDIR)$@; \
          # Run install-info only if it exists.
          # Use `if' instead of just prepending `-' to the
          # line so we notice real errors from install-info.
          # We use `$(SHELL) -c' because some shells do not
          # fail gracefully when there is an unknown command.
                  if $(SHELL) -c 'install-info --version' \
                     >/dev/null 2>&1; then \
                    install-info --dir-file=$(DESTDIR)$(infodir)/dir \
                                 $(DESTDIR)$(infodir)/foo.info; \
                  else true; fi
     
When writing the install target, you must classify all the commands into three categories: normal ones, pre-installation commands and post-installation commands. See Install Command Categories. 


`install-html'
`install-dvi'
`install-pdf'
`install-ps'
These targets install documentation in formats other than Info; they're intended to be called explicitly by the person installing the package, if that format is desired. GNU prefers Info files, so these must be installed by the install target. 
When you have many documentation files to install, we recommend that you avoid collisions and clutter by arranging for these targets to install in subdirectories of the appropriate installation directory, such as htmldir. As one example, if your package has multiple manuals, and you wish to install HTML documentation with many files (such as the  split  mode output by makeinfo --html), you'll certainly want to use subdirectories, or two nodes with the same name in different manuals will overwrite each other. 


`uninstall'
Delete all the installed files the copies that the `install' and `install-*' targets create. 
This rule should not modify the directories where compilation is done, only the directories where files are installed. 

The uninstallation commands are divided into three categories, just like the installation commands. See Install Command Categories. 


`install-strip'
Like install, but strip the executable files while installing them. In simple cases, this target can use the install target in a simple way: 
          install-strip:
                  $(MAKE) INSTALL_PROGRAM='$(INSTALL_PROGRAM) -s' \
                          install
     
But if the package installs scripts as well as real executables, the install-strip target can't just refer to the install target; it has to strip the executables but not the scripts. 

install-strip should not strip the executables in the build directory which are being copied for installation. It should only strip the copies that are installed. 

Normally we do not recommend stripping an executable unless you are sure the program has no bugs. However, it can be reasonable to install a stripped executable for actual execution while saving the unstripped executable elsewhere in case there is a bug. 


`clean'
Delete all files in the current directory that are normally created by building the program. Also delete files in other directories if they are created by this makefile. However, don't delete the files that record the configuration. Also preserve files that could be made by building, but normally aren't because the distribution comes with them. There is no need to delete parent directories that were created with `mkdir -p', since they could have existed anyway. 
Delete .dvi files here if they are not part of the distribution. 


`distclean'
Delete all files in the current directory (or created by this makefile) that are created by configuring or building the program. If you have unpacked the source and built the program without creating any other files, `make distclean' should leave only the files that were in the distribution. However, there is no need to delete parent directories that were created with `mkdir -p', since they could have existed anyway. 

`mostlyclean'
Like `clean', but may refrain from deleting a few files that people normally don't want to recompile. For example, the `mostlyclean' target for GCC does not delete libgcc.a, because recompiling it is rarely necessary and takes a lot of time. 

`maintainer-clean'
Delete almost everything that can be reconstructed with this Makefile. This typically includes everything deleted by distclean, plus more: C source files produced by Bison, tags tables, Info files, and so on. 
The reason we say  almost everything  is that running the command `make maintainer-clean' should not delete configure even if configure can be remade using a rule in the Makefile. More generally, `make maintainer-clean' should not delete anything that needs to exist in order to run configure and then begin to build the program. Also, there is no need to delete parent directories that were created with `mkdir -p', since they could have existed anyway. These are the only exceptions; maintainer-clean should delete everything else that can be rebuilt. 

The `maintainer-clean' target is intended to be used by a maintainer of the package, not by ordinary users. You may need special tools to reconstruct some of the files that `make maintainer-clean' deletes. Since these files are normally included in the distribution, we don't take care to make them easy to reconstruct. If you find you need to unpack the full distribution again, don't blame us. 

To help make users aware of this, the commands for the special maintainer-clean target should start with these two: 

          @echo 'This command is intended for maintainers to use; it'
          @echo 'deletes files that may need special tools to rebuild.'
     


`TAGS'
Update a tags table for this program. 

`info'
Generate any Info files needed. The best way to write the rules is as follows: 
          info: foo.info
          
          foo.info: foo.texi chap1.texi chap2.texi
                  $(MAKEINFO) $(srcdir)/foo.texi
     
You must define the variable MAKEINFO in the Makefile. It should run the makeinfo program, which is part of the Texinfo distribution. 

Normally a GNU distribution comes with Info files, and that means the Info files are present in the source directory. Therefore, the Make rule for an info file should update it in the source directory. When users build the package, ordinarily Make will not update the Info files because they will already be up to date. 


`dvi'
`html'
`pdf'
`ps'
Generate documentation files in the given format, if possible. Here's an example rule for generating DVI files from Texinfo: 
          dvi: foo.dvi
          
          foo.dvi: foo.texi chap1.texi chap2.texi
                  $(TEXI2DVI) $(srcdir)/foo.texi
     
You must define the variable TEXI2DVI in the Makefile. It should run the program texi2dvi, which is part of the Texinfo distribution.3 Alternatively, write just the dependencies, and allow GNU make to provide the command. 

Here's another example, this one for generating HTML from Texinfo: 

          html: foo.html
          
          foo.html: foo.texi chap1.texi chap2.texi
                  $(TEXI2HTML) $(srcdir)/foo.texi
     
Again, you would define the variable TEXI2HTML in the Makefile; for example, it might run makeinfo --no-split --html (makeinfo is part of the Texinfo distribution). 


`dist'
Create a distribution tar file for this program. The tar file should be set up so that the file names in the tar file start with a subdirectory name which is the name of the package it is a distribution for. This name can include the version number. 
For example, the distribution tar file of GCC version 1.40 unpacks into a subdirectory named gcc-1.40. 

The easiest way to do this is to create a subdirectory appropriately named, use ln or cp to install the proper files in it, and then tar that subdirectory. 

Compress the tar file with gzip. For example, the actual distribution file for GCC version 1.40 is called gcc-1.40.tar.gz. 

The dist target should explicitly depend on all non-source files that are in the distribution, to make sure they are up to date in the distribution. See Making Releases. 


`check'
Perform self-tests (if any). The user must build the program before running the tests, but need not install the program; you should write the self-tests so that they work when the program is built but not installed. 
The following targets are suggested as conventional names, for programs in which they are useful. 

installcheck
Perform installation tests (if any). The user must build and install the program before running the tests. You should not assume that $(bindir) is in the search path. 

installdirs
It's useful to add a target named `installdirs' to create the directories where files are installed, and their parent directories. There is a script called mkinstalldirs which is convenient for this; you can find it in the Texinfo package. You can use a rule like this: 
          # Make sure all installation directories (e.g. $(bindir))
          # actually exist by making them if necessary.
          installdirs: mkinstalldirs
                  $(srcdir)/mkinstalldirs $(bindir) $(datadir) \
                                          $(libdir) $(infodir) \
                                          $(mandir)
     
or, if you wish to support DESTDIR, 

          # Make sure all installation directories (e.g. $(bindir))
          # actually exist by making them if necessary.
          installdirs: mkinstalldirs
                  $(srcdir)/mkinstalldirs \
                      $(DESTDIR)$(bindir) $(DESTDIR)$(datadir) \
                      $(DESTDIR)$(libdir) $(DESTDIR)$(infodir) \
                      $(DESTDIR)$(mandir)
     
This rule should not modify the directories where compilation is done. It should do nothing but create installation directories. 



--------------------------------------------------------------------------------
Previous: Standard Targets, Up: Makefile Conventions 
14.6 Install Command Categories
When writing the install target, you must classify all the commands into three categories: normal ones, pre-installation commands and post-installation commands. 

Normal commands move files into their proper places, and set their modes. They may not alter any files except the ones that come entirely from the package they belong to. 

Pre-installation and post-installation commands may alter other files; in particular, they can edit global configuration files or data bases. 

Pre-installation commands are typically executed before the normal commands, and post-installation commands are typically run after the normal commands. 

The most common use for a post-installation command is to run install-info. This cannot be done with a normal command, since it alters a file (the Info directory) which does not come entirely and solely from the package being installed. It is a post-installation command because it needs to be done after the normal command which installs the package's Info files. 

Most programs don't need any pre-installation commands, but we have the feature just in case it is needed. 

To classify the commands in the install rule into these three categories, insert category lines among them. A category line specifies the category for the commands that follow. 

A category line consists of a tab and a reference to a special Make variable, plus an optional comment at the end. There are three variables you can use, one for each category; the variable name specifies the category. Category lines are no-ops in ordinary execution because these three Make variables are normally undefined (and you should not define them in the makefile). 

Here are the three possible category lines, each with a comment that explains what it means: 

             $(PRE_INSTALL)     # Pre-install commands follow.
             $(POST_INSTALL)    # Post-install commands follow.
             $(NORMAL_INSTALL)  # Normal commands follow.

If you don't use a category line at the beginning of the install rule, all the commands are classified as normal until the first category line. If you don't use any category lines, all the commands are classified as normal. 

These are the category lines for uninstall: 

             $(PRE_UNINSTALL)     # Pre-uninstall commands follow.
             $(POST_UNINSTALL)    # Post-uninstall commands follow.
             $(NORMAL_UNINSTALL)  # Normal commands follow.

Typically, a pre-uninstall command would be used for deleting entries from the Info directory. 

If the install or uninstall target has any dependencies which act as subroutines of installation, then you should start each dependency's commands with a category line, and start the main target's commands with a category line also. This way, you can ensure that each command is placed in the right category regardless of which of the dependencies actually run. 

Pre-installation and post-installation commands should not run any programs except for these: 

     [ basename bash cat chgrp chmod chown cmp cp dd diff echo
     egrep expand expr false fgrep find getopt grep gunzip gzip
     hostname install install-info kill ldconfig ln ls md5sum
     mkdir mkfifo mknod mv printenv pwd rm rmdir sed sort tee
     test touch true uname xargs yes

The reason for distinguishing the commands in this way is for the sake of making binary packages. Typically a binary package contains all the executables and other files that need to be installed, and has its own method of installing them so it does not need to run the normal installation commands. But installing the binary package does need to execute the pre-installation and post-installation commands. 

Programs to build binary packages work by extracting the pre-installation and post-installation commands. Here is one way of extracting the pre-installation commands (the -s option to make is needed to silence messages about entering subdirectories): 

     make -s -n install -o all \
           PRE_INSTALL=pre-install \
           POST_INSTALL=post-install \
           NORMAL_INSTALL=normal-install \
       | gawk -f pre-install.awk

where the file pre-install.awk could contain this: 

     $0 ~ /^(normal-install|post-install)[ \t]*$/ {on = 0}
     on {print $0}
     $0 ~ /^pre-install[ \t]*$/ {on = 1}



--------------------------------------------------------------------------------
Next: Error Messages, Previous: Makefile Conventions, Up: Top 
Appendix A Quick Reference
This appendix summarizes the directives, text manipulation functions, and special variables which GNU make understands. See Special Targets, Catalogue of Implicit Rules, and Summary of Options, for other summaries. 

Here is a summary of the directives GNU make recognizes: 

define variable
endef
Define a multi-line, recursively-expanded variable.
See Sequences. 

ifdef variable
ifndef variable
ifeq (a,b)
ifeq "a" "b"
ifeq 'a' 'b'
ifneq (a,b)
ifneq "a" "b"
ifneq 'a' 'b'
else
endif
Conditionally evaluate part of the makefile.
See Conditionals. 

include file
-include file
sinclude file
Include another makefile.
See Including Other Makefiles. 

override variable = value
override variable := value
override variable += value
override variable ?= value
override define variable
endef
Define a variable, overriding any previous definition, even one from the command line.
See The override Directive. 

export
Tell make to export all variables to child processes by default.
See Communicating Variables to a Sub-make. 

export variable
export variable = value
export variable := value
export variable += value
export variable ?= value
unexport variable
Tell make whether or not to export a particular variable to child processes.
See Communicating Variables to a Sub-make. 

vpath pattern path
Specify a search path for files matching a `%' pattern.
See The vpath Directive. 

vpath pattern
Remove all search paths previously specified for pattern. 

vpath
Remove all search paths previously specified in any vpath directive. 
Here is a summary of the built-in functions (see Functions): 

$(subst from,to,text)
Replace from with to in text.
See Functions for String Substitution and Analysis. 

$(patsubst pattern,replacement,text)
Replace words matching pattern with replacement in text.
See Functions for String Substitution and Analysis. 

$(strip string)
Remove excess whitespace characters from string.
See Functions for String Substitution and Analysis. 

$(findstring find,text)
Locate find in text.
See Functions for String Substitution and Analysis. 

$(filter pattern...,text)
Select words in text that match one of the pattern words.
See Functions for String Substitution and Analysis. 

$(filter-out pattern...,text)
Select words in text that do not match any of the pattern words.
See Functions for String Substitution and Analysis. 

$(sort list)
Sort the words in list lexicographically, removing duplicates.
See Functions for String Substitution and Analysis. 

$(word n,text)
Extract the nth word (one-origin) of text.
See Functions for String Substitution and Analysis. 

$(words text)
Count the number of words in text.
See Functions for String Substitution and Analysis. 

$(wordlist s,e,text)
Returns the list of words in text from s to e.
See Functions for String Substitution and Analysis. 

$(firstword names...)
Extract the first word of names.
See Functions for String Substitution and Analysis. 

$(lastword names...)
Extract the last word of names.
See Functions for String Substitution and Analysis. 

$(dir names...)
Extract the directory part of each file name.
See Functions for File Names. 

$(notdir names...)
Extract the non-directory part of each file name.
See Functions for File Names. 

$(suffix names...)
Extract the suffix (the last `.' and following characters) of each file name.
See Functions for File Names. 

$(basename names...)
Extract the base name (name without suffix) of each file name.
See Functions for File Names. 

$(addsuffix suffix,names...)
Append suffix to each word in names.
See Functions for File Names. 

$(addprefix prefix,names...)
Prepend prefix to each word in names.
See Functions for File Names. 

$(join list1,list2)
Join two parallel lists of words.
See Functions for File Names. 

$(wildcard pattern...)
Find file names matching a shell file name pattern (not a `%' pattern).
See The Function wildcard. 

$(realpath names...)
For each file name in names, expand to an absolute name that does not contain any ., .., nor symlinks.
See Functions for File Names. 

$(abspath names...)
For each file name in names, expand to an absolute name that does not contain any . or .. components, but preserves symlinks.
See Functions for File Names. 

$(error text...)
When this function is evaluated, make generates a fatal error with the message text.
See Functions That Control Make. 

$(warning text...)
When this function is evaluated, make generates a warning with the message text.
See Functions That Control Make. 

$(shell command)
Execute a shell command and return its output.
See The shell Function. 

$(origin variable)
Return a string describing how the make variable variable was defined.
See The origin Function. 

$(flavor variable)
Return a string describing the flavor of the make variable variable.
See The flavor Function. 

$(foreach var,words,text)
Evaluate text with var bound to each word in words, and concatenate the results.
See The foreach Function. 

$(call var,param,...)
Evaluate the variable var replacing any references to $(1), $(2) with the first, second, etc. param values.
See The call Function. 

$(eval text)
Evaluate text then read the results as makefile commands. Expands to the empty string.
See The eval Function. 

$(value var)
Evaluates to the contents of the variable var, with no expansion performed on it.
See The value Function. 
Here is a summary of the automatic variables. See Automatic Variables, for full information. 

$@
The file name of the target. 

$%
The target member name, when the target is an archive member. 

$<
The name of the first prerequisite. 

$?
The names of all the prerequisites that are newer than the target, with spaces between them. For prerequisites which are archive members, only the member named is used (see Archives). 

$^
$+
The names of all the prerequisites, with spaces between them. For prerequisites which are archive members, only the member named is used (see Archives). The value of $^ omits duplicate prerequisites, while $+ retains them and preserves their order. 

$*
The stem with which an implicit rule matches (see How Patterns Match). 

$(@D)
$(@F)
The directory part and the file-within-directory part of $@. 

$(*D)
$(*F)
The directory part and the file-within-directory part of $*. 

$(%D)
$(%F)
The directory part and the file-within-directory part of $%. 

$(<D)
$(<F)
The directory part and the file-within-directory part of $<. 

$(^D)
$(^F)
The directory part and the file-within-directory part of $^. 

$(+D)
$(+F)
The directory part and the file-within-directory part of $+. 

$(?D)
$(?F)
The directory part and the file-within-directory part of $?. 
These variables are used specially by GNU make: 

MAKEFILES
Makefiles to be read on every invocation of make.
See The Variable MAKEFILES. 

VPATH
Directory search path for files not found in the current directory.
See VPATH Search Path for All Prerequisites. 

SHELL
The name of the system default command interpreter, usually /bin/sh. You can set SHELL in the makefile to change the shell used to run commands. See Command Execution. The SHELL variable is handled specially when importing from and exporting to the environment. See Choosing the Shell. 

MAKESHELL
On MS-DOS only, the name of the command interpreter that is to be used by make. This value takes precedence over the value of SHELL. See MAKESHELL variable. 

MAKE
The name with which make was invoked. Using this variable in commands has special meaning. See How the MAKE Variable Works. 

MAKELEVEL
The number of levels of recursion (sub-makes).
See Variables/Recursion. 

MAKEFLAGS
The flags given to make. You can set this in the environment or a makefile to set flags.
See Communicating Options to a Sub-make. 
It is never appropriate to use MAKEFLAGS directly on a command line: its contents may not be quoted correctly for use in the shell. Always allow recursive make's to obtain these values through the environment from its parent. 


MAKECMDGOALS
The targets given to make on the command line. Setting this variable has no effect on the operation of make.
See Arguments to Specify the Goals. 

CURDIR
Set to the pathname of the current working directory (after all -C options are processed, if any). Setting this variable has no effect on the operation of make.
See Recursive Use of make. 

SUFFIXES
The default list of suffixes before make reads any makefiles. 

.LIBPATTERNS
Defines the naming of the libraries make searches for, and their order.
See Directory Search for Link Libraries. 


--------------------------------------------------------------------------------
Next: Complex Makefile, Previous: Quick Reference, Up: Top 
Appendix B Errors Generated by Make
Here is a list of the more common errors you might see generated by make, and some information about what they mean and how to fix them. 

Sometimes make errors are not fatal, especially in the presence of a - prefix on a command script line, or the -k command line option. Errors that are fatal are prefixed with the string ***. 

Error messages are all either prefixed with the name of the program (usually `make'), or, if the error is found in a makefile, the name of the file and linenumber containing the problem. 

In the table below, these common prefixes are left off. 

`[foo] Error NN'
`[foo] signal description'
These errors are not really make errors at all. They mean that a program that make invoked as part of a command script returned a non-0 error code (`Error NN'), which make interprets as failure, or it exited in some other abnormal fashion (with a signal of some type). See Errors in Commands. 
If no *** is attached to the message, then the subprocess failed but the rule in the makefile was prefixed with the - special character, so make ignored the error. 


`missing separator. Stop.'
`missing separator (did you mean TAB instead of 8 spaces?). Stop.'
This means that make could not understand much of anything about the command line it just read. GNU make looks for various kinds of separators (:, =, TAB characters, etc.) to help it decide what kind of commandline it's seeing. This means it couldn't find a valid one. 
One of the most common reasons for this message is that you (or perhaps your oh-so-helpful editor, as is the case with many MS-Windows editors) have attempted to indent your command scripts with spaces instead of a TAB character. In this case, make will use the second form of the error above. Remember that every line in the command script must begin with a TAB character. Eight spaces do not count. See Rule Syntax. 


`commands commence before first target. Stop.'
`missing rule before commands. Stop.'
This means the first thing in the makefile seems to be part of a command script: it begins with a TAB character and doesn't appear to be a legal make command (such as a variable assignment). Command scripts must always be associated with a target. 
The second form is generated if the line has a semicolon as the first non-whitespace character; make interprets this to mean you left out the "target: prerequisite" section of a rule. See Rule Syntax. 


`No rule to make target `xxx'.'
`No rule to make target `xxx', needed by `yyy'.'
This means that make decided it needed to build a target, but then couldn't find any instructions in the makefile on how to do that, either explicit or implicit (including in the default rules database). 
If you want that file to be built, you will need to add a rule to your makefile describing how that target can be built. Other possible sources of this problem are typos in the makefile (if that filename is wrong) or a corrupted source tree (if that file is not supposed to be built, but rather only a prerequisite). 


`No targets specified and no makefile found. Stop.'
`No targets. Stop.'
The former means that you didn't provide any targets to be built on the command line, and make couldn't find any makefiles to read in. The latter means that some makefile was found, but it didn't contain any default goal and none was given on the command line. GNU make has nothing to do in these situations. See Arguments to Specify the Makefile. 

`Makefile `xxx' was not found.'
`Included makefile `xxx' was not found.'
A makefile specified on the command line (first form) or included (second form) was not found. 

`warning: overriding commands for target `xxx''
`warning: ignoring old commands for target `xxx''
GNU make allows commands to be specified only once per target (except for double-colon rules). If you give commands for a target which already has been defined to have commands, this warning is issued and the second set of commands will overwrite the first set. See Multiple Rules for One Target. 

`Circular xxx <- yyy dependency dropped.'
This means that make detected a loop in the dependency graph: after tracing the prerequisite yyy of target xxx, and its prerequisites, etc., one of them depended on xxx again. 

`Recursive variable `xxx' references itself (eventually). Stop.'
This means you've defined a normal (recursive) make variable xxx that, when it's expanded, will refer to itself (xxx). This is not allowed; either use simply-expanded variables (:=) or use the append operator (+=). See How to Use Variables. 

`Unterminated variable reference. Stop.'
This means you forgot to provide the proper closing parenthesis or brace in your variable or function reference. 

`insufficient arguments to function `xxx'. Stop.'
This means you haven't provided the requisite number of arguments for this function. See the documentation of the function for a description of its arguments. See Functions for Transforming Text. 

`missing target pattern. Stop.'
`multiple target patterns. Stop.'
`target pattern contains no `%'. Stop.'
`mixed implicit and static pattern rules. Stop.'
These are generated for malformed static pattern rules. The first means there's no pattern in the target section of the rule; the second means there are multiple patterns in the target section; the third means the target doesn't contain a pattern character (%); and the fourth means that all three parts of the static pattern rule contain pattern characters (%) only the first two parts should. See Syntax of Static Pattern Rules. 

`warning: -jN forced in submake: disabling jobserver mode.'
This warning and the next are generated if make detects error conditions related to parallel processing on systems where sub-makes can communicate (see Communicating Options to a Sub-make). This warning is generated if a recursive invocation of a make process is forced to have `-jN' in its argument list (where N is greater than one). This could happen, for example, if you set the MAKE environment variable to `make -j2'. In this case, the sub-make doesn't communicate with other make processes and will simply pretend it has two jobs of its own. 

`warning: jobserver unavailable: using -j1. Add `+' to parent make rule.'
In order for make processes to communicate, the parent will pass information to the child. Since this could result in problems if the child process isn't actually a make, the parent will only do this if it thinks the child is a make. The parent uses the normal algorithms to determine this (see How the MAKE Variable Works). If the makefile is constructed such that the parent doesn't know the child is a make process, then the child will receive only part of the information necessary. In this case, the child will generate this warning message and proceed with its build in a sequential manner. 


--------------------------------------------------------------------------------
Next: GNU Free Documentation License, Previous: Error Messages, Up: Top 
Appendix C Complex Makefile Example
Here is the makefile for the GNU tar program. This is a moderately complex makefile. 

Because it is the first target, the default goal is `all'. An interesting feature of this makefile is that testpad.h is a source file automatically created by the testpad program, itself compiled from testpad.c. 

If you type `make' or `make all', then make creates the tar executable, the rmt daemon that provides remote tape access, and the tar.info Info file. 

If you type `make install', then make not only creates tar, rmt, and tar.info, but also installs them. 

If you type `make clean', then make removes the `.o' files, and the tar, rmt, testpad, testpad.h, and core files. 

If you type `make distclean', then make not only removes the same files as does `make clean' but also the TAGS, Makefile, and config.status files. (Although it is not evident, this makefile (and config.status) is generated by the user with the configure program, which is provided in the tar distribution, but is not shown here.) 

If you type `make realclean', then make removes the same files as does `make distclean' and also removes the Info files generated from tar.texinfo. 

In addition, there are targets shar and dist that create distribution kits. 

     # Generated automatically from Makefile.in by configure.
     # Un*x Makefile for GNU tar program.
     # Copyright (C) 1991 Free Software Foundation, Inc.
     
     # This program is free software; you can redistribute
     # it and/or modify it under the terms of the GNU
     # General Public License ...
     ...
     ...
     
     SHELL = /bin/sh
     
     #### Start of system configuration section. ####
     
     srcdir = .
     
     # If you use gcc, you should either run the
     # fixincludes script that comes with it or else use
     # gcc with the -traditional option.  Otherwise ioctl
     # calls will be compiled incorrectly on some systems.
     CC = gcc -O
     YACC = bison -y
     INSTALL = /usr/local/bin/install -c
     INSTALLDATA = /usr/local/bin/install -c -m 644
     
     # Things you might add to DEFS:
     # -DSTDC_HEADERS        If you have ANSI C headers and
     #                       libraries.
     # -DPOSIX               If you have POSIX.1 headers and
     #                       libraries.
     # -DBSD42               If you have sys/dir.h (unless
     #                       you use -DPOSIX), sys/file.h,
     #                       and st_blocks in `struct stat'.
     # -DUSG                 If you have System V/ANSI C
     #                       string and memory functions
     #                       and headers, sys/sysmacros.h,
     #                       fcntl.h, getcwd, no valloc,
     #                       and ndir.h (unless
     #                       you use -DDIRENT).
     # -DNO_MEMORY_H         If USG or STDC_HEADERS but do not
     #                       include memory.h.
     # -DDIRENT              If USG and you have dirent.h
     #                       instead of ndir.h.
     # -DSIGTYPE=int         If your signal handlers
     #                       return int, not void.
     # -DNO_MTIO             If you lack sys/mtio.h
     #                       (magtape ioctls).
     # -DNO_REMOTE           If you do not have a remote shell
     #                       or rexec.
     # -DUSE_REXEC           To use rexec for remote tape
     #                       operations instead of
     #                       forking rsh or remsh.
     # -DVPRINTF_MISSING     If you lack vprintf function
     #                       (but have _doprnt).
     # -DDOPRNT_MISSING      If you lack _doprnt function.
     #                       Also need to define
     #                       -DVPRINTF_MISSING.
     # -DFTIME_MISSING       If you lack ftime system call.
     # -DSTRSTR_MISSING      If you lack strstr function.
     # -DVALLOC_MISSING      If you lack valloc function.
     # -DMKDIR_MISSING       If you lack mkdir and
     #                       rmdir system calls.
     # -DRENAME_MISSING      If you lack rename system call.
     # -DFTRUNCATE_MISSING   If you lack ftruncate
     #                       system call.
     # -DV7                  On Version 7 Unix (not
     #                       tested in a long time).
     # -DEMUL_OPEN3          If you lack a 3-argument version
     #                       of open, and want to emulate it
     #                       with system calls you do have.
     # -DNO_OPEN3            If you lack the 3-argument open
     #                       and want to disable the tar -k
     #                       option instead of emulating open.
     # -DXENIX               If you have sys/inode.h
     #                       and need it 94 to be included.
     
     DEFS =  -DSIGTYPE=int -DDIRENT -DSTRSTR_MISSING \
             -DVPRINTF_MISSING -DBSD42
     # Set this to rtapelib.o unless you defined NO_REMOTE,
     # in which case make it empty.
     RTAPELIB = rtapelib.o
     LIBS =
     DEF_AR_FILE = /dev/rmt8
     DEFBLOCKING = 20
     
     CDEBUG = -g
     CFLAGS = $(CDEBUG) -I. -I$(srcdir) $(DEFS) \
             -DDEF_AR_FILE=\"$(DEF_AR_FILE)\" \
             -DDEFBLOCKING=$(DEFBLOCKING)
     LDFLAGS = -g
     
     prefix = /usr/local
     # Prefix for each installed program,
     # normally empty or `g'.
     binprefix =
     
     # The directory to install tar in.
     bindir = $(prefix)/bin
     
     # The directory to install the info files in.
     infodir = $(prefix)/info
     
     #### End of system configuration section. ####
     
     SRC1 =  tar.c create.c extract.c buffer.c \
             getoldopt.c update.c gnu.c mangle.c
     SRC2 =  version.c list.c names.c diffarch.c \
             port.c wildmat.c getopt.c
     SRC3 =  getopt1.c regex.c getdate.y
     SRCS =  $(SRC1) $(SRC2) $(SRC3)
     OBJ1 =  tar.o create.o extract.o buffer.o \
             getoldopt.o update.o gnu.o mangle.o
     OBJ2 =  version.o list.o names.o diffarch.o \
             port.o wildmat.o getopt.o
     OBJ3 =  getopt1.o regex.o getdate.o $(RTAPELIB)
     OBJS =  $(OBJ1) $(OBJ2) $(OBJ3)
     AUX =   README COPYING ChangeLog Makefile.in  \
             makefile.pc configure configure.in \
             tar.texinfo tar.info* texinfo.tex \
             tar.h port.h open3.h getopt.h regex.h \
             rmt.h rmt.c rtapelib.c alloca.c \
             msd_dir.h msd_dir.c tcexparg.c \
             level-0 level-1 backup-specs testpad.c
     
     .PHONY: all
     all:    tar rmt tar.info
     
     .PHONY: tar
     tar:    $(OBJS)
             $(CC) $(LDFLAGS) -o $@ $(OBJS) $(LIBS)
     
     rmt:    rmt.c
             $(CC) $(CFLAGS) $(LDFLAGS) -o $@ rmt.c
     
     tar.info: tar.texinfo
             makeinfo tar.texinfo
     
     .PHONY: install
     install: all
             $(INSTALL) tar $(bindir)/$(binprefix)tar
             -test ! -f rmt || $(INSTALL) rmt /etc/rmt
             $(INSTALLDATA) $(srcdir)/tar.info* $(infodir)
     
     $(OBJS): tar.h port.h testpad.h
     regex.o buffer.o tar.o: regex.h
     # getdate.y has 8 shift/reduce conflicts.
     
     testpad.h: testpad
             ./testpad
     
     testpad: testpad.o
             $(CC) -o $@ testpad.o
     
     TAGS:   $(SRCS)
             etags $(SRCS)
     
     .PHONY: clean
     clean:
             rm -f *.o tar rmt testpad testpad.h core
     
     .PHONY: distclean
     distclean: clean
             rm -f TAGS Makefile config.status
     
     .PHONY: realclean
     realclean: distclean
             rm -f tar.info*
     
     .PHONY: shar
     shar: $(SRCS) $(AUX)
             shar $(SRCS) $(AUX) | compress \
               > tar-`sed -e '/version_string/!d' \
                          -e 's/[^0-9.]*\([0-9.]*\).*/\1/' \
                          -e q
                          version.c`.shar.Z
     
     .PHONY: dist
     dist: $(SRCS) $(AUX)
             echo tar-`sed \
                  -e '/version_string/!d' \
                  -e 's/[^0-9.]*\([0-9.]*\).*/\1/' \
                  -e q
                  version.c` > .fname
             -rm -rf `cat .fname`
             mkdir `cat .fname`
             ln $(SRCS) $(AUX) `cat .fname`
             tar chZf `cat .fname`.tar.Z `cat .fname`
             -rm -rf `cat .fname` .fname
     
     tar.zoo: $(SRCS) $(AUX)
             -rm -rf tmp.dir
             -mkdir tmp.dir
             -rm tar.zoo
             for X in $(SRCS) $(AUX) ; do \
                 echo $$X ; \
                 sed 's/$$/^M/' $$X \
                 > tmp.dir/$$X ; done
             cd tmp.dir ; zoo aM ../tar.zoo *
             -rm -rf tmp.dir



--------------------------------------------------------------------------------
Next: Concept Index, Previous: Complex Makefile, Up: Top 
Appendix D GNU Free Documentation License

Version 1.2, November 2002
     Copyright   2000,2001,2002 Free Software Foundation, Inc.
     51 Franklin St, Fifth Floor, Boston, MA  02110-1301, USA
     
     Everyone is permitted to copy and distribute verbatim copies
     of this license document, but changing it is not allowed.

PREAMBLE 
The purpose of this License is to make a manual, textbook, or other functional and useful document free in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others. 

This License is a kind of  copyleft , which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software. 

We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference. 

APPLICABILITY AND DEFINITIONS 
This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The  Document , below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as  you . You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law. 

A  Modified Version  of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language. 

A  Secondary Section  is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them. 

The  Invariant Sections  are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none. 

The  Cover Texts  are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words. 

A  Transparent  copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not  Transparent  is called  Opaque . 

Examples of suitable formats for Transparent copies include plain ascii without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only. 

The  Title Page  means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such,  Title Page  means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text. 

A section  Entitled XYZ  means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as  Acknowledgements ,  Dedications ,  Endorsements , or  History .) To  Preserve the Title  of such a section when you modify the Document means that it remains a section  Entitled XYZ  according to this definition. 

The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License. 

VERBATIM COPYING 
You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3. 

You may also lend copies, under the same conditions stated above, and you may publicly display copies. 

COPYING IN QUANTITY 
If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects. 

If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages. 

If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public. 

It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document. 

MODIFICATIONS 
You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version: 

Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission. 
List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement. 
State on the Title page the name of the publisher of the Modified Version, as the publisher. 
Preserve all the copyright notices of the Document. 
Add an appropriate copyright notice for your modifications adjacent to the other copyright notices. 
Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below. 
Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice. 
Include an unaltered copy of this License. 
Preserve the section Entitled  History , Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled  History  in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence. 
Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the  History  section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission. 
For any section Entitled  Acknowledgements  or  Dedications , Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein. 
Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles. 
Delete any section Entitled  Endorsements . Such a section may not be included in the Modified Version. 
Do not retitle any existing section to be Entitled  Endorsements  or to conflict in title with any Invariant Section. 
Preserve any Warranty Disclaimers. 
If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles. 

You may add a section Entitled  Endorsements , provided it contains nothing but endorsements of your Modified Version by various parties for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard. 

You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one. 

The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version. 

COMBINING DOCUMENTS 
You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers. 

The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work. 

In the combination, you must combine any sections Entitled  History  in the various original documents, forming one section Entitled  History ; likewise combine any sections Entitled  Acknowledgements , and any sections Entitled  Dedications . You must delete all sections Entitled  Endorsements.  

COLLECTIONS OF DOCUMENTS 
You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects. 

You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document. 

AGGREGATION WITH INDEPENDENT WORKS 
A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an  aggregate  if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document. 

If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate. 

TRANSLATION 
Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail. 

If a section in the Document is Entitled  Acknowledgements ,  Dedications , or  History , the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title. 

TERMINATION 
You may not copy, modify, sublicense, or distribute the Document except as expressly provided for under this License. Any other attempt to copy, modify, sublicense or distribute the Document is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. 

FUTURE REVISIONS OF THIS LICENSE 
The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See http://www.gnu.org/copyleft/. 

Each version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License  or any later version  applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. 

D.1 ADDENDUM: How to use this License for your documents
To use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page: 

       Copyright (C)  year  your name.
       Permission is granted to copy, distribute and/or modify this document
       under the terms of the GNU Free Documentation License, Version 1.2
       or any later version published by the Free Software Foundation;
       with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
       Texts.  A copy of the license is included in the section entitled ``GNU
       Free Documentation License''.

If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts, replace the  with...Texts.  line with this: 

         with the Invariant Sections being list their titles, with
         the Front-Cover Texts being list, and with the Back-Cover Texts
         being list.

If you have Invariant Sections without Cover Texts, or some other combination of the three, merge those two alternatives to suit the situation. 

If your document contains nontrivial examples of program code, we recommend releasing these examples in parallel under your choice of free software license, such as the GNU General Public License, to permit their use in free software. 

Automake is a tool for automatically generating Makefile.ins from files called Makefile.am. Each Makefile.am is basically a series of make variable definitions1, with rules being thrown in occasionally. The generated Makefile.ins are compliant with the GNU Makefile standards. 

The GNU Makefile Standards Document (see Makefile Conventions) is long, complicated, and subject to change. The goal of Automake is to remove the burden of Makefile maintenance from the back of the individual GNU maintainer (and put it on the back of the Automake maintainer). 

The typical Automake input file is simply a series of variable definitions. Each such file is processed to create a Makefile.in. There should generally be one Makefile.am per directory of a project. 

Automake does constrain a project in certain ways; for instance it assumes that the project uses Autoconf (see Introduction), and enforces certain restrictions on the configure.in contents2. 

Automake requires perl in order to generate the Makefile.ins. However, the distributions created by Automake are fully GNU standards-compliant, and do not require perl in order to be built. 

Mail suggestions and bug reports for Automake to bug-automake@gnu.org. 



--------------------------------------------------------------------------------
Node: Generalities, Next: Examples, Previous: Introduction, Up: Top 

General ideas
The following sections cover a few basic ideas that will help you understand how Automake works. 

General Operation: General operation of Automake 
Strictness: Standards conformance checking 
Uniform: The Uniform Naming Scheme 
Canonicalization: How derived variables are named 
User Variables: Variables reserved for the user 
Auxiliary Programs: Programs automake might require 


--------------------------------------------------------------------------------
Node: General Operation, Next: Strictness, Previous: Generalities, Up: Generalities 

General Operation
Automake works by reading a Makefile.am and generating a Makefile.in. Certain variables and targets defined in the Makefile.am instruct Automake to generate more specialized code; for instance, a bin_PROGRAMS variable definition will cause targets for compiling and linking programs to be generated. 

The variable definitions and targets in the Makefile.am are copied verbatim into the generated file. This allows you to add arbitrary code into the generated Makefile.in. For instance the Automake distribution includes a non-standard cvs-dist target, which the Automake maintainer uses to make distributions from his source control system. 

Note that most GNU make extensions are not recognized by Automake. Using such extensions in a Makefile.am will lead to errors or confusing behavior. 

A special exception is that the GNU make append operator, +=, is supported. This operator appends its right hand argument to the variable specified on the left. Automake will translate the operator into an ordinary = operator; += will thus work with any make program. 

Automake tries to keep comments grouped with any adjoining targets or variable definitions. 

A target defined in Makefile.am generally overrides any such target of a similar name that would be automatically generated by automake. Although this is a supported feature, it is generally best to avoid making use of it, as sometimes the generated rules are very particular. 

Similarly, a variable defined in Makefile.am or AC_SUBST'ed from configure.in will override any definition of the variable that automake would ordinarily create. This feature is more often useful than the ability to override a target definition. Be warned that many of the variables generated by automake are considered to be for internal use only, and their names might change in future releases. 

When examining a variable definition, Automake will recursively examine variables referenced in the definition. For example, if Automake is looking at the content of foo_SOURCES in this snippet 

     xs = a.c b.c
     foo_SOURCES = c.c $(xs)
     
it would use the files a.c, b.c, and c.c as the contents of foo_SOURCES. 

Automake also allows a form of comment which is not copied into the output; all lines beginning with ## (leading spaces allowed) are completely ignored by Automake. 

It is customary to make the first line of Makefile.am read: 

     ## Process this file with automake to produce Makefile.in
     


--------------------------------------------------------------------------------
Node: Strictness, Next: Uniform, Previous: General Operation, Up: Generalities 

Strictness
While Automake is intended to be used by maintainers of GNU packages, it does make some effort to accommodate those who wish to use it, but do not want to use all the GNU conventions. 

To this end, Automake supports three levels of strictness--the strictness indicating how stringently Automake should check standards conformance. 

The valid strictness levels are: 

foreign 
Automake will check for only those things which are absolutely required for proper operations. For instance, whereas GNU standards dictate the existence of a NEWS file, it will not be required in this mode. The name comes from the fact that Automake is intended to be used for GNU programs; these relaxed rules are not the standard mode of operation. 

gnu 
Automake will check--as much as possible--for compliance to the GNU standards for packages. This is the default. 

gnits 
Automake will check for compliance to the as-yet-unwritten Gnits standards. These are based on the GNU standards, but are even more detailed. Unless you are a Gnits standards contributor, it is recommended that you avoid this option until such time as the Gnits standard is actually published (which may never happen). 
For more information on the precise implications of the strictness level, see Gnits. 

Automake also has a special "cygnus" mode which is similar to strictness but handled differently. This mode is useful for packages which are put into a "Cygnus" style tree (e.g., the GCC tree). For more information on this mode, see Cygnus. 



--------------------------------------------------------------------------------
Node: Uniform, Next: Canonicalization, Previous: Strictness, Up: Generalities 

The Uniform Naming Scheme
Automake variables generally follow a uniform naming scheme that makes it easy to decide how programs (and other derived objects) are built, and how they are installed. This scheme also supports configure time determination of what should be built. 

At make time, certain variables are used to determine which objects are to be built. The variable names are made of several pieces which are concatenated together. 

The piece which tells automake what is being built is commonly called the primary. For instance, the primary PROGRAMS holds a list of programs which are to be compiled and linked. 

A different set of names is used to decide where the built objects should be installed. These names are prefixes to the primary which indicate which standard directory should be used as the installation directory. The standard directory names are given in the GNU standards (see Directory Variables). Automake extends this list with pkglibdir, pkgincludedir, and pkgdatadir; these are the same as the non-pkg versions, but with @PACKAGE@ appended. For instance, pkglibdir is defined as $(libdir)/@PACKAGE@. 

For each primary, there is one additional variable named by prepending EXTRA_ to the primary name. This variable is used to list objects which may or may not be built, depending on what configure decides. This variable is required because Automake must statically know the entire list of objects that may be built in order to generate a Makefile.in that will work in all cases. 

For instance, cpio decides at configure time which programs are built. Some of the programs are installed in bindir, and some are installed in sbindir: 

     EXTRA_PROGRAMS = mt rmt
     bin_PROGRAMS = cpio pax
     sbin_PROGRAMS = @MORE_PROGRAMS@
     
Defining a primary without a prefix as a variable, e.g., PROGRAMS, is an error. 

Note that the common dir suffix is left off when constructing the variable names; thus one writes bin_PROGRAMS and not bindir_PROGRAMS. 

Not every sort of object can be installed in every directory. Automake will flag those attempts it finds in error. Automake will also diagnose obvious misspellings in directory names. 

Sometimes the standard directories--even as augmented by Automake-- are not enough. In particular it is sometimes useful, for clarity, to install objects in a subdirectory of some predefined directory. To this end, Automake allows you to extend the list of possible installation directories. A given prefix (e.g. zar) is valid if a variable of the same name with dir appended is defined (e.g. zardir). 

For instance, until HTML support is part of Automake, you could use this to install raw HTML documentation: 

     htmldir = $(prefix)/html
     html_DATA = automake.html
     
The special prefix noinst indicates that the objects in question should be built but not installed at all. This is usually used for objects required to build the rest of your package, for instance static libraries (see A Library), or helper scripts. 

The special prefix check indicates that the objects in question should not be built until the make check command is run. Those objects are not installed either. 

The current primary names are PROGRAMS, LIBRARIES, LISP, PYTHON, JAVA, SCRIPTS, DATA, HEADERS, MANS, and TEXINFOS. 

Some primaries also allow additional prefixes which control other aspects of automake's behavior. The currently defined prefixes are dist_, nodist_, and nobase_. These prefixes are explained later (see Program and Library Variables). 



--------------------------------------------------------------------------------
Node: Canonicalization, Next: User Variables, Previous: Uniform, Up: Generalities 

How derived variables are named
Sometimes a Makefile variable name is derived from some text the maintainer supplies. For instance, a program name listed in _PROGRAMS is rewritten into the name of a _SOURCES variable. In cases like this, Automake canonicalizes the text, so that program names and the like do not have to follow Makefile variable naming rules. All characters in the name except for letters, numbers, the strudel (@), and the underscore are turned into underscores when making variable references. 

For example, if your program is named sniff-glue, the derived variable name would be sniff_glue_SOURCES, not sniff-glue_SOURCES. Similarly the sources for a library named libmumble++.a should be listed in the libmumble___a_SOURCES variable. 

The strudel is an addition, to make the use of Autoconf substitutions in variable names less obfuscating. 



--------------------------------------------------------------------------------
Node: User Variables, Next: Auxiliary Programs, Previous: Canonicalization, Up: Generalities 

Variables reserved for the user
Some Makefile variables are reserved by the GNU Coding Standards for the use of the "user" - the person building the package. For instance, CFLAGS is one such variable. 

Sometimes package developers are tempted to set user variables such as CFLAGS because it appears to make their job easier - they don't have to introduce a second variable into every target. 

However, the package itself should never set a user variable, particularly not to include switches which are required for proper compilation of the package. Since these variables are documented as being for the package builder, that person rightfully expects to be able to override any of these variables at build time. 

To get around this problem, automake introduces an automake-specific shadow variable for each user flag variable. (Shadow variables are not introduced for variables like CC, where they would make no sense.) The shadow variable is named by prepending AM_ to the user variable's name. For instance, the shadow variable for YFLAGS is AM_YFLAGS. 



--------------------------------------------------------------------------------
Node: Auxiliary Programs, Previous: User Variables, Up: Generalities 

Programs automake might require
Automake sometimes requires helper programs so that the generated Makefile can do its work properly. There are a fairly large number of them, and we list them here. 

ansi2knr.c 
ansi2knr.1 
These two files are used by the automatic de-ANSI-fication support (see ANSI). 

compile 
This is a wrapper for compilers which don't accept both -c and -o at the same time. It is only used when absolutely required. Such compilers are rare. 

config.guess 
config.sub 
These programs compute the canonical triplets for the given build, host, or target architecture. These programs are updated regularly to support new architectures and fix probes broken by changes in new kernel versions. You are encouraged to fetch the latest versions of these files from <ftp://ftp.gnu.org/gnu/config/> before making a release. 

depcomp 
This program understands how to run a compiler so that it will generate not only the desired output but also dependency information which is then used by the automatic dependency tracking feature. 

elisp-comp 
This program is used to byte-compile Emacs Lisp code. 

install-sh 
This is a replacement for the install program which works on platforms where install is unavailable or unusable. 

mdate-sh 
This script is used to generate a version.texi file. It examines a file and prints some date information about it. 

missing 
This wraps a number of programs which are typically only required by maintainers. If the program in question doesn't exist, missing prints an informative warning and attempts to fix things so that the build can continue. 

mkinstalldirs 
This works around the fact that mkdir -p is not portable. 

py-compile 
This is used to byte-compile Python scripts. 

texinfo.tex 
Not a program, this file is required for make dvi, make ps and make pdf to work when Texinfo sources are in the package. 

ylwrap 
This program wraps lex and yacc and ensures that, for instance, multiple yacc instances can be invoked in a single directory in parallel. 


--------------------------------------------------------------------------------
Node: Examples, Next: Invoking Automake, Previous: Generalities, Up: Top 

Some example packages
Complete: A simple example, start to finish 
Hello: A classic program 
true: Building true and false 


--------------------------------------------------------------------------------
Node: Complete, Next: Hello, Previous: Examples, Up: Examples 

A simple example, start to finish
Let's suppose you just finished writing zardoz, a program to make your head float from vortex to vortex. You've been using Autoconf to provide a portability framework, but your Makefile.ins have been ad-hoc. You want to make them bulletproof, so you turn to Automake. 

The first step is to update your configure.in to include the commands that automake needs. The way to do this is to add an AM_INIT_AUTOMAKE call just after AC_INIT: 

     AC_INIT(zardoz, 1.0)
     AM_INIT_AUTOMAKE
     ...
     
Since your program doesn't have any complicating factors (e.g., it doesn't use gettext, it doesn't want to build a shared library), you're done with this part. That was easy! 

Now you must regenerate configure. But to do that, you'll need to tell autoconf how to find the new macro you've used. The easiest way to do this is to use the aclocal program to generate your aclocal.m4 for you. But wait... maybe you already have an aclocal.m4, because you had to write some hairy macros for your program. The aclocal program lets you put your own macros into acinclude.m4, so simply rename and then run: 

     mv aclocal.m4 acinclude.m4
     aclocal
     autoconf
     
Now it is time to write your Makefile.am for zardoz. Since zardoz is a user program, you want to install it where the rest of the user programs go: bindir. Additionally, zardoz has some Texinfo documentation. Your configure.in script uses AC_REPLACE_FUNCS, so you need to link against $(LIBOBJS). So here's what you'd write: 

     bin_PROGRAMS = zardoz
     zardoz_SOURCES = main.c head.c float.c vortex9.c gun.c
     zardoz_LDADD = $(LIBOBJS)
     
     info_TEXINFOS = zardoz.texi
     
Now you can run automake --add-missing to generate your Makefile.in and grab any auxiliary files you might need, and you're done! 



--------------------------------------------------------------------------------
Node: Hello, Next: true, Previous: Complete, Up: Examples 

A classic program
GNU hello is renowned for its classic simplicity and versatility. This section shows how Automake could be used with the GNU Hello package. The examples below are from the latest beta version of GNU Hello, but with all of the maintainer-only code stripped out, as well as all copyright comments. 
Of course, GNU Hello is somewhat more featureful than your traditional two-liner. GNU Hello is internationalized, does option processing, and has a manual and a test suite. 

Here is the configure.in from GNU Hello. Please note: The calls to AC_INIT and AM_INIT_AUTOMAKE in this example use a deprecated syntax. For the current approach, see the description of AM_INIT_AUTOMAKE in Public macros. 

     dnl Process this file with autoconf to produce a configure script.
     AC_INIT(src/hello.c)
     AM_INIT_AUTOMAKE(hello, 1.3.11)
     AM_CONFIG_HEADER(config.h)
     
     dnl Set of available languages.
     ALL_LINGUAS="de fr es ko nl no pl pt sl sv"
     
     dnl Checks for programs.
     AC_PROG_CC
     AC_ISC_POSIX
     
     dnl Checks for libraries.
     
     dnl Checks for header files.
     AC_STDC_HEADERS
     AC_HAVE_HEADERS(string.h fcntl.h sys/file.h sys/param.h)
     
     dnl Checks for library functions.
     AC_FUNC_ALLOCA
     
     dnl Check for st_blksize in struct stat
     AC_ST_BLKSIZE
     
     dnl internationalization macros
     AM_GNU_GETTEXT
     AC_OUTPUT([Makefile doc/Makefile intl/Makefile po/Makefile.in \
                src/Makefile tests/Makefile tests/hello],
        [chmod +x tests/hello])
     
The AM_ macros are provided by Automake (or the Gettext library); the rest are standard Autoconf macros. 

The top-level Makefile.am: 

     EXTRA_DIST = BUGS ChangeLog.O
     SUBDIRS = doc intl po src tests
     
As you can see, all the work here is really done in subdirectories. 

The po and intl directories are automatically generated using gettextize; they will not be discussed here. 

In doc/Makefile.am we see: 

     info_TEXINFOS = hello.texi
     hello_TEXINFOS = gpl.texi
     
This is sufficient to build, install, and distribute the GNU Hello manual. 

Here is tests/Makefile.am: 

     TESTS = hello
     EXTRA_DIST = hello.in testdata
     
The script hello is generated by configure, and is the only test case. make check will run this test. 

Last we have src/Makefile.am, where all the real work is done: 

     bin_PROGRAMS = hello
     hello_SOURCES = hello.c version.c getopt.c getopt1.c getopt.h system.h
     hello_LDADD = @INTLLIBS@ @ALLOCA@
     localedir = $(datadir)/locale
     INCLUDES = -I../intl -DLOCALEDIR=\"$(localedir)\"
     


--------------------------------------------------------------------------------
Node: true, Previous: Hello, Up: Examples 

Building true and false
Here is another, trickier example. It shows how to generate two programs (true and false) from the same source file (true.c). The difficult part is that each compilation of true.c requires different cpp flags. 

     bin_PROGRAMS = true false
     false_SOURCES =
     false_LDADD = false.o
     
     true.o: true.c
             $(COMPILE) -DEXIT_CODE=0 -c true.c
     
     false.o: true.c
             $(COMPILE) -DEXIT_CODE=1 -o false.o -c true.c
     
Note that there is no true_SOURCES definition. Automake will implicitly assume that there is a source file named true.c, and define rules to compile true.o and link true. The true.o: true.c rule supplied by the above Makefile.am, will override the Automake generated rule to build true.o. 

false_SOURCES is defined to be empty--that way no implicit value is substituted. Because we have not listed the source of false, we have to tell Automake how to link the program. This is the purpose of the false_LDADD line. A false_DEPENDENCIES variable, holding the dependencies of the false target will be automatically generated by Automake from the content of false_LDADD. 

The above rules won't work if your compiler doesn't accept both -c and -o. The simplest fix for this is to introduce a bogus dependency (to avoid problems with a parallel make): 

     true.o: true.c false.o
             $(COMPILE) -DEXIT_CODE=0 -c true.c
     
     false.o: true.c
             $(COMPILE) -DEXIT_CODE=1 -c true.c && mv true.o false.o
     
Also, these explicit rules do not work if the de-ANSI-fication feature is used (see ANSI). Supporting de-ANSI-fication requires a little more work: 

     true._o: true._c false.o
             $(COMPILE) -DEXIT_CODE=0 -c true.c
     
     false._o: true._c
             $(COMPILE) -DEXIT_CODE=1 -c true.c && mv true._o false.o
     
As it turns out, there is also a much easier way to do this same task. Some of the above techniques are useful enough that we've kept the example in the manual. However if you were to build true and false in real life, you would probably use per-program compilation flags, like so: 

     bin_PROGRAMS = false true
     
     false_SOURCES = true.c
     false_CPPFLAGS = -DEXIT_CODE=1
     
     true_SOURCES = true.c
     true_CPPFLAGS = -DEXIT_CODE=0
     
In this case Automake will cause true.c to be compiled twice, with different flags. De-ANSI-fication will work automatically. In this instance, the names of the object files would be chosen by automake; they would be false-true.o and true-true.o. (The name of the object files rarely matters.) 



--------------------------------------------------------------------------------
Node: Invoking Automake, Next: configure, Previous: Examples, Up: Top 

Creating a Makefile.in
To create all the Makefile.ins for a package, run the automake program in the top level directory, with no arguments. automake will automatically find each appropriate Makefile.am (by scanning configure.in; see configure) and generate the corresponding Makefile.in. Note that automake has a rather simplistic view of what constitutes a package; it assumes that a package has only one configure.in, at the top. If your package has multiple configure.ins, then you must run automake in each directory holding a configure.in. (Alternatively, you may rely on Autoconf's autoreconf, which is able to recurse your package tree and run automake where appropriate.) 

You can optionally give automake an argument; .am is appended to the argument and the result is used as the name of the input file. This feature is generally only used to automatically rebuild an out-of-date Makefile.in. Note that automake must always be run from the topmost directory of a project, even if being used to regenerate the Makefile.in in some subdirectory. This is necessary because automake must scan configure.in, and because automake uses the knowledge that a Makefile.in is in a subdirectory to change its behavior in some cases. 

Automake will run autoconf to scan configure.in and its dependencies (aclocal.m4), therefore autoconf must be in your PATH. If there is an AUTOCONF variable in your environment it will be used instead of autoconf, this allows you to select a particular version of Autoconf. By the way, don't misunderstand this paragraph: Automake runs autoconf to scan your configure.in, this won't build configure and you still have to run autoconf yourself for this purpose. 

automake accepts the following options: 

-a 
--add-missing 
Automake requires certain common files to exist in certain situations; for instance config.guess is required if configure.in runs AC_CANONICAL_HOST. Automake is distributed with several of these files (see Auxiliary Programs); this option will cause the missing ones to be automatically added to the package, whenever possible. In general if Automake tells you a file is missing, try using this option. By default Automake tries to make a symbolic link pointing to its own copy of the missing file; this can be changed with --copy. 
Many of the potentially-missing files are common scripts whose location may be specified via the AC_CONFIG_AUX_DIR macro. Therefore, AC_CONFIG_AUX_DIR's setting affects whether a file is considered missing, and where the missing file is added (see Optional). 


--libdir=dir 
Look for Automake data files in directory dir instead of in the installation directory. This is typically used for debugging. 

-c 
--copy 
When used with --add-missing, causes installed files to be copied. The default is to make a symbolic link. 

--cygnus 
Causes the generated Makefile.ins to follow Cygnus rules, instead of GNU or Gnits rules. For more information, see Cygnus. 

-f 
--force-missing 
When used with --add-missing, causes standard files to be reinstalled even if they already exist in the source tree. This involves removing the file from the source tree before creating the new symlink (or, with --copy, copying the new file). 

--foreign 
Set the global strictness to foreign. For more information, see Strictness. 

--gnits 
Set the global strictness to gnits. For more information, see Gnits. 

--gnu 
Set the global strictness to gnu. For more information, see Gnits. This is the default strictness. 

--help 
Print a summary of the command line options and exit. 

-i 
--ignore-deps 
This disables the dependency tracking feature in generated Makefiles; see Dependencies. 

--include-deps 
This enables the dependency tracking feature. This feature is enabled by default. This option is provided for historical reasons only and probably should not be used. 

--no-force 
Ordinarily automake creates all Makefile.ins mentioned in configure.in. This option causes it to only update those Makefile.ins which are out of date with respect to one of their dependents. 
Due to a bug in its implementation, this option is currently ignored. It will be fixed in Automake 1.8. 


-o dir 
--output-dir=dir 
Put the generated Makefile.in in the directory dir. Ordinarily each Makefile.in is created in the directory of the corresponding Makefile.am. This option is deprecated and will be removed in a future release. 

-v 
--verbose 
Cause Automake to print information about which files are being read or created. 

--version 
Print the version number of Automake and exit. 

-W CATEGORY 


--warnings=category 
Output warnings falling in category. category can be one of: 
gnu 
warnings related to the GNU Coding Standards (see Top). 

obsolete 
obsolete features or constructions 

portability 
portability issues (e.g., use of Make features which are known not portable) 

syntax 
weird syntax, unused variables, typos 

unsupported 
unsupported or incomplete features 

all 
all the warnings 

none 
turn off all the warnings 

error 
treat warnings as errors 
A category can be turned off by prefixing its name with no-. For instance -Wno-syntax will hide the warnings about unused variables. 

The categories output by default are syntax and unsupported. Additionally, gnu is enabled in --gnu and --gnits strictness. 

portability warnings are currently disabled by default, but they will be enabled in --gnu and --gnits strictness in a future release. 

The environment variable WARNINGS can contain a comma separated list of categories to enable. It will be taken into account before the command-line switches, this way -Wnone will also ignore any warning category enabled by WARNINGS. This variable is also used by other tools like autoconf; unknown categories are ignored for this reason. 



--------------------------------------------------------------------------------
Node: configure, Next: Top level, Previous: Invoking Automake, Up: Top 

Scanning configure.in
Automake scans the package's configure.in to determine certain information about the package. Some autoconf macros are required and some variables must be defined in configure.in. Automake will also use information from configure.in to further tailor its output. 

Automake also supplies some Autoconf macros to make the maintenance easier. These macros can automatically be put into your aclocal.m4 using the aclocal program. 

Requirements: Configuration requirements 
Optional: Other things Automake recognizes 
Invoking aclocal: Auto-generating aclocal.m4 
aclocal options: aclocal command line arguments 
Macro search path: Modifying aclocal's search path 
Macros: Autoconf macros supplied with Automake 
Extending aclocal: Writing your own aclocal macros 


--------------------------------------------------------------------------------
Node: Requirements, Next: Optional, Previous: configure, Up: configure 

Configuration requirements
The one real requirement of Automake is that your configure.in call AM_INIT_AUTOMAKE. This macro does several things which are required for proper Automake operation (see Macros). 

Here are the other macros which Automake requires but which are not run by AM_INIT_AUTOMAKE: 

AC_CONFIG_FILES 
AC_OUTPUT 
Automake uses these to determine which files to create (see Creating Output Files). A listed file is considered to be an Automake generated Makefile if there exists a file with the same name and the .am extension appended. Typically, AC_CONFIG_FILES([foo/Makefile]) will cause Automake to generate foo/Makefile.in if foo/Makefile.am exists. 
These files are all removed by make distclean. 



--------------------------------------------------------------------------------
Node: Optional, Next: Invoking aclocal, Previous: Requirements, Up: configure 

Other things Automake recognizes
Every time Automake is run it calls Autoconf to trace configure.in. This way it can recognize the use of certain macros and tailor the generated Makefile.in appropriately. Currently recognized macros and their effects are: 

AC_CONFIG_HEADERS 
Automake will generate rules to rebuild these headers. Older versions of Automake required the use of AM_CONFIG_HEADER (see Macros); this is no longer the case today. 

AC_CONFIG_AUX_DIR 
Automake will look for various helper scripts, such as mkinstalldirs, in the directory named in this macro invocation. (The full list of scripts is: config.guess, config.sub, depcomp, elisp-comp, compile, install-sh, ltmain.sh, mdate-sh, missing, mkinstalldirs, py-compile, texinfo.tex, and ylwrap.) Not all scripts are always searched for; some scripts will only be sought if the generated Makefile.in requires them. 
If AC_CONFIG_AUX_DIR is not given, the scripts are looked for in their standard locations. For mdate-sh, texinfo.tex, and ylwrap, the standard location is the source directory corresponding to the current Makefile.am. For the rest, the standard location is the first one of ., .., or ../.. (relative to the top source directory) that provides any one of the helper scripts. See Finding `configure' Input. 

Required files from AC_CONFIG_AUX_DIR are automatically distributed, even if there is no Makefile.am in this directory. 


AC_CANONICAL_HOST 
Automake will ensure that config.guess and config.sub exist. Also, the Makefile variables host_alias and host_triplet are introduced. See Getting the Canonical System Type. 

AC_CANONICAL_SYSTEM 
This is similar to AC_CANONICAL_HOST, but also defines the Makefile variables build_alias and target_alias. See Getting the Canonical System Type. 

AC_LIBSOURCE 
AC_LIBSOURCES 
AC_LIBOBJ 
Automake will automatically distribute any file listed in AC_LIBSOURCE or AC_LIBSOURCES. 
Note that the AC_LIBOBJ macro calls AC_LIBSOURCE. So if an Autoconf macro is documented to call AC_LIBOBJ([file]), then file.c will be distributed automatically by Automake. This encompasses many macros like AC_FUNC_ALLOCA, AC_FUNC_MEMCMP, AC_REPLACE_FUNCS, and others. 

By the way, direct assignments to LIBOBJS are no longer supported. You should always use AC_LIBOBJ for this purpose. See AC_LIBOBJ vs. LIBOBJS. 


AC_PROG_RANLIB 
This is required if any libraries are built in the package. See Particular Program Checks. 

AC_PROG_CXX 
This is required if any C++ source is included. See Particular Program Checks. 

AC_PROG_F77 
This is required if any Fortran 77 source is included. This macro is distributed with Autoconf version 2.13 and later. See Particular Program Checks. 

AC_F77_LIBRARY_LDFLAGS 
This is required for programs and shared libraries that are a mixture of languages that include Fortran 77 (see Mixing Fortran 77 With C and C++). See Autoconf macros supplied with Automake. 

AC_PROG_LIBTOOL 
Automake will turn on processing for libtool (see Introduction). 

AC_PROG_YACC 
If a Yacc source file is seen, then you must either use this macro or define the variable YACC in configure.in. The former is preferred (see Particular Program Checks). 

AC_PROG_LEX 
If a Lex source file is seen, then this macro must be used. See Particular Program Checks. 

AC_SUBST 
The first argument is automatically defined as a variable in each generated Makefile.in. See Setting Output Variables. 
If the Autoconf manual says that a macro calls AC_SUBST for var, or defined the output variable var then var will be defined in each generated Makefile.in. E.g. AC_PATH_XTRA defines X_CFLAGS and X_LIBS, so you can use the variable in any Makefile.am if AC_PATH_XTRA is called. 


AM_C_PROTOTYPES 
This is required when using automatic de-ANSI-fication; see ANSI. 

AM_GNU_GETTEXT 
This macro is required for packages which use GNU gettext (see gettext). It is distributed with gettext. If Automake sees this macro it ensures that the package meets some of gettext's requirements. 

AM_MAINTAINER_MODE 
This macro adds a --enable-maintainer-mode option to configure. If this is used, automake will cause maintainer-only rules to be turned off by default in the generated Makefile.ins. This macro defines the MAINTAINER_MODE conditional, which you can use in your own Makefile.am. 


--------------------------------------------------------------------------------
Node: Invoking aclocal, Next: aclocal options, Previous: Optional, Up: configure 

Auto-generating aclocal.m4
Automake includes a number of Autoconf macros which can be used in your package; some of them are actually required by Automake in certain situations. These macros must be defined in your aclocal.m4; otherwise they will not be seen by autoconf. 

The aclocal program will automatically generate aclocal.m4 files based on the contents of configure.in. This provides a convenient way to get Automake-provided macros, without having to search around. Also, the aclocal mechanism allows other packages to supply their own macros. 

At startup, aclocal scans all the .m4 files it can find, looking for macro definitions (see Macro search path). Then it scans configure.in. Any mention of one of the macros found in the first step causes that macro, and any macros it in turn requires, to be put into aclocal.m4. 

The contents of acinclude.m4, if it exists, are also automatically included in aclocal.m4. This is useful for incorporating local macros into configure. 

aclocal tries to be smart about looking for new AC_DEFUNs in the files it scans. It also tries to copy the full text of the scanned file into aclocal.m4, including both # and dnl comments. If you want to make a comment which will be completely ignored by aclocal, use ## as the comment leader. 

aclocal options: Options supported by aclocal 
Macro search path: How aclocal finds .m4 files 


--------------------------------------------------------------------------------
Node: aclocal options, Next: Macro search path, Previous: Invoking aclocal, Up: configure 

aclocal options
aclocal accepts the following options: 

--acdir=dir 
Look for the macro files in dir instead of the installation directory. This is typically used for debugging. 

--help 
Print a summary of the command line options and exit. 

-I dir 
Add the directory dir to the list of directories searched for .m4 files. 

--output=file 
Cause the output to be put into file instead of aclocal.m4. 

--print-ac-dir 
Prints the name of the directory which aclocal will search to find third-party .m4 files. When this option is given, normal processing is suppressed. This option can be used by a package to determine where to install a macro file. 

--verbose 
Print the names of the files it examines. 

--version 
Print the version number of Automake and exit. 


--------------------------------------------------------------------------------
Node: Macro search path, Next: Macros, Previous: aclocal options, Up: configure 

Macro search path
By default, aclocal searches for .m4 files in the following directories, in this order: 

acdir-APIVERSION 
This is where the .m4 macros distributed with automake itself are stored. APIVERSION depends on the automake release used; for automake 1.6.x, APIVERSION = 1.6. 

acdir 
This directory is intended for third party .m4 files, and is configured when automake itself is built. This is @datadir@/aclocal/, which typically expands to ${prefix}/share/aclocal/. To find the compiled-in value of acdir, use the --print-ac-dir option (see aclocal options). 
As an example, suppose that automake-1.6.2 was configured with --prefix=/usr/local. Then, the search path would be: 

/usr/local/share/aclocal-1.6/ 
/usr/local/share/aclocal/ 
As explained in (see aclocal options), there are several options that can be used to change or extend this search path. 

Modifying the macro search path: --acdir
The most obvious option to modify the search path is --acdir=dir, which changes default directory and drops the APIVERSION directory. For example, if one specifies --acdir=/opt/private/, then the search path becomes: 

/opt/private/ 
Note that this option, --acdir, is intended for use by the internal automake test suite only; it is not ordinarily needed by end-users. 

Modifying the macro search path: -I dir
Any extra directories specified using -I options (see aclocal options) are prepended to this search list. Thus, aclocal -I /foo -I /bar results in the following search path: 

/foo 
/bar 
acdir-APIVERSION 
acdir 
Modifying the macro search path: dirlist
There is a third mechanism for customizing the search path. If a dirlist file exists in acdir, then that file is assumed to contain a list of directories, one per line, to be added to the search list. These directories are searched after all other directories. 

For example, suppose acdir/dirlist contains the following: 

     /test1
     /test2
     
and that aclocal was called with the -I /foo -I /bar options. Then, the search path would be 

/foo 
/bar 
acdir-APIVERSION 
acdir 
/test1 
/test2 
If the --acdir=dir option is used, then aclocal will search for the dirlist file in dir. In the --acdir=/opt/private/ example above, aclocal would look for /opt/private/dirlist. Again, however, the --acdir option is intended for use by the internal automake test suite only; --acdir is not ordinarily needed by end-users. 

dirlist is useful in the following situation: suppose that automake version 1.6.2 is installed with $prefix=/usr by the system vendor. Thus, the default search directories are 

/usr/share/aclocal-1.6/ 
/usr/share/aclocal/ 
However, suppose further that many packages have been manually installed on the system, with $prefix=/usr/local, as is typical. In that case, many of these "extra" .m4 files are in /usr/local/share/aclocal. The only way to force /usr/bin/aclocal to find these "extra" .m4 files is to always call aclocal -I /usr/local/share/aclocal. This is inconvenient. With dirlist, one may create the file 

/usr/share/aclocal/dirlist 

which contains only the single line 

/usr/local/share/aclocal 

Now, the "default" search path on the affected system is 

/usr/share/aclocal-1.6/ 
/usr/share/aclocal/ 
/usr/local/share/aclocal/ 
without the need for -I options; -I options can be reserved for project-specific needs (my-source-dir/m4/), rather than using it to work around local system-dependent tool installation directories. 

Similarly, dirlist can be handy if you have installed a local copy Automake on your account and want aclocal to look for macros installed at other places on the system. 



--------------------------------------------------------------------------------
Node: Macros, Next: Extending aclocal, Previous: Macro search path, Up: configure 

Autoconf macros supplied with Automake
Automake ships with several Autoconf macros that you can use from your configure.in. When you use one of them it will be included by aclocal in aclocal.m4. 

Public macros: Macros that you can use. 
Private macros: Macros that you should not use. 


--------------------------------------------------------------------------------
Node: Public macros, Next: Private macros, Previous: Macros, Up: Macros 

Public macros
AM_CONFIG_HEADER 
Automake will generate rules to automatically regenerate the config header. This obsolete macro is a synonym of AC_CONFIG_HEADERS today (see Optional). 

AM_ENABLE_MULTILIB 
This is used when a "multilib" library is being built. The first optional argument is the name of the Makefile being generated; it defaults to Makefile. The second option argument is used to find the top source directory; it defaults to the empty string (generally this should not be used unless you are familiar with the internals). See Multilibs. 

AM_C_PROTOTYPES 
Check to see if function prototypes are understood by the compiler. If so, define PROTOTYPES and set the output variables U and ANSI2KNR to the empty string. Otherwise, set U to _ and ANSI2KNR to ./ansi2knr. Automake uses these values to implement automatic de-ANSI-fication. 

AM_HEADER_TIOCGWINSZ_NEEDS_SYS_IOCTL 
If the use of TIOCGWINSZ requires <sys/ioctl.h>, then define GWINSZ_IN_SYS_IOCTL. Otherwise TIOCGWINSZ can be found in <termios.h>. 

AM_INIT_AUTOMAKE([OPTIONS]) 
AM_INIT_AUTOMAKE(PACKAGE, VERSION, [NO-DEFINE]) 
Runs many macros required for proper operation of the generated Makefiles. 
This macro has two forms, the first of which is preferred. In this form, AM_INIT_AUTOMAKE is called with a single argument -- a space-separated list of Automake options which should be applied to every Makefile.am in the tree. The effect is as if each option were listed in AUTOMAKE_OPTIONS. 

The second, deprecated, form of AM_INIT_AUTOMAKE has two required arguments: the package and the version number. This form is obsolete because the package and version can be obtained from Autoconf's AC_INIT macro (which itself has an old and a new form). 

If your configure.in has: 

          AC_INIT(src/foo.c)
          AM_INIT_AUTOMAKE(mumble, 1.5)
          
you can modernize it as follows: 
          AC_INIT(mumble, 1.5)
          AC_CONFIG_SRCDIR(src/foo.c)
          AM_INIT_AUTOMAKE
          
Note that if you're upgrading your configure.in from an earlier version of Automake, it is not always correct to simply move the package and version arguments from AM_INIT_AUTOMAKE directly to AC_INIT, as in the example above. The first argument to AC_INIT should be the name of your package (e.g. GNU Automake), not the tarball name (e.g. automake) that you used to pass to AM_INIT_AUTOMAKE. Autoconf tries to derive a tarball name from the package name, which should work for most but not all package names. (If it doesn't work for yours, you can use the four-argument form of AC_INIT -- supported in Autoconf versions greater than 2.52g -- to provide the tarball name explicitly). 

By default this macro AC_DEFINE's PACKAGE and VERSION. This can be avoided by passing the no-define option, as in: 

          AM_INIT_AUTOMAKE([gnits 1.5 no-define dist-bzip2])
          
or by passing a third non-empty argument to the obsolete form. 

AM_PATH_LISPDIR 
Searches for the program emacs, and, if found, sets the output variable lispdir to the full path to Emacs' site-lisp directory. 
Note that this test assumes the emacs found to be a version that supports Emacs Lisp (such as GNU Emacs or XEmacs). Other emacsen can cause this test to hang (some, like old versions of MicroEmacs, start up in interactive mode, requiring C-x C-c to exit, which is hardly obvious for a non-emacs user). In most cases, however, you should be able to use C-c to kill the test. In order to avoid problems, you can set EMACS to "no" in the environment, or use the --with-lispdir option to configure to explicitly set the correct path (if you're sure you have an emacs that supports Emacs Lisp. 


AM_PROG_AS 
Use this macro when you have assembly code in your project. This will choose the assembler for you (by default the C compiler) and set CCAS, and will also set CCASFLAGS if required. 

AM_PROG_CC_C_O 
This is like AC_PROG_CC_C_O, but it generates its results in the manner required by automake. You must use this instead of AC_PROG_CC_C_O when you need this functionality. 

AM_PROG_CC_STDC 
If the C compiler is not in ANSI C mode by default, try to add an option to output variable CC to make it so. This macro tries various options that select ANSI C on some system or another. It considers the compiler to be in ANSI C mode if it handles function prototypes correctly. 
If you use this macro, you should check after calling it whether the C compiler has been set to accept ANSI C; if not, the shell variable am_cv_prog_cc_stdc is set to no. If you wrote your source code in ANSI C, you can make an un-ANSIfied copy of it by using the ansi2knr option (see ANSI). 

This macro is a relic from the time Autoconf didn't offer such a feature. AM_PROG_CC_STDC's logic has now been merged into Autoconf's AC_PROG_CC macro, therefore you should use the latter instead. Chances are you are already using AC_PROG_CC, so you can simply remove the AM_PROG_CC_STDC call and turn all occurrences of $am_cv_prog_cc_stdc into $ac_cv_prog_cc_stdc. AM_PROG_CC_STDC will be marked as obsolete (in the Autoconf sense) in Automake 1.8. 


AM_PROG_LEX 
Like AC_PROG_LEX (see Particular Program Checks), but uses the missing script on systems that do not have lex. HP-UX 10 is one such system. 

AM_PROG_GCJ 
This macro finds the gcj program or causes an error. It sets GCJ and GCJFLAGS. gcj is the Java front-end to the GNU Compiler Collection. 

AM_SYS_POSIX_TERMIOS 
Check to see if POSIX termios headers and functions are available on the system. If so, set the shell variable am_cv_sys_posix_termios to yes. If not, set the variable to no. 

AM_WITH_DMALLOC 
Add support for the dmalloc package. If the user configures with --with-dmalloc, then define WITH_DMALLOC and add -ldmalloc to LIBS. 

AM_WITH_REGEX 
Adds --with-regex to the configure command line. If specified (the default), then the regex regular expression library is used, regex.o is put into LIBOBJS, and WITH_REGEX is defined. If --without-regex is given, then the rx regular expression library is used, and rx.o is put into LIBOBJS. 


--------------------------------------------------------------------------------
Node: Private macros, Previous: Public macros, Up: Macros 

Private macros
The following macros are private macros you should not call directly. They are called by the other public macros when appropriate. Do not rely on them, as they might be changed in a future version. Consider them as implementation details; or better, do not consider them at all: skip this section! 

_AM_DEPENDENCIES 
AM_SET_DEPDIR 
AM_DEP_TRACK 
AM_OUTPUT_DEPENDENCY_COMMANDS 
These macros are used to implement Automake's automatic dependency tracking scheme. They are called automatically by automake when required, and there should be no need to invoke them manually. 

AM_MAKE_INCLUDE 
This macro is used to discover how the user's make handles include statements. This macro is automatically invoked when needed; there should be no need to invoke it manually. 

AM_PROG_INSTALL_STRIP 
This is used to find a version of install which can be used to strip a program at installation time. This macro is automatically included when required. 

AM_SANITY_CHECK 
This checks to make sure that a file created in the build directory is newer than a file in the source directory. This can fail on systems where the clock is set incorrectly. This macro is automatically run from AM_INIT_AUTOMAKE. 


--------------------------------------------------------------------------------
Node: Extending aclocal, Previous: Macros, Up: configure 

Writing your own aclocal macros
The aclocal program doesn't have any built-in knowledge of any macros, so it is easy to extend it with your own macros. 

This can be used by libraries which want to supply their own Autoconf macros for use by other programs. For instance the gettext library supplies a macro AM_GNU_GETTEXT which should be used by any package using gettext. When the library is installed, it installs this macro so that aclocal will find it. 

A macro file's name should end in .m4. Such files should be installed in $(datadir)/aclocal. This is as simple as writing: 

     aclocaldir = $(datadir)/aclocal
     aclocal_DATA = mymacro.m4 myothermacro.m4
     
A file of macros should be a series of properly quoted AC_DEFUN's (see Macro Definitions). The aclocal programs also understands AC_REQUIRE (see Prerequisite Macros), so it is safe to put each macro in a separate file. Each file should have no side effects but macro definitions. Especially, any call to AC_PREREQ should be done inside the defined macro, not at the beginning of the file. 

Starting with Automake 1.8, aclocal will warn about all underquoted calls to AC_DEFUN. We realize this will annoy a lot of people, because aclocal was not so strict in the past and many third party macros are underquoted; and we have to apologize for this temporary inconvenience. The reason we have to be stricter is that a future implementation of aclocal will have to temporary include all these third party .m4 files, maybe several times, even those which are not actually needed. Doing so should alleviate many problem of the current implementation, however it requires a stricter style from the macro authors. Hopefully it is easy to revise the existing macros. For instance 

     # bad style
     AC_PREREQ(2.57)
     AC_DEFUN(AX_FOOBAR,
     [AC_REQUIRE([AX_SOMETHING])dnl
     AX_FOO
     AX_BAR
     ])
     
should be rewritten as 

     AC_DEFUN([AX_FOOBAR],
     [AC_PREREQ(2.57)dnl
     AC_REQUIRE([AX_SOMETHING])dnl
     AX_FOO
     AX_BAR
     ])
     
Wrapping the AC_PREREQ call inside the macro ensures that Autoconf 2.57 will not be required if AX_FOOBAR is not actually used. Most importantly, quoting the first argument of AC_DEFUN allows the macro to be redefined or included twice (otherwise this first argument would be expansed during the second definition). 

If you have been directed here by the aclocal diagnostic but are not the maintainer of the implicated macro, you will want to contact the maintainer of that macro. Please make sure you have the last version of the macro and that the problem already hasn't been reported before doing so: people tend to work faster when they aren't flooded by mails. 



--------------------------------------------------------------------------------
Node: Top level, Next: Alternative, Previous: configure, Up: Top 

The top-level Makefile.am
Recursing subdirectories
In packages with subdirectories, the top level Makefile.am must tell Automake which subdirectories are to be built. This is done via the SUBDIRS variable. 

The SUBDIRS variable holds a list of subdirectories in which building of various sorts can occur. Many targets (e.g. all) in the generated Makefile will run both locally and in all specified subdirectories. Note that the directories listed in SUBDIRS are not required to contain Makefile.ams; only Makefiles (after configuration). This allows inclusion of libraries from packages which do not use Automake (such as gettext). 

In packages that use subdirectories, the top-level Makefile.am is often very short. For instance, here is the Makefile.am from the GNU Hello distribution: 

     EXTRA_DIST = BUGS ChangeLog.O README-alpha
     SUBDIRS = doc intl po src tests
     
When Automake invokes make in a subdirectory, it uses the value of the MAKE variable. It passes the value of the variable AM_MAKEFLAGS to the make invocation; this can be set in Makefile.am if there are flags you must always pass to make. 

The directories mentioned in SUBDIRS must be direct children of the current directory. For instance, you cannot put src/subdir into SUBDIRS. Instead you should put SUBDIRS = subdir into src/Makefile.am. Automake can be used to construct packages of arbitrary depth this way. 

By default, Automake generates Makefiles which work depth-first (postfix). However, it is possible to change this ordering. You can do this by putting . into SUBDIRS. For instance, putting . first will cause a prefix ordering of directories. All clean targets are run in reverse order of build targets. 

Conditional subdirectories
It is possible to define the SUBDIRS variable conditionally if, like in the case of GNU Inetutils, you want to only build a subset of the entire package. 

To illustrate how this works, let's assume we have two directories src/ and opt/. src/ should always be built, but we want to decide in ./configure whether opt/ will be built or not. (For this example we will assume that opt/ should be built when the variable $want_opt was set to yes.) 

Running make should thus recurse into src/ always, and then maybe in opt/. 

However make dist should always recurse into both src/ and opt/. Because opt/ should be distributed even if it is not needed in the current configuration. This means opt/Makefile should be created unconditionally. 3 

There are two ways to setup a project like this. You can use Automake conditionals (see Conditionals) or use Autoconf AC_SUBST variables (see Setting Output Variables). Using Automake conditionals is the preferred solution. 

Conditional subdirectories with AM_CONDITIONAL
configure should output the Makefile for each directory and define a condition into which opt/ should be built. 

     ...
     AM_CONDITIONAL([COND_OPT], [test "$want_opt" = yes])
     AC_CONFIG_FILES([Makefile src/Makefile opt/Makefile])
     ...
     
Then SUBDIRS can be defined in the top-level Makefile.am as follows. 

     if COND_OPT
       MAYBE_OPT = opt
     endif
     SUBDIRS = src $(MAYBE_OPT)
     
As you can see, running make will rightly recurse into src/ and maybe opt/. 

As you can't see, running make dist will recurse into both src/ and opt/ directories because make dist, unlike make all, doesn't use the SUBDIRS variable. It uses the DIST_SUBDIRS variable. 

In this case Automake will define DIST_SUBDIRS = src opt automatically because it knows that MAYBE_OPT can contain opt in some condition. 

Conditional subdirectories with AC_SUBST
Another idea is to define MAYBE_OPT from ./configure using AC_SUBST: 

     ...
     if test "$want_opt" = yes; then
       MAYBE_OPT=opt
     else
       MAYBE_OPT=
     fi
     AC_SUBST([MAYBE_OPT])
     AC_CONFIG_FILES([Makefile src/Makefile opt/Makefile])
     ...
     
In this case the top-level Makefile.am should look as follows. 

     SUBDIRS = src $(MAYBE_OPT)
     DIST_SUBDIRS = src opt
     
The drawback is that since Automake cannot guess what the possible values of MAYBE_OPT are, it is necessary to define DIST_SUBDIRS. 

How DIST_SUBDIRS is used
As shown in the above examples, DIST_SUBDIRS is used by targets that need to recurse in all directories, even those which have been conditionally left out of the build. 

Precisely, DIST_SUBDIRS is used by make dist, make distclean, and make maintainer-clean. All other recursive targets use SUBDIRS. 

Automake will define DIST_SUBDIRS automatically from the possibles values of SUBDIRS in all conditions. 

If SUBDIRS contains AC_SUBST variables, DIST_SUBDIRS will not be defined correctly because Automake doesn't know the possible values of these variables. In this case DIST_SUBDIRS needs to be defined manually. 



--------------------------------------------------------------------------------
Node: Alternative, Next: Rebuilding, Previous: Top level, Up: Top 

An Alternative Approach to Subdirectories
If you've ever read Peter Miller's excellent paper, Recursive Make Considered Harmful, the preceding section on the use of subdirectories will probably come as unwelcome advice. For those who haven't read the paper, Miller's main thesis is that recursive make invocations are both slow and error-prone. 

Automake provides sufficient cross-directory support 4 to enable you to write a single Makefile.am for a complex multi-directory package. 

By default an installable file specified in a subdirectory will have its directory name stripped before installation. For instance, in this example, the header file will be installed as $(includedir)/stdio.h: 

     include_HEADERS = inc/stdio.h
     
However, the nobase_ prefix can be used to circumvent this path stripping. In this example, the header file will be installed as $(includedir)/sys/types.h: 

     nobase_include_HEADERS = sys/types.h
     
nobase_ should be specified first when used in conjunction with either dist_ or nodist_ (see Dist). For instance: 

     nobase_dist_pkgdata_DATA = images/vortex.pgm
     


--------------------------------------------------------------------------------
Node: Rebuilding, Next: Programs, Previous: Alternative, Up: Top 

Rebuilding Makefiles
Automake generates rules to automatically rebuild Makefiles, configure, and other derived files like Makefile.in. 

If you are using AM_MAINTAINER_MODE in configure.in, then these automatic rebuilding rules are only enabled in maintainer mode. 

Sometimes you need to run aclocal with an argument like -I to tell it where to find .m4 files. Since sometimes make will automatically run aclocal, you need a way to specify these arguments. You can do this by defining ACLOCAL_AMFLAGS; this holds arguments which are passed verbatim to aclocal. This variable is only useful in the top-level Makefile.am. 



--------------------------------------------------------------------------------
Node: Programs, Next: Other objects, Previous: Rebuilding, Up: Top 

Building Programs and Libraries
A large part of Automake's functionality is dedicated to making it easy to build programs and libraries. 

A Program: Building a program 
A Library: Building a library 
A Shared Library: Building a Libtool library 
Program and Library Variables: Variables controlling program and library builds 
LIBOBJS: Special handling for LIBOBJS and ALLOCA 
Program variables: Variables used when building a program 
Yacc and Lex: Yacc and Lex support 
C++ Support: 
Assembly Support: 
Fortran 77 Support: 
Java Support: 
Support for Other Languages: 
ANSI: Automatic de-ANSI-fication 
Dependencies: Automatic dependency tracking 
EXEEXT: Support for executable extensions 


--------------------------------------------------------------------------------
Node: A Program, Next: A Library, Previous: Programs, Up: Programs 

Building a program
In order to build a program, you need to tell Automake which sources are part of it, and which libraries it should be linked with. 

This section also covers conditional compilation of sources or programs. Most of the comments about these also apply to libraries (see A Library) and libtool libraries (see A Shared Library). 

Program Sources: Defining program sources 
Linking: Linking with libraries or extra objects 
Conditional Sources: Handling conditional sources 
Conditional Programs: Building program conditionally 


--------------------------------------------------------------------------------
Node: Program Sources, Next: Linking, Previous: A Program, Up: A Program 

Defining program sources
In a directory containing source that gets built into a program (as opposed to a library or a script), the PROGRAMS primary is used. Programs can be installed in bindir, sbindir, libexecdir, pkglibdir, or not at all (noinst). They can also be built only for make check, in which case the prefix is check. 

For instance: 

     bin_PROGRAMS = hello
     
In this simple case, the resulting Makefile.in will contain code to generate a program named hello. 

Associated with each program are several assisting variables which are named after the program. These variables are all optional, and have reasonable defaults. Each variable, its use, and default is spelled out below; we use the "hello" example throughout. 

The variable hello_SOURCES is used to specify which source files get built into an executable: 

     hello_SOURCES = hello.c version.c getopt.c getopt1.c getopt.h system.h
     
This causes each mentioned .c file to be compiled into the corresponding .o. Then all are linked to produce hello. 

If hello_SOURCES is not specified, then it defaults to the single file hello.c; that is, the default is to compile a single C file whose base name is the name of the program itself. (This is a terrible default but we are stuck with it for historical reasons.) 

Multiple programs can be built in a single directory. Multiple programs can share a single source file, which must be listed in each _SOURCES definition. 

Header files listed in a _SOURCES definition will be included in the distribution but otherwise ignored. In case it isn't obvious, you should not include the header file generated by configure in a _SOURCES variable; this file should not be distributed. Lex (.l) and Yacc (.y) files can also be listed; see Yacc and Lex. 



--------------------------------------------------------------------------------
Node: Linking, Next: Conditional Sources, Previous: Program Sources, Up: A Program 

Linking the program
If you need to link against libraries that are not found by configure, you can use LDADD to do so. This variable is used to specify additional objects or libraries to link with; it is inappropriate for specifying specific linker flags, you should use AM_LDFLAGS for this purpose. 

Sometimes, multiple programs are built in one directory but do not share the same link-time requirements. In this case, you can use the prog_LDADD variable (where prog is the name of the program as it appears in some _PROGRAMS variable, and usually written in lowercase) to override the global LDADD. If this variable exists for a given program, then that program is not linked using LDADD. 

For instance, in GNU cpio, pax, cpio and mt are linked against the library libcpio.a. However, rmt is built in the same directory, and has no such link requirement. Also, mt and rmt are only built on certain architectures. Here is what cpio's src/Makefile.am looks like (abridged): 

     bin_PROGRAMS = cpio pax @MT@
     libexec_PROGRAMS = @RMT@
     EXTRA_PROGRAMS = mt rmt
     
     LDADD = ../lib/libcpio.a @INTLLIBS@
     rmt_LDADD =
     
     cpio_SOURCES = ...
     pax_SOURCES = ...
     mt_SOURCES = ...
     rmt_SOURCES = ...
     
prog_LDADD is inappropriate for passing program-specific linker flags (except for -l, -L, -dlopen and -dlpreopen). So, use the prog_LDFLAGS variable for this purpose. 

It is also occasionally useful to have a program depend on some other target which is not actually part of that program. This can be done using the prog_DEPENDENCIES variable. Each program depends on the contents of such a variable, but no further interpretation is done. 

If prog_DEPENDENCIES is not supplied, it is computed by Automake. The automatically-assigned value is the contents of prog_LDADD, with most configure substitutions, -l, -L, -dlopen and -dlpreopen options removed. The configure substitutions that are left in are only @LIBOBJS@ and @ALLOCA@; these are left because it is known that they will not cause an invalid value for prog_DEPENDENCIES to be generated. 



--------------------------------------------------------------------------------
Node: Conditional Sources, Next: Conditional Programs, Previous: Linking, Up: A Program 

Conditional compilation of sources
You can't put a configure substitution (e.g., @FOO@) into a _SOURCES variable. The reason for this is a bit hard to explain, but suffice to say that it simply won't work. Automake will give an error if you try to do this. 

Fortunately there are two other ways to achieve the same result. One is to use configure substitutions in _LDADD variables, the other is to use an Automake conditional. 

Conditional compilation using _LDADD substitutions
Automake must know all the source files that could possibly go into a program, even if not all the files are built in every circumstance. Any files which are only conditionally built should be listed in the appropriate EXTRA_ variable. For instance, if hello-linux.c or hello-generic.c were conditionally included in hello, the Makefile.am would contain: 

     bin_PROGRAMS = hello
     hello_SOURCES = hello-common.c
     EXTRA_hello_SOURCES = hello-linux.c hello-generic.c
     hello_LDADD = @HELLO_SYSTEM@
     hello_DEPENDENCIES = @HELLO_SYSTEM@
     
You can then setup the @HELLO_SYSTEM@ substitution from configure.in: 

     ...
     case $host in
       *linux*) HELLO_SYSTEM='hello-linux.$(OBJEXT)' ;;
       *)       HELLO_SYSTEM='hello-generic.$(OBJEXT)' ;;
     esac
     AC_SUBST([HELLO_SYSTEM])
     ...
     
In this case, HELLO_SYSTEM should be replaced by hello-linux.o or hello-bsd.o, and added to hello_DEPENDENCIES and hello_LDADD in order to be built and linked in. 

Conditional compilation using Automake conditionals
An often simpler way to compile source files conditionally is to use Automake conditionals. For instance, you could use this Makefile.am construct to build the same hello example: 

     bin_PROGRAMS = hello
     if LINUX
     hello_SOURCES = hello-linux.c hello-common.c
     else
     hello_SOURCES = hello-generic.c hello-common.c
     endif
     
In this case, your configure.in should setup the LINUX conditional using AM_CONDITIONAL (see Conditionals). 

When using conditionals like this you don't need to use the EXTRA_ variable, because Automake will examine the contents of each variable to construct the complete list of source files. 

If your program uses a lot of files, you will probably prefer a conditional +=. 

     bin_PROGRAMS = hello
     hello_SOURCES = hello-common.c
     if LINUX
     hello_SOURCES += hello-linux.c
     else
     hello_SOURCES += hello-generic.c
     endif
     


--------------------------------------------------------------------------------
Node: Conditional Programs, Previous: Conditional Sources, Up: A Program 

Conditional compilation of programs
Sometimes it is useful to determine the programs that are to be built at configure time. For instance, GNU cpio only builds mt and rmt under special circumstances. The means to achieve conditional compilation of programs are the same you can use to compile source files conditionally: substitutions or conditionals. 

Conditional programs using configure substitutions
In this case, you must notify Automake of all the programs that can possibly be built, but at the same time cause the generated Makefile.in to use the programs specified by configure. This is done by having configure substitute values into each _PROGRAMS definition, while listing all optionally built programs in EXTRA_PROGRAMS. 

     bin_PROGRAMS = cpio pax $(MT)
     libexec_PROGRAMS = $(RMT)
     EXTRA_PROGRAMS = mt rmt
     
As explained in EXEEXT, Automake will rewrite bin_PROGRAMS, libexec_PROGRAMS, and EXTRA_PROGRAMS, appending $(EXEEXT) to each binary. Obviously it cannot rewrite values obtained at run-time through configure substitutions, therefore you should take care of appending $(EXEEXT) yourself, as in AC_SUBST([MT], ['mt${EXEEXT}']). 

Conditional programs using Automake conditionals
You can also use Automake conditionals (see Conditionals) to select programs to be built. In this case you don't have to worry about $(EXEEXT) or EXTRA_PROGRAMS. 

     bin_PROGRAMS = cpio pax
     if WANT_MT
       bin_PROGRAMS += mt
     endif
     if WANT_RMT
       libexec_PROGRAMS = rmt
     endif
     


--------------------------------------------------------------------------------
Node: A Library, Next: A Shared Library, Previous: A Program, Up: Programs 

Building a library
Building a library is much like building a program. In this case, the name of the primary is LIBRARIES. Libraries can be installed in libdir or pkglibdir. 

See A Shared Library, for information on how to build shared libraries using libtool and the LTLIBRARIES primary. 

Each _LIBRARIES variable is a list of the libraries to be built. For instance to create a library named libcpio.a, but not install it, you would write: 

     noinst_LIBRARIES = libcpio.a
     
The sources that go into a library are determined exactly as they are for programs, via the _SOURCES variables. Note that the library name is canonicalized (see Canonicalization), so the _SOURCES variable corresponding to liblob.a is liblob_a_SOURCES, not liblob.a_SOURCES. 

Extra objects can be added to a library using the library_LIBADD variable. This should be used for objects determined by configure. Again from cpio: 

     libcpio_a_LIBADD = $(LIBOBJS) $(ALLOCA)
     
In addition, sources for extra objects that will not exist until configure-time must be added to the BUILT_SOURCES variable (see Sources). 



--------------------------------------------------------------------------------
Node: A Shared Library, Next: Program and Library Variables, Previous: A Library, Up: Programs 

Building a Shared Library
Building shared libraries portably is a relatively complex matter. For this reason, GNU Libtool (see Introduction) was created to help build shared libraries in a platform-independent way. 

Libtool Concept: Introducing Libtool 
Libtool Libraries: Declaring Libtool Libraries 
Conditional Libtool Libraries: Building Libtool Libraries Conditionally 
Conditional Libtool Sources: Choosing Library Sources Conditionally 
Libtool Convenience Libraries: Building Convenience Libtool Libraries 
Libtool Modules: Building Libtool Modules 
Libtool Flags: Using _LIBADD and _LDFLAGS 
LTLIBOBJ: Using $(LTLIBOBJ) 
Libtool Issues: Common Issues Related to Libtool's Use 


--------------------------------------------------------------------------------
Node: Libtool Concept, Next: Libtool Libraries, Previous: A Shared Library, Up: A Shared Library 

The Libtool Concept
Libtool abstracts shared and static libraries into a unified concept henceforth called libtool libraries. Libtool libraries are files using the .la suffix, and can designate a static library, a shared library, or maybe both. Their exact nature cannot be determined until ./configure is run: not all platforms support all kinds of libraries, and users can explicitly select which libraries should be built. (However the package's maintainers can tune the default, See The AC_PROG_LIBTOOL macro.) 

Because object files for shared and static libraries must be compiled differently, libtool is also used during compilation. Object files built by libtool are called libtool objects: these are files using the .lo suffix. Libtool libraries are built from these libtool objects. 

You should not assume anything about the structure of .la or .lo files and how libtool constructs them: this is libtool's concern, and the last thing one wants is to learn about libtool's guts. However the existence of these files matters, because they are used as targets and dependencies in Makefiles when building libtool libraries. There are situations where you may have to refer to these, for instance when expressing dependencies for building source files conditionally (see Conditional Libtool Sources). 

People considering writing a plug-in system, with dynamically loaded modules, should look into libltdl: libtool's dlopening library (see Using libltdl). This offers a portable dlopening facility to load libtool libraries dynamically, and can also achieve static linking where unavoidable. 

Before we discuss how to use libtool with Automake in details, it should be noted that the libtool manual also has a section about how to use Automake with libtool (see Using Automake with Libtool). 



--------------------------------------------------------------------------------
Node: Libtool Libraries, Next: Conditional Libtool Libraries, Previous: Libtool Concept, Up: A Shared Library 

Building Libtool Libraries
Automake uses libtool to build libraries declared with the LTLIBRARIES primary. Each _LTLIBRARIES variable is a list of libtool libraries to build. For instance, to create a libtool library named libgettext.la, and install it in libdir, write: 

     lib_LTLIBRARIES = libgettext.la
     libgettext_la_SOURCES = gettext.c gettext.h ...
     
Automake predefines the variable pkglibdir, so you can use pkglib_LTLIBRARIES to install libraries in $(libdir)/@PACKAGE@/. 



--------------------------------------------------------------------------------
Node: Conditional Libtool Libraries, Next: Conditional Libtool Sources, Previous: Libtool Libraries, Up: A Shared Library 

Building Libtool Libraries Conditionally
Like conditional programs (see Conditional Programs), there are two main ways to build conditional libraries: using Automake conditionals or using Autoconf AC_SUBSTitutions. 

The important implementation detail you have to be aware of is that the place where a library will be installed matters to libtool: it needs to be indicated at link-time using the -rpath option. 

For libraries whose destination directory is known when Automake runs, Automake will automatically supply the appropriate -rpath option to libtool. This is the case for libraries listed explicitly in some installable _LTLIBRARIES variables such as lib_LTLIBRARIES. 

However, for libraries determined at configure time (and thus mentioned in EXTRA_LTLIBRARIES), Automake does not know the final installation directory. For such libraries you must add the -rpath option to the appropriate _LDFLAGS variable by hand. 

The examples below illustrate the differences between these two methods. 

Here is an example where $(WANTEDLIBS) is an AC_SUBSTed variable set at ./configure-time to either libfoo.la, libbar.la, both, or none. Although $(WANTEDLIBS) appears in the lib_LTLIBRARIES, Automake cannot guess it relates to libfoo.la or libbar.la by the time it creates the link rule for these two libraries. Therefore the -rpath argument must be explicitly supplied. 

     EXTRA_LTLIBRARIES = libfoo.la libbar.la
     lib_LTLIBRARIES = $(WANTEDLIBS)
     libfoo_la_SOURCES = foo.c ...
     libfoo_la_LDFLAGS = -rpath '$(libdir)'
     libbar_la_SOURCES = bar.c ...
     libbar_la_LDFLAGS = -rpath '$(libdir)'
     
Here is how the same Makefile.am would look using Automake conditionals named WANT_LIBFOO and WANT_LIBBAR. Now Automake is able to compute the -rpath setting itself, because it's clear that both libraries will end up in $(libdir) if they are installed. 

     lib_LTLIBRARIES =
     if WANT_LIBFOO
     lib_LTLIBRARIES += libfoo.la
     endif
     if WANT_LIBBAR
     lib_LTLIBRARIES += libbar.la
     endif
     libfoo_la_SOURCES = foo.c ...
     libbar_la_SOURCES = bar.c ...
     


--------------------------------------------------------------------------------
Node: Conditional Libtool Sources, Next: Libtool Convenience Libraries, Previous: Conditional Libtool Libraries, Up: A Shared Library 

Libtool Libraries with Conditional Sources
Conditional compilation of sources in a library can be achieved in the same way as conditional compilation of sources in a program (see Conditional Sources). The only difference is that _LIBADD should be used instead of _LDADD and that it should mention libtool objects (.lo files). 

So, to mimic the hello example from Conditional Sources, we could build a libhello.la library using either hello-linux.c or hello-generic.c with the following Makefile.am. 

     lib_LTLIBRARIES = libhello.la
     libhello_la_SOURCES = hello-common.c
     EXTRA_libhello_la_SOURCES = hello-linux.c hello-generic.c
     libhello_la_LIBADD = $(HELLO_SYSTEM)
     libhello_la_DEPENDENCIES = $(HELLO_SYSTEM)
     
And make sure $(HELLO_SYSTEM) is set to either hello-linux.lo or hello-generic.lo in ./configure. 

Or we could simply use an Automake conditional as follows. 

     lib_LTLIBRARIES = libhello.la
     libhello_la_SOURCES = hello-common.c
     if LINUX
     libhello_la_SOURCES += hello-linux.c
     else
     libhello_la_SOURCES += hello-generic.c
     endif
     


--------------------------------------------------------------------------------
Node: Libtool Convenience Libraries, Next: Libtool Modules, Previous: Conditional Libtool Sources, Up: A Shared Library 

Libtool Convenience Libraries
Sometimes you want to build libtool libraries which should not be installed. These are called libtool convenience libraries and are typically used to encapsulate many sublibraries, later gathered into one big installed library. 

Libtool convenience libraries are declared by noinst_LTLIBRARIES, check_LTLIBRARIES, or even EXTRA_LTLIBRARIES. Unlike installed libtool libraries they do not need an -rpath flag at link time (actually this is the only difference). 

Convenience libraries listed in noinst_LTLIBRARIES are always built. Those listed in check_LTLIBRARIES are built only upon make check. Finally, libraries listed in EXTRA_LTLIBRARIES are never built explicitly: Automake outputs rules to build them, but if the library does not appear as a Makefile dependency anywhere it won't be built (this is why EXTRA_LTLIBRARIES is used for conditional compilation). 

Here is a sample setup merging libtool convenience libraries from subdirectories into one main libtop.la library. 

     # -- Top-level Makefile.am --
     SUBDIRS = sub1 sub2 ...
     lib_LTLIBRARIES = libtop.la
     libtop_la_SOURCES =
     libtop_la_LIBADD = \
       sub1/libsub1.la \
       sub2/libsub2.la \
       ...
     
     # -- sub1/Makefile.am --
     noinst_LTLIBRARIES = libsub1.la
     libsub1_la_SOURCES = ...
     
     # -- sub2/Makefile.am --
     # showing nested convenience libraries
     SUBDIRS = sub2.1 sub2.2 ...
     noinst_LTLIBRARIES = libsub2.la
     libsub2_la_SOURCES =
     libsub2_la_LIBADD = \
       sub21/libsub21.la \
       sub22/libsub22.la \
       ...
     


--------------------------------------------------------------------------------
Node: Libtool Modules, Next: Libtool Flags, Previous: Libtool Convenience Libraries, Up: A Shared Library 

Libtool Modules
These are libtool libraries meant to be dlopened. They are indicated to libtool by passing -module at link-time. 

     pkglib_LTLIBRARIES = mymodule.la
     mymodule_la_SOURCES = doit.c
     mymodule_LDFLAGS = -module
     
Ordinarily, Automake requires that a Library's name starts with lib. However, when building a dynamically loadable module you might wish to use a "nonstandard" name. 



--------------------------------------------------------------------------------
Node: Libtool Flags, Next: LTLIBOBJ, Previous: Libtool Modules, Up: A Shared Library 

_LIBADD and _LDFLAGS
As shown in previous sections, the library_LIBADD variable should be used to list extra libtool objects (.lo files) or libtool libraries (.la) to add to library. 

The library_LDFLAGS variable is the place to list additional libtool flags, such as -version-info, -static, and a lot more. See See Using libltdl. 



--------------------------------------------------------------------------------
Node: LTLIBOBJ, Next: Libtool Issues, Previous: Libtool Flags, Up: A Shared Library 

LTLIBOBJS
Where an ordinary library might include $(LIBOBJS), a libtool library must use $(LTLIBOBJS). This is required because the object files that libtool operates on do not necessarily end in .o. 

Nowadays, the computation of LTLIBOBJS from LIBOBJS is performed automatically by Autoconf (see AC_LIBOBJ vs. LIBOBJS). 



--------------------------------------------------------------------------------
Node: Libtool Issues, Previous: LTLIBOBJ, Up: A Shared Library 

A regular expression is a pattern that describes a set of strings. Regular expressions are constructed analogously to arithmetic expressions, by using various operators to combine smaller expressions. grep understands two different versions of regular expression syntax: "basic"(BRE) and "extended"(ERE). In GNU grep, there is no difference in available functionality using either syntax. In other implementations, basic regular expressions are less powerful. The following description applies to extended regular expressions; differences for basic regular expressions are summarized afterwards. 

The fundamental building blocks are the regular expressions that match a single character. Most characters, including all letters and digits, are regular expressions that match themselves. Any metacharacter with special meaning may be quoted by preceding it with a backslash. 

A regular expression may be followed by one of several repetition operators: 


`.' 
The period `.' matches any single character. 

`?' 
The preceding item is optional and will be matched at most once. 

`*' 
The preceding item will be matched zero or more times. 

`+' 
The preceding item will be matched one or more times. 

`{n}' 
The preceding item is matched exactly n times. 

`{n,}' 
The preceding item is matched n or more times. 

`{n,m}' 
The preceding item is matched at least n times, but not more than m times. 

Two regular expressions may be concatenated; the resulting regular expression matches any string formed by concatenating two substrings that respectively match the concatenated subexpressions. 

Two regular expressions may be joined by the infix operator `|'; the resulting regular expression matches any string matching either subexpression. 

Repetition takes precedence over concatenation, which in turn takes precedence over alternation. A whole subexpression may be enclosed in parentheses to override these precedence rules. 

GNU Finger is a utility program designed to allow users of Unix hosts on the Internet network to get information about each other. It is a direct replacement for the Berkeley 4.3 finger code, although it produces different looking output and is designed to run on a wide variety of systems. 

Why Another Finger?
Originally, each host on the Internet network consisted of a single, reasonably powerful computer, capable of handling many users at the same time. Typically, a site (physical location of computer users) would have only one or two computers, even if they had 20 or more people who used them. If a user at site A wanted to know about users logged on at site B, a simple program could be invoked to query the host at site B about the users which were logged on. 

With the onset of more-power-per-person computing, the mainframe has been set aside. A modern computing facility usually consists of one user per host, and many hosts per site. This makes it a trial to find out about logged on users at another site, since you must query each host to find out about the single user who is logged on. If the site had 20 hosts, you would have to invoke a finger program 20 times just to find out who was logged on! 

GNU Finger is a simple and effective way around this problem. For sites with many hosts, a single host may be designated as the finger server host. This host collects information about who is logged on to other hosts at that site. If a user at site A wants to know about users logged on at site B, only the server host need be queried, instead of each host at that site. This is very convenient. 

GNU Finger is a direct replacement for existing finger programs. Since the finger protocol (rules for communication) is very simple, GNU Finger follows that protocol in responding to simple requests. But GNU Finger also implements another protocol which allows two finger programs to exchange information in a predetermined way, which allows faster and wider bandwidth communication. 

Finger delivers information about users in varying formats, depending on how it is invoked. finger invoked without any options performs a site wide finger request, no matter which machine it has been invoked from. Switch arguments exist for getting the "long" form of finger information and for getting information only about the local machine. 

If a user on host A wants to know about a user on host B, finger must make a network connection to host B. If host B is running a finger program, that program is asked to relay information about the user in question through the connection back to host A, where finger can display it. 

GNU Finger also runs a server daemon process on the server host, whose job is to keep track of which users are logged in to local machines. 

An optional and currently unsupported feature is passing of graphic images. This is built on the new protocol. A user at site A (e.g. MIT) may see the picture of a user at site B (e.g. UCSB), by typing a finger request. The conversion of graphic data from one format to another is done through GNU Finger; no site need know where or how such images are stored on any other site to be able to display those images. You should ask your system administrator to find out whether he has chose to include this functionality on your network. 

Using Finger
Arguments to GNU Finger
The basic argument to Finger is a user@host pair. The user portion is the name of the user about whom you would like information. The host is a machine that the user has an account on. When invoked in this manner, GNU Finger displays the list of hosts that this user is currently logged in on, or, if the user is not logged in, the last time and location that he or she was. host may be expressed as any valid Internet address (i.e. dot-notation, host.domain, etc). 

If host is non-existent, the local host is assumed. If user is blank or unspecified, it is assumed that you want information about all users. 

The content and format of the output of GNU finger depends on what is being fingered: 

User 
Displays login information about user. If `--info' or `--l' is also specified, finger will display the full name, home directory, shell, mail forwarding, and `.plan' and/or `.project' file. This is what the output will look like: 
bash$ finger --info bson@gnu.ai.mit.edu
[apple-gunkies.gnu.ai.mit.edu]

Jan Brittenson (bson)
Home: /home/fsf/bson
Shell: /usr/local/bin/bash
Mail forwarded to bson@ai.mit.edu.
No mail.
  User     Real Name         What    Idle  TTY  Host      Console Location
bson     Jan Brittenson     fgrep          *p0 apple-gu (nutrimat.gnu.ai.)
bson     Jan Brittenson              1:57  *sb nutrimat 

Plan:
	To hack GNU Finger

The following is output, in the order listed, when asking for long information (`-l' or `--info') about a particular user: 
Real and login names. 
Home directory. 
Login shell. 
Mail forwarding. 
Whether the user has any unread mail, and if so, when it was last read. 
Current login information, in the same format as produced by a short finger (see below for an explanation). If the user isn't currently logged on, then the last login time and remote host (if known) is reported. 
A `~/.plan' file. If the file isn't readable by everyone, then a message is printed to this effect. 
A `~/.projects' file. This file, like `~/.plan', should be readable by everyone. 

bash$ finger bson@gnu.ai.mit.edu
[apple-gunkies.gnu.ai.mit.edu]
  User     Real Name         What    Idle  TTY  Host      Console Location
bson     Jan Brittenson     fgrep          *p0 apple-gu (nutrimat.gnu.ai.)
bson     Jan Brittenson              1:57  *sb nutrimat 

Here is an explanation of what each column contains in the short example: 
User 
The user login name. 
Real Name 
The real name of the user. 
What 
The current or last program run by the user, depending on the system in use. On System V Release 4, for instance, the current program is shown, but on BSD it will be the last terminated program. 
Idle 
The time the user has been idle, as hours:minutes, or the first 7 characters of a string such as "14 days". 
TTY 
The significant portion of the user's terminal connection. Exactly what portion this is is system dependent. For example, on System V Release 4, it might be `*40' for `/dev/pty/40', while on BSD it might be `*p0' for `/dev/ttyp0'. If preceded by an asterisk ("*"), then the user allows anyone to send messages to this particular terminal. 
Host 
The host the user is logged onto. 
Console Location 
Where the user's console is located. If logged in over the network, then this will be the most significant portion of the remote host name if known. A host name is always parenthesized. 
In the general short output (i.e. not for a particular host), GNU Finger lists the least idle login for a particular user on each host. A single user often has several logins, since on some systems each window opened creates its own login record. In addition to the least idle login, the console login is also always listed, regardless of how long it has been idle. To list all login records for a particular, host, use the special target `.local'. For example, while the following may be part of the general short finger listing for the host `mole.gnu.ai.mit.edu': 
brendan  Brendan Kehoe               5:09  *p8 mole     (lisa.cygnus.com)
info     InfoMaster                 12:12  *p4 mole     (hal)
law      Jeffrey A. Law              3:52  *p7 mole     (128.110.4.17:0.0)
rms      Richard Stallman   sendmai  1:34  *p1 mole     (unix:0.0)
rms      Richard Stallman           6 days *co mole

The last line is the console. The following might be listed by the command `finger .local@mole.gnu.ai.mit.edu': 
[mole.gnu.ai.mit.edu]
  User     Real Name         What    Idle  TTY  Host      Console Location
brendan  Brendan Kehoe               5:18  *p8 mole     (lisa.cygnus.com)
info     InfoMaster                 12:22  *p4 mole     (hal)
law      Jeffrey A. Law              4:01  *p7 mole     (128.110.4.17:0.0)
rms      Richard Stallman   sendmai  1:44  *p1 mole     (unix:0.0)
rms      Richard Stallman           23:08  *p0 mole     (unix:0.0)
rms      Richard Stallman           1 day, *p2 mole     (unix:0.0)
rms      Richard Stallman           6 days *co mole

Mailing List or Alias 
Expands the mailing list or alias and displays the recipients. You always have to use `--info' or (or `-l') when fingering a mailing list or mail alias, otherwise mail aliases won't be looked up due to the extra processing involved. This is what the output will look like: 
% finger --info postmaster@gnu.ai.mit.edu
postmaster is an alias for the following:
    Roland McGrath <roland>,
    <tower@prep.ai.mit.edu>,
    Noah Friedman <friedman>,
    Michael I Bushnell <mib>

User-defined Target 
Allows the remote host to display specific information, such as price lists, literature, or weather forecasts. For example: 
% finger .site@gnu.ai.mit.edu

This is the FSF GNU Project. For more information, please contact
"postmaster". For information about guest accounts, please contact
"request".

Command Line Options
There are a number of command line options that you can give to GNU Finger: 

`--face' 
`-f' 
Ask Finger for the face of all users information has been requested about. An explicit user list has to be provided. 
`--info' 
`-l' 
`-i' 
Display the "long" form of information for the users fingered. The exact information returned depends on what finger software is run on the remote host -- GNU Finger, for instance, returns specific information. `-l' is supplied for backwards compatibility; finger as distributed from Berkeley has this option. 
`--brief' 
`-b' 
Display the "short" form of information for the users fingered. This is the opposite of `--info'. 
`--port port' 
`-P port' 
Make a connection to port, which can be either a numerical port number or a service name from `/etc/services'. 
`--help' 
`-h' 
Print a description of all options. 
Special User Names
You can give GNU Finger one of several "special" user names. These user names all begin with a period (`.') and instruct the receiving finger daemon to do something that only a GNU Finger daemon can do. Currently, the "special" names are: 

`.free' 
Return a list of free machines. `Free' machines are those that have no users logged in, or have been idle for a long time. The information returned makes it clear which one is true. 
`.all' 
Return the information about every machine that the Finger server knows about. `.all' is equivalent to issuing the finger command without specifying the user name. 
`.site' 
Returns information about the site, such as company and location. 
`.clients' 
Returns a list of the clients that the GNU Finger server knows about. Also lists who is logged onto the console. 
`.faces' 
Return a list of the faces that this server has available. The last line output tells you how many lines were listed previously. 
`.local' 
Finger only at the specific machine. This allows finger to continue to be useful even in the event that the server is down. It also allows you to examine all the login records of a user. Normally, the server only keeps track of the most recently active login record for each user. 
`.help' 
Describe services provided by the finger server. 
Advanced Use
How Finger Works
GNU Finger is the collective name for a set of programs: 

`finger' 
Parses the command line and connects to the finger server, `in.fingerd', on the finger server. Returns the output from the server. finger connects to in.fingerd on the host specified in the command line. This is the only program you need to know anything about if you're a regular user. You should refer to this program as the finger client to avoid possible confusion. 
`fingerd' 
Regularly connects to in.cfingerd on the clients specified in the `fingerdir/clients' file, to obtain finger data. This client data is saved in the file `fingerdir/userdata'. fingerd should run on the host specified in the `fingerdir/serverhost' file. fingerd should be started at boot time. 
`in.fingerd' 
Responds to finger connections through inetd. Should be attached to the `finger' service via `/etc/inetd.conf'. in.fingerd behaves somewhat differently depending on what host it runs on: on the server host it reads the `fingerdir/userdata' database, on all other hosts it forwards all requests (unless `.local' is the target) to in.fingerd on the host specified in `fingerdir/serverhost'. in.fingerd reads the `fingerdir/userdata' database, various system files, and makes SMTP connections to the host specified in the `fingerdir/mailhost' file. 
`in.cfingerd' 
This is the program that responds to call-ins from fingerd by sampling the status on the client and forwarding it to fingerd. It should be configured to respond to the `cfinger' service specified in the `clients' configuration file, or port 2003 if nothing else is specified. 
The `~/.fingerrc' Script
When the GNU Finger server receives a request for information about a user it looks to see if the user has a `.fingerrc' file in the home directory. If such a file exists, and is executable, then this file is executed, and the normal finger output is passed to it as input. Its output becomes what is returned for the request. Thus, it can be used to: 

Disable fingering a specific user by linking `~user/.fingerrc' to `/bin/true'. 
Entirely replace the output of finger by ignoring its input. Below is a sample script which could be put in `~price-info/.fingerrc'. 
#! /bin/sh
#
# This sample script replaces the output of GNU Finger
#
echo Hack-O-Matic Consulting Services, Inc. "   " `date`

cat <<ETX

Hi, thanks for asking us about prices on our newly introduced support
services for Free Software. Below is a list of new services; for a list
of our previous services, please send mail to this address and someone
will contact you. Please don't forget to tell us how to reach you.

ETX
cat ~/info/new-stuff

Filter the output to make changes. If the script below is put in `~bson/.fingerrc' on the finger server host for the domain `gnu.ai.mit.edu': 
#! /bin/sh
#
# This sample filter replaces the "Project:" tag with
# "Working on:"
#
sed -e 's/^Project:$/Working on:/g'

Then when `finger -l bson@gnu.ai.mit.edu' is run, the output could look something like: 
Jan Brittenson (bson)
Home: /home/fsf/bson
Shell: /usr/local/bin/bash
No mail.

Jan Brittenson (bson) is not presently logged in.
Last seen at wombat.gnu.ai.mit.edu on Tue Sep  1 15:08:12 1992

No plan.

Working on:
        1. Hacking GNU Finger
        2. Making friends

User-defined Targets
Various special targets can be added as executable files in the directory `fingerdir/targets'. Each file name in this directory starts with a letter describing when to execute it, followed by a hyphen and the target name. The three letters are: 

`l' 
In response to a "long" finger; usually by typing `finger -l'. 
`s' 
In response to a "short" finger; usually the default if no options are given. 
`x' 
In response to either a "long" or "short" request. 
For instance, the special-target file `x-.help' is run for either `finger .help' or `finger -l .help', whereas the special- target file `l-prices' is run only for `finger -l prices'. 

The special-target file is run as super-user, and is given no input. By convention, all GNU Finger sites should support at least: 

`.help' 
Display message describing what features and special targets exist on this site. 
`.site' 
Display message describing the site. This may include such things as the company name, its address, and how to contact the system manager. 
How Finger Picks a Port
When invoked, the GNU Finger client looks to see if a `--port' option was specified on the command line. If so, then this becomes the port number or service used. Otherwise, the client looks to see what name it was started under, removes any leading directory path, and any trailing suffixes. A suffix is the part of a filename that follows a dot, including the dot itself. This is the service name used. For instance, if GNU Finger is installed as `/usr/local/bin/finger.new', then the service `finger' is used. If it's installed as `/usr/local/bin/gfinger', then the service `gfinger' is used. This behavior can be changed by the system administrator during installation. 

Security Issues
One question that often arises when installing networking software which adds new functionality is whether it can be considered sufficiently secure. The most significant new function in GNU Finger with regard to security is the ability for a user to have a `.fingerrc' in the home directory. The following are the precautions take by GNU Finger: 

Check whether `.fingerrc' is writable to anyone except the owner. Notice that check is not enabled by default, since FSF users like anyone to be able to write any file -- enable this check during installation by editing `config.h'. 
Check whether `.fingerrc' is owned by the user in whose home directory it's found. This, like the previous check, is disabled by default. It really only makes sense on systems where ordinary users can't give away their files. 
Execute the script through the user's login shell, using the command "shell -c script". This means that a user who has had his account disabled (i.e. shell set to a program that prints a notice or just dies) can't run a `.fingerrc' script. This behavior can be changed by hard-coding the shell in `config.h'. 
Bug Reports
You are strongly encouraged to submit a bug report to bug-gnu-utils@gnu.ai.mit.edu if you have problems with this beta release of GNU Finger. Here are some things that are generally helpful to mention, when relevant: 

The GNU Finger version and where you obtained the distribution. 
Your hardware. 
Operating system. 
C Compiler used. 
The arguments you gave to `./configure'. 
Changes you made to the GNU Finger configuration files. 
Any changes you have made to the source code or installation procedures. 
Your network configuration; don't forget to mention what hardware, operating systems, and C compilers are used on each node where the bug manifests itself. 
Any problems you had during installation. 
If you're having problems compiling or installing GNU Finger, then the following is particularly helpful to mention: 

The command line used to invoke configure. 
The current directory. 
The command line used to invoke make and make version (particularly if you're using GNU Make). 
A capture of the compiler output. 
Please feel free to include any patches, as well. 

Installation
Basic Installation
Here are the steps that you will need to take in order to install GNU Finger. 

Pick a machine which will be the local finger server for your network. Create a `clients' file, and install it in `/usr/local/etc/fingerdir' (or the EtcDir as specified in `config.h'). Put the names of all hosts that should report to the finger server in this file. Don't forget to include the finger server itself. 
For each client (the designated server is also a client), do the following, 
Change your working directory to be the top of the GNU Finger sources. For instance, if you have placed the source in `/src/gnu/finger' you would type `cd /src/gnu/finger'. 
In the source directory, type `make clean' and `./configure' if this host is different from the previous one. If you're using csh on an old version of System V, you might need to type `sh ./configure' instead to prevent csh from trying to execute configure itself. The configure shell script attempts to guess correct values for various system-dependent variables used during compilation, and creates the Makefile(s) (one in each subdirectory of the source directory). In some packages it creates a C header file containing system-dependent definitions. It also creates a file `config.status' that you can run in the future to recreate the current configuration. Running configure takes a minute or two. While it is running, it prints some messages that tell what it is doing. If you don't want to see the messages, run configure with its standard output redirected to `/dev/null'; for example, `./configure >/dev/null'. To compile the package in a different directory from the one containing the source code, you must use a version of make that supports the VPATH variable, such as GNU make. `cd' to the directory where you want the object files and executables to go and run configure. configure automatically checks for the source code in the directory that configure is in and in `..'. If for some reason configure is not in the source code directory that you are configuring, then it will report that it can't find the source code. In that case, run configure with the option `--srcdir=DIR', where DIR is the directory that contains the source code. You can tell `configure' to figure out the configuration for your system, and record it in `config.status', without actually configuring the package (creating `Makefile' and perhaps a configuration header file). To do this, give configure the `--no-create' option. Later, you can run ./config.status to actually configure the package for a particular host. This option is useful mainly in `Makefile' rules for updating `config.status' and `Makefile'. You can also give `config.status' the `--recheck' option, which makes it re-run configure with the same arguments you used before. This is useful if you change configure. `configure' ignores any other arguments that you give it. If you want to install the GNU Finger configuration files somewhere other than `/usr/local/etc/fingerdir', then you should edit the files `./config.h' and `include/fingerpaths.h' now. You need to specify the alternate locations of where the configuration files will be kept. If you want to include the unsupported code for mugshots, then you should now also choose one of the face formats, as well as edit `lib/Makefile.in', `lib/site/Makefile.in', and `src/Makefile.in' to compile and link in the files necessary. If your system requires unusual options for compilation or linking that configure doesn't know about, you can give configure initial values for some variables by setting them in the environment. In Bourne-compatible shells, you can do that on the command line like this: 
CC='gcc -traditional' DEFS=-D_POSIX_SOURCE ./configure

The `make' variables that you might want to override with environment variables when running configure are: (For these variables, any value given in the environment overrides the value that `configure' would choose:) 
CC 
C compiler program. Default is `cc', or `gcc' if `gcc' is in your search path. 
INSTALL 
Program to use to install files. Default is `install' if you have it, `cp' otherwise. 
(For these variables, any value given in the environment is added to the value that `configure' chooses:) 
DEFS 
Configuration options, in the form `-Dfoo -Dbar ...' 
LIBS 
Libraries to link with, in the form `-lfoo -lbar ...' 
To build and/or install the GNU Finger executables and standard targets, issue one of the following commands: 
`make server' 
To build and install all executables, plus install the finger-specific server configuration files. Use this if the host is the designated finger server. 
`make client' 
To build all executables, but install only those used by the non-server clients. Use this unless the host is the designated finger server. 
`make all' 
To build all executables, but perform no installation. 
The above commands build `lib/libfinger.a' and the main programs in `src': finger, in.fingerd, and in.cfingerd, as well as fingerd on the server. If you want to, you can override the `make' variables CFLAGS and LDFLAGS like this: 
make CFLAGS=-O2 LDFLAGS=-s

Modify the system configuration so that the client has (refer to the system documentation for details on how to do this on a particular system): 
Entries in the system `services' file, or equivalent, which mentions the correct TCP port for in.cfingerd (port 2003) and in.fingerd (port 79). If port 2003 is already used by something else, then read the section on Configuration Files for details on how to specify a port other than 2003 in the `/usr/local/etc/fingerdir/clients' file. A good name for the service is `cfinger'. 
Entries in the system `inetd.conf' file, or equivalent, which contains references to in.cfingerd and in.fingerd. in.fingerd needs to be run with UID root. Consult your system documentation for details on how to do this. in.cfingerd should be run with UID root on System V derivatives. 
a `/usr/local/etc/fingerdir/serverhost' file which contains the name of the GNU Finger server host, 
a `/usr/local/etc/fingerdir/mailhost' file which contains the name of the mail server to ask for user mail forwarding information and mailing list expansion, and 
the inetd daemon restarted, so that server and user requests can be answered. 
When you have performed the above steps for each client, log onto the designated GNU Finger server. 
Start the server daemon, fingerd. You should arrange to have fingerd started every time the server host is rebooted. For exact details on how to do this, please refer to the server host's system documentation. 
If you chose to include the mugshots option, now might be a good time to install the mugshots. But first, try getting a face from another site running GNU Finger! For example, you might try 
finger --face bfox@aurel.cns.caltech.edu

Modify the files `x-.help' and `x-.site' in `/usr/local/etc/fingerdir/targets' for your site. 
Now you're all set! You might like to read through the section on Configuration Files. 

Configuration Files
This section describes the format of the GNU Finger configuration files. 

The `clients' file
The `/usr/local/etc/fingerdir/clients' file contains a list of clients that the GNU Finger server fingerd is supposed to poll. You can edit this file and then send the finger server a SIGHUP to tell it that the configuration has changed. Each line in the file should be either the name of a host or a comment. The name can be preceded by @port, to tell the finger server to poll the particular host by using a port other than 2003. A comment is any line that starts with a hash sign (#). Below is a sample `clients' file: 

# This file contains all GNU Finger clients on the gnu.ai.mit.edu
# network. Apple-gunkies is the GNU Finger server (see ``serverhost'').
apple-gunkies.gnu.ai.mit.edu

# Albert is the mail exchanger (see ``mailhost'').
albert.gnu.ai.mit.edu

# Spiff is a Sony, so port 2003 is already used for `mbanks'.
# Use port 2010 instead.
@2010 spiff.gnu.ai.mit.edu

churchy.gnu.ai.mit.edu
mole.gnu.ai.mit.edu
geech.gnu.ai.mit.edu
wookumz.gnu.ai.mit.edu
nutrimat.gnu.ai.mit.edu
kropotkin.gnu.ai.mit.edu
goldman.gnu.ai.mit.edu
hal.gnu.ai.mit.edu
wombat.gnu.ai.mit.edu

Although this sample `clients' file contains the fully qualified domain names of the hosts, it's usually enough to specify only the host name portion. Explicit IP addresses can be used too, but this is a practise strongly discouraged. Notice that the server is also in the clients file and has a in.cfingerd; this is necessary in order for the server to correctly poll itself. 

The `serverhost' file
The `/usr/local/etc/fingerdir/serverhost' file holds the name of the GNU Finger server host; this is as the name implies, the host that the GNU Finger server fingerd runs on. Lines starting with a hash sign (#) are treated as comments. Below is a sample `serverhost' file: 

# A-g does all the finger stuff
apple-gunkies.gnu.ai.mit.edu

The `mailhost' file
The `/usr/local/etc/fingerdir/mailhost' file holds the name of the mail exchanger host for the network. This host should know how to talk SMTP; this file should never hold the name of a host that can't. It's contacted to obtain mail forwarding information and to expand mailing lists if a `.forward' file can't be found in the user's home directory. GNU Finger always looks and reports on user `.forward' files regardless of whether `mailhost' exists or not. Any lines in this file that start with a hash sign (#) are treated as comments. 

The `forwardhost' file
The `/usr/local/etc/fingerdir/forwardhost' file holds the name of the host to forward finger requests to when the current finger server can't find a matching user name or mail alias. No forwarding takes place if this file doesn't exist. Any lines that start with a hash sign (#) are treated as comments. This is a sample output of what it can look like when a request is forwarded: 

malformed-names bool ; 
} ;

 



Usage: 
The auth statement configures the parameters of the authentication service. 

listen statement 
This statement determines on which addresses radiusd will listen for incoming authentication requests. Its argument is a comma-separated list of items in the form ip:port-number. ip can be either an IP address in familiar "dotted-quad" notation or a hostname. :port-number part may be omitted, in which case the default authentication port is assumed. 

If the listen statement is omitted, radiusd will accept incoming requests from any interface on the machine. 

The special value no disables listening for authentication requests. 

The following example configures radius to listen for the incoming requests on the default authentication port on the address 10.10.10.1 and on port 1645 on address 10.10.11.2. 

  listen 10.10.10.1, 10.10.11.2:1645;

 



forward statement 
This statement enables forwarding of the requests to the given set of servers. Forwarding is an experimental feature of GNU Radius, it differs from proxying in that the requests are sent to the remote server (or servers) and processed locally. The remote server is not expected to reply. 

This mode is intended primarily for debugging purposes. It could also be useful in some very complex and unusual configurations. 


Numeric statements 

port 
Sets the number of which UDP port to listen on for the authentication requests. 

max-requests 
Sets the maximum number of authentication requests in the queue. Any surplus requests will be discarded. 

time-to-live 
Sets the request time-to-live in seconds. The time-to-live is the time to wait for the completion of the request. If the request job isn't completed within this interval of time it is cleared, the corresponding child process killed and the request removed from the queue. 

request-cleanup-delay 
Sets the request cleanup delay in seconds, i.e. determines how long will the completed authentication request reside in the queue. 

password-expire-warning 
Sets the time interval for password expiration warning. If user's password expires within given number of seconds, radiusd will send a warning along with authentication-acknowledge response. Default is 0. 

Boolean statements 

detail 
When set to true, radiusd will produce the detailed log of each received packet in the file `radacct/nasname/detail.auth'. The format of such log files is identical to the format of detailed accounting files (see section 8.2 Detailed Request Accounting). 

strip-names 
Determines whether radiusd should strip any prefixes/suffixes off the username before logging. 

checkrad-assume-logged 
See section 5.1.11 mlc statement, for the description of this setting. It is accepted in auth for compatibility with previous versions of GNU Radius. 

trace-rules 
Enables tracing of the configuration rules that were matched during processing of each received authentication request. See section 10.1 Rule Tracing, for detailed information about this mode. 

reject-malformed-names 
Enables sending access-reject replies for the access-accept requests that contain an invalid value in User-Name attribute. By default such requests are discarded without answering. See the description of username-chars (see section Option statement). 


Character statement 

compare-attribute-flag 
The argument to this statement is a character from `1' through `9'. This statement modifies the request comparison method for authentication requests. See section 6.1 Extended Comparison, for a detailed description of its usage. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.4 acct statement 

Syntax: 
  acct {
        listen ( addr-list | no ); 
        forward addr-list ; 
        port number ; 
        detail bool; 
        system bool;
        max-requests number ; 
        time-to-live number ; 
        request-cleanup-delay number ; 
        compare-atribute-flag character ; 
        trace-rules bool ; 
} ;

 


Usage: 
The acct statement configures the parameters of the accounting service. 


listen statement 
This statement determines on which addresses radiusd will listen for incoming accounting requests. Its argument is a comma-separated list of items in the form ip:port-number. ip can be either an IP address in familiar "dotted-quad" notation or a hostname. :port-number part may be omitted, in which case the default accounting port is assumed. 

If the listen statement is omitted, radiusd will accept incoming requests from any interface on the machine. 

The special value no disables listening for accounting requests. 

The following example configures radius to listen for the incoming requests on the default accounting port on the address 10.10.10.1 and on port 1646 on address 10.10.11.2. 

  listen 10.10.10.1, 10.10.11.2:1646;

 



forward statement 
This statement enables forwarding of the requests to the given set of servers. Forwarding is an experimental feature of GNU Radius, it differs from proxying in that the requests are sent to the remote server (or servers) and processed locally. The remote server is not expected to reply. 

This mode is intended primarily for debugging purposes. It could also be useful in some very complex and unusual configurations. 


Numeric statements 

port 
Sets the number of which port to listen for the authentication requests. 

max-requests 
Sets the maximum number of accounting requests in the queue. Any surplus requests will be discarded. 

time-to-live 
Sets the request time-to-live in seconds. The time-to-live is the time to wait for the completion of the request. If the request job isn't completed within this interval of time it is cleared, the corresponding child process killed and the request removed from the queue. 

request-cleanup-delay 
Sets the request cleanup delay in seconds, i.e. determines how long will the completed account request reside in the queue. 


Boolean statements 

detail 
When set to no, disables detailed accounting (see section 8.2 Detailed Request Accounting). 

system 
When set to no, disables system accounting (see section 8.1 System Accounting). Notice, that this will disable simultaneous use checking as well, unless you supply an alternative MLC method (currently SQL, See section 7.9 Multiple Login Checking, for the detailed discussion of this). 

trace-rules 
Enables tracing of the configuration rules that were matched during processing of each received accounting request. See section 10.1 Rule Tracing, for detailed information about this mode. 

Character statement 

compare-attribute-flag 
The argument to this statement is a character from `1' through `9'. This statement modifies the request comparison method for authentication requests. See section 6.1 Extended Comparison, for a detailed description of its usage. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.5 usedbm statement 

Syntax: 
  usedbm ( yes | no ) ;

 


Usage 
The usedbm statement determines whether the DBM support should be enabled. 

no 
Do not use DBM support at all. 

yes 
Use only the DBM database and ignore `raddb/users'. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.6 snmp statement 

Syntax: 
  snmp {
        port portno ; 
        listen ( addr-list | no ); 
        max-requests number ; 
        time-to-live number ; 
        request-cleanup-delay number ; 
        ident string ; 
        community name ( rw | ro ) ; 
        network name network [ network ... ] ; 
        acl {
                allow network_name community_name ; 
                deny network_name ; 
        } ; 
        storage {
                file filename ; 
                perms number ; 
                max-nas-count number ; 
                max-port-count number ; 
        } ; 
};

 


Usage 
The snmp statement configures the SNMP service. 

listen statement 
The listen statement determines on which addresses radiusd will listen for incoming SNMP requests. The argument is a comma-separated list of items in the form ip:port-number. The ip can be either an IP address in familiar "dotted-quad" notation or a hostname. The :port-number part may be omitted, in which case the default SNMP port (161) is used. 

If the listen statement is omitted, radiusd will accept incoming requests from any interface on the machine. 

The special value no disables listening for SNMP requests. 

The following example configures radius to listen for the incoming SNMP requests on the default SNMP port on the address 10.10.10.1 and on port 4500 on address 10.10.11.2. 

  listen 10.10.10.1, 10.10.11.2:4500;

 



Numeric statements 

port 
Sets the number of which port to listen for the SNMP requests. 

max-requests 
Sets the maximum number of SNMP requests in the queue. Any surplus requests will be discarded. 

time-to-live 
Sets the request time-to-live in seconds. The time-to-live is the time to wait for the completion of the request. If the request job isn't completed within this interval of time it is cleared, the corresponding child process killed and the request removed from the queue. 

request-cleanup-delay 
Sets the request cleanup delay in seconds, i.e. determines how long will the completed SNMP request reside in the queue. 


String statements 

ident 
Sets the SNMP server identification string. 

Community and network definitions 

community name ( rw | ro ) 
Defines the community name as read-write (rw) or read-only (ro). 

network name network [ network ... ] 
Groups several networks or hosts under one logical network name. 


Access-Control List definitions 

allow network_name community_name 
allow hosts from the group network_name access to community community_name. 

deny NETWORK_NAME 
Deny access to SNMP service from any host in the group network_name. 

Storage control 
GNU Radius stores the SNMP monitoring data in an area of shared memory mapped to an external file. This allows all subprocesses to share this information and to accumulate the statistics across invocations of the daemon. 

The storage statement controls the usage of the storage for the SNMP data. 


file 
Sets the file name for the SNMP storage file. Unless the filename begins with a `/' it is taken as relative to the current logging directory. 

perms 
Sets the access permissions for the storage file. Notice, that this statement does not interpret its argument as octal by default, so be sure to prefix it with `0' to use an octal value. 

max-nas-count 
Sets maximum number of NASes the storage file is able to handle. Default is 512. Raise this number if you see the following message in your log file: 
  reached SNMP storage limit for the number of
monitored NASes: increase max-nas-count

 



max-port-count 
Sets maximum number of ports the storage file is able to handle. Default is 1024. Raise this number if you see the following message in your log file: 
  reached SNMP storage limit for the number of
monitored ports: increase max-port-count

 





--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.7 rewrite statement. 
(This message will disappear, once this node revised.) 

Syntax: 
  rewrite {
        stack-size number ; 
        load-path string ; 
        load string ; 
};

 


Numeric statements 

stack-size 
Configures runtime stack size for Rewrite. The number is the size of stack in words. The default value is 4096. 

String statements 
load-path 
Add specified pathname to the list of directories searched for rewrite files. 
load 
Loads the specified source file on startup. Unless string is an absolute pathname, it will be searched in directories set up by load-path statement. 

Loading 
The default load path is `RADDB':`DATADIR'/rewrite. <FIXME> Describe the loading process in detail. Also, some kind of autoloading is necessary for Rewrite. </> 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.8 guile statement 
(This message will disappear, once this node revised.) 
The guile statement allows to configure server interface with Guile. 


Syntax 
  guile {
        debug bool ; 
        load-path string ; 
        load string ; 
        load-module string [ string ... ] ; 
        eval expression [ expression ... ] ; 
        gc-interval number ; 
        outfile string ; 
};

 



Usage 

Boolean statements 

debug 
When set to yes, enables debugging evaluator and backtraces on Guile scripts. 

Numeric statements 

gc-interval 
Configures the forced garbage collections. By default the invocation of the garbage collector is run by the internal Guile mechanism. However, you may force Radius to trigger the garbage collection at fixed time intervals. The gc-interval statement sets such interval in seconds. 
For more information about Guile memory management system in general and garbage collections in particular, see section `Memory Management and Garbage Collection' in The Guile Reference Manual. 



String statements 
eval 
Evaluates its argument as Scheme expression. 

load-path 
Adds specified pathname to %load-path variable. 

load 
Loads the specified source file on startup. 

load-module 
Loads the specified Scheme module on startup. This statement takes an arbitrary number of arguments. The first argument specifies the name of the module to load, the rest of arguments is passed to the module initialization funtion. Module initialization function is a function named `module-init', where module is the module name. Arguments are converted using usual Guile rules, except that the ones starting with a dash (`-') are converted to keyword arguments. <FIXME> Describe the loading sequence in more detail. Why are modules preferred over plain SCM programs, etc. </> 

outfile 
Redirects the standard output and standard error streams of the Guile functions to the given file. Unless the filename starts with `/', it is taken relative to the current logging directory. 
See section 11.3 Guile, for a detailed description of Guile extensions interface. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.9 message statement 
The message statement allows to set up the messages that are returned to the user with authentication-response packets. 


Syntax 
  message {
        account-closed string ; 
        password-expired string ; 
        password-expire-warning string ; 
        access-denied string ; 
        realm-quota string ; 
        multiple-login string ; 
        second-login string ; 
        timespan-violation string ; 
};

 


All variables in message block take a string argument. In string you can use the usual C backslash notation to represent non-printable characters. The use of %C{} and %R{} sequences is also allowed (see section 5.14 Macro Substitution). 


String statements 
account-closed 
This message will be returned to the user whose account is administratively closed. 
password-expired 
This message will be returned to the user whose password has expired. 
password-expire-warning 
This is a warning message that will be returned along with an authentication-acknowledge packet for the user whose password will expire in less than n seconds. The value of n is set by password-expire-warning variable in auth block. See section 5.1.3 auth statement. In this string, you can use the %R{Password-Expire-Days} substitution, to represent the actual number of days left to the expiration date. The default is 
  Password Will Expire in %R{Password-Expire-Days} Days\r\n

 


access-denied 
This message is returned to the user who supplies an incorrect password or a not-existent user-name as his authentication credentials. 
realm-quota 
This message is returned when the user is trying to log in using a realm, and number of users that are currently logged in from this realm reaches maximum value. For a description of realms, see 3.4.2.2 Realms. 
multiple-login 
This message is returned to the user, who has logged in more than allowed number of times. For description of how to set the maximum number of concurrent logins, see 14.3.25 Simultaneous-Use. 
second-login 
This is a special case of multiple-login, which is used when the user's login limit is 1. 
timespan-violation 
This message is returned to the user who is trying to login outside of allowed time interval. For description of how to limit user's login time, see 14.3.14 Login-Time. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.10 filters statement 
The filters statement configures user-defined external filters. See section 11.1 Filters, for the detailed discussion of external filters. 


Syntax 
  filters {
        filter ident {
                exec-path path ;
                error-log filename ;
                common bool [max-wait];
                auth {
                        input-format fmt ;
                        wait-reply bool ;
                };
                acct {
                        input-format fmt ;
                        wait-reply bool ;
                };
        };
        ...
};

 


Each filter directive defines a new filter. The ident argument declares the name of the filter. This string must be used in Exec-Program-Wait or Acct-Ext-Program attributes to trigger invocation of this filter (see section 14.3.7 Exec-Program-Wait). 


Usage 

exec-path path 
Absolute path to the filter program. 

error-log filename 
Redirect error output from the filter program to filename. If the filename does not start with a slash, it is taken relative to the current logging directory (see section log-dir). 

auth 
acct 
These compound statements define authentication and accounting parts of this filter. Any one of them may be missing. The two statements allowed within auth and acct blocks are: 


input-format fmt 
Format of the input line for this filter. Usually this string uses %C{} notations (see section 5.14 Macro Substitution). 
You can also use the return value from a rewrite function as input line to the filter. To do so, declare: 

          input-format "=my_func()";

 


where my_func is the name of the rewrite function to invoke. The function must return string value. 


wait-reply bool 
If the filter prints a single line of output for each input line, set this to yes. Otherwise, if the filter produces no output, use wait-reply no. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.11 mlc statement 

Syntax 
  mlc {
        method (system|sql);  
        checkrad-assume-logged bool;
};

 



Usage 
Mlc statement configures multiple login checking subsystem (see section 7.9 Multiple Login Checking). 


method 
Sets the method of retrieving information about the currently open sessions. Currently two methods are implemented. Setting method to system will use system accounting database (see section 8.1 System Accounting). This is the default method. Setting it to sql will use SQL database. 

checkrad-assume-logged 
radiusd consults the value of this variable when the NAS does not responds to checkrad queries (see section 7.9 Multiple Login Checking). If this variable is set to yes, the daemon will proceed as if the NAS returned "yes", i.e. it will assume the user is logged in. Otherwise radiusd assumes the user is not logged in. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.2 Dictionary of Attributes -- `raddb/dictionary' 
The dictionary file `raddb/dictionary' defines the symbolic names for radius attributes and their values (see section 3.1 Attributes). The file consists of a series of statements, each statement occupies one line. 

In the detailed discussion below we use the following meta-syntactic characters: 


number 
Denotes a decimal, octal or hexagesimal number. Usual C conventions are honored, i.e. if number starts with `0x' or `0X' it is read as a hex number, if it starts with `0' it is read as an octal number, otherwise it is read as a decimal one. 
type 
Denotes an attribute type. These are valid attribute types: 

string 
A string type. 
integer 
An integer type. 
ipaddr 
IP address in a dotted-quad form. 
date 
A date in the format: "MON DD CCYY", where MON is the usual three-character abbreviation, DD is day of month (1-31), CCYY is the year, including the century. 

5.2.1 Comments    Introducing a comment line. 
5.2.2 $INCLUDE Statement    Include a file. 
5.2.3 VENDOR Statement    Define a vendor-id. 
5.2.4 ATTRIBUTE statement    Define an attribute translation. 
5.2.5 Blocks of Vendor-Specific Attributes    Blocks of vendor-specific attributes 
5.2.6 ALIAS statement    Define alternative name for an attribute. 
5.2.7 PROPERTY statement    Define attribute properties. 
5.2.8 VALUE Statement    Define a value translation. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.2.1 Comments 
Comments are introduced by a pound sign (`#'). Everything starting from the first occurrence of `#' up to the end of line is ignored. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.2.2 $INCLUDE Statement 

Syntax 
  $INCLUDE `filename'

 


Usage 
The $INCLUDE statement causes the contents of the file `filename' to be read in and processed. The file is looked up in the Radius database directory, unless its name starts with a slash. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.2.3 VENDOR Statement 

Syntax 
  VENDOR  vendor-name vendor-id

 


Usage 
A VENDOR statement defines the symbolic name vendor-name for vendor identifier vendor-id. This name can subsequently be used in ATTRIBUTE statements to define Vendor-Specific attribute translations. See section 14.1.26 Vendor-Specific. 

Example 
  VENDOR  Livingston  307

 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.2.4 ATTRIBUTE statement 
Syntax 
  ATTRIBUTE  name  number  type [vendor] [flags]

 


Usage 
The ATTRIBUTE statement defines the internal representation of an attribute: its symbolic name, data type and syntactical usage. Its parts have the following meaning: 

name 
The attribute name. 
number 
The attribute ID (number). 
type 
The attribute type. 
vendor 
Vendor name for vendor-specific attributes. For usual attributes this field is empty or contains a dash (`-'). The latter usage is for compatibility with previos version of GNU Radius 
flags 
Flags, defining attribute properties (see section 3.1 Attributes). 
The attribute property flags consist of a sequence of letters, whose meaning is determined by the following rules: (2) 


The attribute usage is described by three pairs of symbols, enclosed in square brackets. Each pair describes how the attribute can be used in each of three configuration files. The first pair corresponds to `raddb/users', the second one corresponds to `raddb/hints', and the third one corresponds to `raddb/huntgroups'. Within each pair, the letter `L' in first position means that the attribute is allowed in LHS of a rule. The letter `R' in second position means that the attribute is allowed in RHS of a rule. The absence of any of these letters is indicated by dash (`-'). Thus, the following usage specification: 
          [L--RLR]

 


means that the attribute may be used in LHS of a rule in `raddb/users', in RHS of a rule in `raddb/hints', and in both sides of a rule in `raddb/huntgroups'. 

The attribute additivity is described by one of the following letters: 
= 
Additivity = Replace 
+ 
Additivity = Append 
N 
Additivity = None 
The presence of letter `P' in property flags raises the propagation bit. 
Letter `l' (lower-case ell) enables logging the given attribute in detail file (see section 8.2 Detailed Request Accounting). This is meaningful only for internal attributes, i.e. the ones whose decimal value is greater than 255 (see section 14.3 Radius Internal Attributes). By default such attributes do not appear in detailed logs. The flag `l' reverts this behavior. 
Letter `E' marks attributes encrypted as described in RFC 2138. Currently these are User-Password and CHAP-Password. 
Letter `T' marks attribute encrypted according to RFC 2868. 
The characters from `1' to `9' denote nine user-defined flags (see section 6.1 Extended Comparison). 

Example 
  ATTRIBUTE  Service-Type  6 integer - [LR-RLR]=P 

 


This statement declares that the attribute number 6 will be referred to by the symbolic name `Service-Type'. The attribute is of integer data type and it may be used in any part of matching rules, except in LHS of a `raddb/hints' rule. The additivity of Service-Type is set to `Replace'. The attribute will be propagated through the proxy chain. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.2.5 Blocks of Vendor-Specific Attributes 

Syntax 
  BEGIN VENDOR vendor-name [vendor-id]
...
END 

 


Usage 
The BEGIN keyword marks start of the block of definitions of vendor-specific attributes. The block is terminated by END keyword, optionally followed by an arbitrary number of words, which are regarded as a comment. The block may contain any valid dictionary declarations, except other blocks: nesting of declaration blocks is not allowed. 
If vendor-id is absent, the value of vendor ID is looked up in the internal table of vendors; therefore, it must be defined before BEGIN statement (see section 5.2.3 VENDOR Statement). 

BEGIN--END block alters the handling of ATTRIBUTE statements within it. If ATTRIBUTE statement does not contain an explicit vendor-id specification, the value of vendor-id is used instead. 

For compatibility with FreeRadius an alternative syntax is also supported: 

  BEGIN-VENDOR vendor-name
...
END-VENDOR vendor-name

 


Such compatibility blocks must appear only ater the declaration of vendor-name (see section 5.2.3 VENDOR Statement). 


Example 
The following is the usual way of definig vendor-specific attributes: 

  VENDOR          Livingston      307

ATTRIBUTE       LE-Terminate-Detail     2       string  Livingston
ATTRIBUTE       LE-Advice-of-Charge     3       string  Livingston

 


The following two examples show the alternative ways:   VENDOR Livingston 307
BEGIN VENDOR Livingston
ATTRIBUTE       LE-Terminate-Detail     2       string  
ATTRIBUTE       LE-Advice-of-Charge     3       string
END

 


  BEGIN VENDOR Livingston 307
ATTRIBUTE       LE-Terminate-Detail     2       string  
ATTRIBUTE       LE-Advice-of-Charge     3       string
END

 


These three examples are completely equivalent to each other. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.2.6 ALIAS statement 
Syntax 
  ALIAS name  alt-name

 


Usage 
The ALIAS statement defines an altenative name alt-name for attribute name. The latter should already be defined, otherwise an error occurs. 

Example 
  ALIAS User-Password Password

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.2.7 PROPERTY statement 
Syntax 
  PROPERTY  name  flags
PROPERTY  name  +flags [-flags ...]

 


Usage 
The PROPERTY statement redefines property flags for attribute name. The attribute must be defined, otherwise an error occurs. The PROPERTY statement has two forms. In first form, it takes a single argument, representing new property flags for the attribute. In its second form it takes any number of arguments, each of them preceeded by `+' sign, inidicating addition of properties, or by `-' sign, indicating removal of these. 
See section 5.2.4 ATTRIBUTE statement, for the discussion of attribute property flags. 


Example 
The following example defines that the attribute User-Password may be used only on left-hand side of a `raddb/users' entry, and that it is transmitted in encrypted form. 

  PROPERTY  User-Password [L-----]E

 


Next example illustrates adding and removing attribute properties: 

  PROPERTY  My-Attrib     +P -=

 


it adds propagation bit (`P') and removes `replace' additivity from My-Attrib attribute. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.2.8 VALUE Statement 

Syntax 
  VALUE   Attribute-Translation       Value-Translation       number

 


Usage 
The VALUE statement assigns a translation string to a given value of an integer attribute. Attribute-Translation specifies the attribute and the Value-Translation specifies the name assigned to the value number of this attribute. 

Example 
The following assigns the translation string `Login-User' to the value 1 of the attribute `Service-Type'. 

  VALUE  Service-Type  Login-User  1

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.3 Clients List -- `raddb/clients' 
The `raddb/clients' lists NASes which are allowed to make authentication requests. As usual, the `#' character introduces a comment. Each record in the file consists of two fields, separated by whitespace. The fields are: 


NAS name 
Specifies a hostname or IP address of the NAS. 
Key 
Lists the encryption key shared between the server and this NAS. 
If the set of NASes share the same encryption key, there are two ways to list it in `raddb/clients'. First, if these NASes lie in a single network, you can specify this network address in NAS name field, e.g.: 

  10.10.10.0/27   seCRet

 


Notice also that specifying full netmask after the `/' character is also allowed, so that the above example could also be written as follows: 

  10.10.10.0/255.255.255.224   seCRet

 


Otherwise, the keyword DEFAULT may be used as NAS name. This notation will match any IP address, so it should be used with caution. 


5.3.1 Example of `clients' file    An example of clients file. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.3.1 Example of `clients' file 
  # This is a list of clients which are allowed to make authentication 
# requests.
# Each record consists of two fields:
#       i.  Valid hostname.
#       ii. The shared encryption key for this hostname. 
#
#Client Name            Key
#----------------       -------------------
myhost.dom.ain          guessme         
merlin                  emrys           
11.10.10.10             secRet

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.4 NAS List -- `raddb/naslist' 
The `raddb/naslist' file contains a list of NASes known to the Radius server. Each record in the file consist of the following four fields, the first two being mandatory, the last two being optional: 


NAS name 
Specifies either a hostname or IP address for a single NAS or a CIDR net block address for a set of NASes. The word `DEFAULT' may be used in this field to match any NAS. (3) 

Short Name 
This field defines a short name under which this NAS will be listed in logfiles. The short name is also used as a name of the subdirectory where the detailed logs are stored. 

Type 
Specifies the type of this NAS. Using this value radiusd determines the way to query NAS about the presence of a given user on it (see section 7.9 Multiple Login Checking). The two special types: `true' and `false', can be used to disable NAS querying. When the type field contains `true', radiusd assumes the user is logged in to the NAS, when it contains `false', radiusd assumes the user is not logged in. Otherwise, the type is used as a link to `nastypes' entry (see section 5.5 NAS Types -- `raddb/nastypes'). 
If this field is not present `true' is assumed. 


Arguments 
Additional arguments describing the NAS. Multiple arguments must be separated by commas. No intervening whitespace is allowed in this field. 
There are two groups of nas arguments: nas-specific arguments and nas-querying arguments. Nas-specific arguments are used to modify a behavior of radiusd when sending or receiving the information to or from a particular NAS. 

Nas-querying arguments control the way radiusd queries a NAS for confirmation of a user's session (see section 7.9 Multiple Login Checking). These arguments override the ones specified in `nastypes' and can thus be used to override the default values. 

The nas-specific arguments currently implemented are: 


broken_pass 
This is a boolean argument that controls the encryption of user passwords, longer than 16 octets. By default, radiusd uses method specified by RFC 2865. However some NASes, most notably MAX Ascend series, implement a broken method of encoding long passwords. This flag instructs radiusd to use broken method of password encryption for the given NAS. 

compare-auth-flag=flag 
Instructs radius to use attributes marked with a given user-defined flag when comparing authentication requests. It overrides compare-attribute-flag (see section 5.1.3 auth statement) for this particular NAS. See section 6.1 Extended Comparison, for a detailed description of its usage. 

compare-acct-flag=flag 
Instructs radius to use attributes marked with a given user-defined flag when comparing accounting requests. It overrides compare-attribute-flag (see section 5.1.4 acct statement) for this particular NAS. See section 6.1 Extended Comparison, for a detailed description of its usage. 
See section 3.4.1 Checking for Duplicate Requests, for general description of request comparison methods. 

For the list of nas-querying arguments, See section Full list of allowed arguments. 


5.4.1 Example of `naslist' file     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.4.1 Example of `naslist' file 
  # raddb/naslist: contains a list of Network Access Servers 
#
# Each record consists of following fields:
#
#       i.      A valid hostname or IP address for the client.
#       ii.     The short name to use in the logfiles for this NAS.
#       iii.    Type of device. Valid values are `true', `false' and
#               those defined in raddb/nastypes file.

# NAS Name              Short Name      Type
#----------------       ----------      ----
myhost.dom.ain          myhost          unix
merlin                  merlin          max 
11.10.10.10             arthur          livingston

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.5 NAS Types -- `raddb/nastypes' 
The `raddb/nastypes' file describes the ways to query NASes about active user sessions. 


5.5.1 Syntax of `raddb/nastypes'    Syntax described. 
5.5.2 Example of nastypes file.     
5.5.3 Standard NAS types    NAS types defined in standard nastypes file. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.5.1 Syntax of `raddb/nastypes' 
(This message will disappear, once this node revised.) 

Syntax 
Each record consists of three fields separated by any amount of whitespace. The fields are: 

Type 
Type of the NAS which is described in this record. 
Method 
Method to use to query a NAS of given type. 
Arguments 
Arguments to pass to this method. Each argument is a pair arg=value, where arg is its name and value is a value assigned to it. The list of predefined argument names follows. Note, that no intervening whitespace is allowed in this field. 

Methods 
Version 1.3 of GNU Radius supports following querying methods: finger, snmp, external and guile. <FIXME> Describe these fully </> . 


Arguments 
In the discussion below n means numeric and s string value. 

The following arguments are predefined: 


Common for all methods 

function=s 
Specifies the check function to use with this method (see section 11.2.5 Login Verification Functions). This argument must be present. For description of how this function is applied, see 7.9 Multiple Login Checking. 
port=n 
Use port number n instead of the default for the given method. 

Method snmp 

password=s 
Use community s instead of the default. This argument must be present. 
retries=n 
Retry n times before giving up. 
timeout=n 
Timeout n seconds on each retry. 

Method finger 

timeout=n 
Give up if the NAS does not respond within n seconds. 
notcp 
tcp=0 
Disable the use of T/TCP for hosts with a broken TCP implementation. 
arg=subst 
Send subst to finger, instead of username. subst must be one of macro variables, described below. 

Macro variables 
The following macro-variables are recognized and substituted when encountered in the value pair of an argument: <FIXME> Describe new syntax for extendable strings. Notice, that the use of old meta-characters is deprecated. </> 


`%u' 
Expands to username. 
`%s' 
Expands to session id. 
`%d' 
Expands to session id converted to decimal representation. 
`%p' 
Expands to port number. 
`%P' 
Expands to port number + 1. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.5.2 Example of nastypes file. 
Note, that in the following example the long lines are broken into several lines for readability. 

  # Type     Method          Args
# ----     ------          ----
unix       finger       function=check_unix
max-f      finger       function=check_max_finger
max        snmp         oid=.1.3.6.1.4.1.529.12.3.1.4.%d,
                        function=check_snmp_u
as5300-f   finger       function=check_as5300_finger
as5300     snmp         oid=.1.3.6.1.4.1.9.9.150.1.1.3.1.2.%d,
                        function=check_snmp_u
livingston snmp         oid=.1.3.6.1.4.1.307.3.2.1.1.1.5.%P,
                        function=check_snmp_s

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.5.3 Standard NAS types 
The `nastypes' shipped with version 1.3 of GNU Radius defines following NAS types: 


unix -- UNIX boxes running Finger 
This type suits for UNIX boxes running finger service able to return information about dial-up users active on them. To enable finger checking of a unix host add following to your `naslist' file:   #Hostname       Shortname   Type
#--------       ---------   ----
nas.name        T           unix

 


max-f -- MAX Ascend with Finger 
Use this type if you have MAX Ascend terminal server that answers finger queries. The `naslist' entry for such NAS will look like: 
  #Hostname       Shortname   Type  Flags
#--------       ---------   ----  -----
nas.name        T           max-f broken_pass

 


Note the use of broken_pass flag. It is needed for most MAX Ascend servers (see section 5.4 NAS List -- `raddb/naslist'). 


max -- MAX Ascend, answering SNMP 
Use this type if you have MAX Ascend terminal server that answers SNMP queries. The `naslist' entry for such NAS will look like: 
  #Hostname       Shortname   Type  Flags
#--------       ---------   ----  -----
nas.name        T           max-f broken_pass,community=comm

 


Replace comm with your actual SNMP community name. 


as5300-f -- Cisco AS5300 running finger 
as5300 -- Cisco AS5300 answering SNMP 
livingston -- Livingston Portmaster 
Type livingston queries portmaster using SNMP. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.6 Request Processing Hints -- `raddb/hints' 
The `raddb/hints' file is used to modify the contents of the incoming request depending on the username. For a detailed description of this, See section 3.4.3 Hints. 

The file contains data in Matching Rule format (see section 3.3 Matching Rule). 

Notice, that versions of GNU Radius up to 1.0 allowed to use only a subset of attributes in the check list of a `hints' entry, namely: 


Suffix 
Prefix 
Group 
User-ID 
This requirement has been removed in version 1.0. 


5.6.1 Example of `hints' file    An example of `hints' file. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.6.1 Example of `hints' file 
  ## If the username starts with `U', append the UUCP hint 
DEFAULT         Prefix = "U", Strip-User-Name = No
                Hint = "UUCP"
## If the username ends with `.slip', append the SLIP service data
## and remove the suffix from the user name.
DEFAULT         Suffix = ".slip",
                   Strip-User-Name = Yes
                Hint = "SLIP",
                   Service-Type = Framed-User,
                   Framed-Protocol = SLIP

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.7 Huntgroups -- `raddb/huntgroups' 
The `raddb/huntgroups' contains the definitions of the huntgroups. For a detailed description of huntgroup concept, See section 3.4.4 Huntgroups. 

The file contains data in Matching Rule format (see section 3.3 Matching Rule). 


5.7.1 Example of `huntgroups' file.    An example of the `huntgroups' file. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.7.1 Example of `huntgroups' file. 
  ## This defines the packet rewriting function for the server 11.10.10.11
DEFAULT NAS-IP-Address = 11.10.10.11, Rewrite-Function = "max_fixup"
        NULL

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.8 List of Proxy Realms -- `raddb/realms' 
The `raddb/realms' file lists remote Radius servers that are allowed to communicate with the local Radius server (see section 3.4.2 Proxying). 

Each record consists of up to three fields, separated by whitespace. Two of them are mandatory. The fields are: 


Realm name 
Specifies the name of the realm being defined, i.e. part of the login name after the `@' symbol. There are three special forms of this field. 
The name `NOREALM' defines the empty realm, i.e. lines marked with this name will match user names without any realm suffix. 

The name `DEFAULT' defines the default realm (see section 3.4.2.2 Realms). The lines with this realm name will match any user name, not matched by any other line in `raddb/realms'. 


Remote server list 
A comma-separated list of remote servers to which the requests for this realm should be forwarded. Each item in the list is: 

  servername[:auth-port[:acct-port]]

 


Optional auth-port and acct-port are the authentication and accounting port numbers. If acct-port is omitted, it is computed as auth-port + 1. If auth-port is omitted, the default authentication port number is used. 

The servers from this list are tried in turn until any of them replies or the list is exhausted, whichever occurs first. The timeout value and number of retries for each server are set via timeout and retry flags (see below). 

There may be cases where you would wish a particular realm to be served by the server itself. It is tempting to write 

  # Wrong!
realm.name      localhost

 


however, this will not work. The special form of the server list is provided for this case. It is the word `LOCAL'. The correct configuration line for the above case will thus be: 

  # Use this to declare a locally handled realm
realm.nam       LOCAL

 



Flags (optional) 
The flags meaningful in `raddb/realms' are 


ignorecase 
Boolean value. When set, enables case-insensitive comparison of realm names. For example, if a realm were defined as 
  myrealm.net     remote.server.net:1812  ignorecase

 


then user name `user@MyREAlm.NeT' will match this definition. 


strip 
Boolean value. Controls whether the realm name should be stripped off the username before forwarding the request to the remote server. Setting strip enables stripping, setting nostrip disables it. Default is to always strip user names. 

quota=num 
Set maximum number of concurrent logins allowed from this realm to the given value (num). 

timeout 
Number of seconds to wait for reply from the remote server before retransmitting the request. 

retries 
Number of attempts to connect a server. If the server does not respond after the last attempt, the next server from the list is tried. 

auth 
Proxy only authentication requests. 

acct 
Proxy only accounting requests. 

5.8.1 Example of `realms' file    An example of `realms' file. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.8.1 Example of `realms' file 

Example 1. 
  # Realm                 Remote server[:port]            flags
#----------------       ---------------------           --------
that.net                radius.that.net                 nostrip
dom.ain                 server.dom.ain:3000             strip,quota=20
remote.net              srv1.remote.net,srv2.remote.net 

 



Example 2. 
  # Realm                 Remote server[:port]            flags
#----------------       ---------------------           --------
NOREALM                 radius.server.net               
that.net                radius.that.net                 nostrip
dom.ain                 server.dom.ain:3000             strip,quota=20

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.9 User Profiles -- `raddb/users' 
File `raddb/users' contains the list of User Profiles. See section 3.4.5 User Profiles, for a description of its purpose. 


5.9.1 Example of `users' file    An example of `users' file. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.9.1 Example of `users' file 
  ## The following entry is matched when the user appends ``.ppp'' to his
## username when logging in.
## The suffix is removed from the user name, then the password is
## looked up in the SQL database.
## Users may log in at any time. They get PPP service.
DEFAULT Suffix = ".ppp",
                Auth-Type = SQL,
                Login-Time = "Al",
                Simultaneous-Use = 1,
                Strip-User-Name = Yes
        Service-Type = Framed-User,
                Framed-Protocol = PPP

## This is for SLIP users.
## This entry is matched when the auth request matches ``SLIP'' hint
DEFAULT Hint = "SLIP",
                Auth-Type = Mysql
        Service-Type = Framed-User
                Framed-Protocol = SLIP

## The following authenticates users using system passwd files.
## The users are allowed to log in from 7:55 to 23:05 on any weekday,
## except the weekend, and from 07:55 to 12:00 on Sunday.
## Only one login is allowed per user.
## The program telauth is used to further check the authentication
## information and provide the reply pairs
## Note the use of backslashes to split a long line.
DEFAULT Auth-Type = System,
                Login-Time = "Wk0755-2305,Su0755-1200",
                Simultaneous-Use = 1
        Exec-Program-Wait = "/usr/local/sbin/telauth \
                             %C{User-Name} \
                             %C{Calling-Station-Id} \
                             %C{NAS-IP-Address} \
                             %C{NAS-Port-Id}"

## This particular user is authenticated via PAM. He is presented a
## choice from `raddb/menus/menu1' file.
gray    Auth-Type = Pam
        Menu = menu1

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.10 List of Blocked Users -- `raddb/access.deny' 
The `raddb/access.deny' file contains a list of user names which are not allowed to log in via Radius. Each user name is listed on a separate line. As usual, the `#' character introduces an end-of-line comment. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.11 SQL Configuration -- `raddb/sqlserver' 
The `raddb/sqlserver' file configures the connection to SQL server. 

The file uses simple line-oriented `keyword -- value' format. Comments are introduced by `#' character. 

The `sqlserver' statements can logically be subdivided into following groups: SQL Client Parameters, configuring the connection between SQL client and the server, Authentication Server Parameters, Authorization Parameters, and Accounting server parameters. 


5.11.1 SQL Client Parameters     
5.11.2 Authentication Server Parameters     
5.11.3 Authorization Parameters     
5.11.4 Accounting Parameters     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.11.1 SQL Client Parameters 
These parameters configure various aspects of connection between SQL client and the server. 


interface iface-type 
Specifies the SQL interface to use. Currently supported values for iface-type are mysql and postgres. Depending on this, the default communication port number is set: it is 3306 for interface mysql and 5432 for interface postgres. Use of this statement is only meaningful when the package was configured with both `--with-mysql' and `--with-postgres' option. 
server string 
Specifies the hostname or IP address of the SQL server. 
port number 
Sets the SQL communication port number. It can be omitted if your server uses the default port. 
login string 
Sets the SQL user login name. 
password password 
Sets the SQL user password. 
keepopen bool 
Specify whether radiusd should try to keep the connection open. When set to no (the default), radiusd will open new connection before the transaction and close it right after finishing it. We recommend setting keepopen to yes for heavily loaded servers, since opening the new connection can take a substantial amount of time and slow down the operation considerably. 
idle_timeout number 
Set idle timeout in seconds for an open SQL connection. The connection is closed if it remains inactive longer that this amount of time. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.11.2 Authentication Server Parameters 
(This message will disappear, once this node revised.) 
These parameters configure the SQL authentication. The general syntax is: 


doauth bool 
When set to yes, enables authentication via SQL. All auth_ keywords are ignored if doauth is set to no. 

auth_db string 
Specifies the name of the database containing authentication information. 

auth_query string 
Specifies the SQL query to be used to obtain user's password from the database. The query should return exactly one string value -- the password. 

group_query string 
Specifies the query that retrieves the list of user groups the user belongs to. This query is used when Group or Group-Name attribute appears in the LHS of a user's or hint's profile. 

auth_success_query string 
This query is executed when an authentication succeeds. See section 7.10 Controlling Authentication Probes, for the detailed discussion of its purpose. 

auth_failure_query string 
This query is executed upon an authentication failure. See section 7.10 Controlling Authentication Probes, for the detailed discussion of its purpose. 


Example of Authentication Server Parameters 
Let's suppose the authentication information is kept in the tables passwd and groups. 

The passwd table contains user passwords. A user is allowed to have different passwords for different services. The table structure is: 

  CREATE TABLE passwd (
  user_name           varchar(32) binary default '' not null,
  service             char(16) default 'Framed-PPP' not null,
  password            char(64) 
);

 


Additionally, the table groups contains information about user groups a particular user belongs to. Its structure is: 

  CREATE TABLE groups (
  user_name           char(32) binary default '' not null,
  user_group          char(32) 
);

 


The queries used to retrieve the information from these tables will then look like: 

  auth_query  SELECT password
            FROM passwd
            WHERE user_name = '%C{User-Name}'
            AND service = '%C{Auth-Data}'

group_query SELECT user_group
            FROM groups
            WHERE user_name = '%C{User-Name}'

 


It is supposed, that the information about the particular service a user is wishing to obtain, will be kept in Auth-Data attribute in LHS of a user's profile. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.11.3 Authorization Parameters 
These parameters define queries used to retrieve the authorization information from the SQL database. All the queries refer to the authentication database. 


check_attr_query string 
This query must return a list of triplets: 
  attr-name, attr-value, opcode

 


The query is executed before comparing the request with the profile entry. The values returned by the query are added to LHS of the entry. opcode here means one of valid operation codes: `=', `!=', `<', `>', `<=', `>='. 


reply_attr_query string 
This query must return pairs: 
  attr-name, attr-value

 


The query is executed after a successful match, the values it returns are added to the RHS list of the matched entry, and are therefore returned to the NAS in the reply packet. 


Example of Authorization Parameters 
Suppose your attribute information is stored in a SQL table of the following structure: 

  CREATE TABLE attrib (
  user_name varchar(32) default '' not null,
  attr      char(32) default '' not null,
  value     char(128),
  op enum("=", "!=", "<", ">", "<=", ">=") default null
);

 


Each row of the table contains the attribute-value pair for a given user. If op field is NULL, the row describes RHS (reply) pair. Otherwise, it describes a LHS (check) pair. The authorization queries for this table will look as follows: 

  check_attr_query  SELECT attr,value,op \
                  FROM attrib \
                  WHERE user_name='%u' \
                  AND op IS NOT NULL

reply_attr_query  SELECT attr,value \
                  FROM attrib \
                  WHERE user_name='%u' \
                  AND op IS NULL

 


Now, let's suppose the `raddb/users' contains only one entry: 

  DEFAULT Auth-Type = SQL
        Service-Type = Framed-User   

 


And the attrib table contains following rows: 

user_name  attr  value  op 
jsmith  NAS-IP-Address  10.10.10.1   =  
jsmith  NAS-Port-Id  20  <= 
jsmith  Framed-Protocol  PPP  NULL 
jsmith  Framed-IP-Address  10.10.10.11  NULL 


Then, when the user jsmith is trying to authenticate, the following happens: 


Radius finds the matching entry (DEFAULT) in the `raddb/users'. 
It queries the database using the check_attr_query. The triplets it returns are then added to the LHS of the profile entry. Thus, the LHS will contain: 
  Auth-Type = SQL,
NAS-IP-Address = 10.10.10.1,
NAS-Port-Id <= 20

 


Radius compares the incoming request with the LHS pairs thus obtained. If the comparison fails, it rejects the authentication. Note that the Auth-Type attributes itself triggers execution of auth_query, described in the previous section. 
After a successful authentication, Radius queries the database, using reply_attr_query, and adds its return to the list of RHS pairs. The RHS pairs will then be: 
  Service-Type = Framed-User,
Framed-Protocol = PPP,
Framed-IP-Address = 10.10.10.11

 


This list is returned to the NAS along with the authentication accept packet. 

Thus, this configuration allows the user jsmith to use only NAS 10.10.10.1, ports from 1 to 20 inclusive. If the user meets these conditions, he is allowed to use PPP service, and is assigned IP address 10.10.10.11. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.11.4 Accounting Parameters 
To perform the SQL accounting radiusd needs to know the database where it is to store the accounting information. This information is supplied by the following statements: 


doacct bool 
When set to yes enables SQL accounting. All acct_ keywords are ignored if doacct is set to no. 
acct_db string 
Specifies the name of the database where the accounting information is to be stored. 
Further, radiusd needs to know which information it is to store into the database and when. Each of five accounting request types (see section 3.2.2 Accounting Requests) has a SQL query associated with it. Thus, when radius receives an accounting request, it determines the query to use by the value of Acct-Status-Type attribute. 

Following statements define the accounting queries: 


acct_start_query string 
Specifies the SQL query to be used when Session Start Packet is received. Typically, this would be some INSERT statement (see section 5.11.4.1 Writing SQL Accounting Query Templates). 
acct_stop_query string 
Specifies the SQL query to be used when Session Stop Packet is received. Typically, this would be some UPDATE statement. 
acct_stop_query string 
Specifies the SQL query to be executed upon arrival of a Keepalive Packet. Typically, this would be some UPDATE statement. 
acct_nasup_query string 
Specifies the SQL query to be used upon arrival of an Accounting Off Packet. 
acct_nasdown_query string 
Specifies the SQL query to be used when a NAS sends Accounting On Packet. 
None of these queries should return any values. 

Three queries are designed for use by multiple login checking mechanism (see section 7.9 Multiple Login Checking): 


mlc_user_query string 
A query retrieving a list of sessions currently opened by the given user. 
mlc_realm_query string 
A query to retrieve a list of sessions currently open for the given realm. 
mlc_stop_query string 
A query to mark given record as hung. 

5.11.4.1 Writing SQL Accounting Query Templates    Writing SQL accounting query templates. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.11.4.1 Writing SQL Accounting Query Templates 
Let's suppose you have an accounting table of the following structure: 

  CREATE TABLE calls (
  status              int(3),
  user_name           char(32),
  event_date_time     datetime DEFAULT '0000-00-00 00:00:00' NOT NULL,
  nas_ip_address      char(17),
  nas_port_id         int(6),
  acct_session_id     char(16) DEFAULT '' NOT NULL,
  acct_session_time   int(11),
  acct_input_octets   int(11),
  acct_output_octets  int(11),
  connect_term_reason int(4),
  framed_ip_address   char(17),
  called_station_id   char(32),
  calling_station_id  char(32)
);

 


On receiving the Session Start Packet we would insert a record into this table with status set to 1. At this point the columns acct_session_time, acct_input_octets, acct_output_octets as well as connect_term_reason are unknown, so we will set them to 0: 

  # Query to be used on session start
acct_start_query     INSERT INTO calls \
                     VALUES(%C{Acct-Status-Type},\
                            '%u',\
                            '%G',\
                            '%C{NAS-IP-Address}',\
                            %C{NAS-Port-Id},\
                            '%C{Acct-Session-Id}',\
                            0,\
                            0,\
                            0,\
                            0,\
                            '%C{Framed-IP-Address}',\
                            '%C{Called-Station-Id}',\
                            '%C{Calling-Station-Id}')

 


Then, when the Session Stop Packet request arrives we will look up the record having status = 1, user_name matching the value of User-Name attribute, and acct_session_id matching that of Acct-Session-Id attribute. Once the record is found, we will update it, setting 

  status = 2
acct_session_time = value of Acct-Session-Time attribute
acct_input_octets = value of Acct-Input-Octets attribute
acct_output_octets = value of Acct-Output-Octets attribute
connect_term_reason = value of Acct-Terminate-Cause attribute

 


Thus, every record with status = 1 will represent the active session and every record with status = 2 will represent the finished and correctly closed record. The constructed acct_stop_query is then: 

  # Query to be used on session end
acct_stop_query      UPDATE calls \
                     SET status=%C{Acct-Status-Type},\
                         acct_session_time=%C{Acct-Session-Time},\
                         acct_input_octets=%C{Acct-Input-Octets},\
                         acct_output_octets=%C{Acct-Output-Octets},\
                         connect_term_reason=%C{Acct-Terminate-Cause} \
                     WHERE user_name='%C{User-Name}' \
                     AND status = 1 \
                     AND acct_session_id='%C{Acct-Session-Id}' 

 


Upon receiving a Keepalive Packet we will update the information stored with acct_start_query: 

  acct_alive_query  UPDATE calls \
                  SET acct_session_time=%C{Acct-Session-Time},\
                      acct_input_octets=%C{Acct-Input-Octets},\
                      acct_output_octets=%C{Acct-Output-Octets},\
                      framed_ip_address=%C{Framed-IP-Address} \
                  WHERE user_name='%C{User-Name}' \
                  AND status = 1 \
                  AND acct_session_id='%C{Acct-Session-Id}'

 


Further, there may be times when it is necessary to bring some NAS down. To correctly close the currently active sessions on this NAS we will define a acct_nasdown_query so that it would set status column to 2 and update acct_session_time in all records having status = 1 and nas_ip_address equal to IP address of the NAS. Thus, all sessions on a given NAS will be closed correctly when it brought down. The acct_session_time can be computed as difference between the current time and the time stored in event_date_time column: 

  # Query to be used when a NAS goes down, i.e. when it sends 
# Accounting-Off packet
acct_nasdown_query UPDATE calls \
                   SET status=2,\
                       acct_session_time=unix_timestamp(now())-\
                               unix_timestamp(event_date_time) \
                   WHERE status=1 \
                   AND nas_ip_address='%C{NAS-IP-Address}'

 


We have not covered only one case: when a NAS crashes, e.g. due to a power failure. In this case it does not have a time to send Accounting-Off request and all its records remain open. But when the power supply is restored, the NAS will send an Accounting On packet, so we define a acct_nasup_query to set status column to 3 and update acct_session_time in all open records belonging to this NAS. Thus we will know that each record having status = 3 represents a crashed session. The query constructed will be: 

  # Query to be used when a NAS goes up, i.e. when it sends 
# Accounting-On packet
acct_nasup_query   UPDATE calls \
                   SET status=3,\
                       acct_session_time=unix_timestamp(now())-\
                               unix_timestamp(event_date_time) \
                   WHERE status=1 \
                   AND nas_ip_address='%C{NAS-IP-Address}'

 


If you plan to use SQL database for multiple login checking (see section 7.9 Multiple Login Checking), you will have to supply at least two additional queries for retrieving the information about currently active sessions for a given user and realm (see section 7.9.1 Retrieving Session Data). Each of these queries must return a list consisting of 5-element tuples: 

  user-name, nas-ip-address, nas-port-id, acct-session-id

 


For example, in our setup these queries will be: 

  mlc_user_query SELECT user_name,nas_ip_address,\
                      nas_port_id,acct_session_id \
               FROM calls \
               WHERE user_name='%C{User-Name}' \
               AND status = 1

mlc_realm_query SELECT user_name,nas_ip_address,\
                       nas_port_id,acct_session_id \
                FROM calls \
                WHERE realm_name='%C{Realm-Name}'     

 


While performing multiple login checking radiusd will eventually need to close hung records, i.e. such records that are marked as open in the database (status=1, in our setup), but are actually not active (See section 7.9.2 Verifying Active Sessions, for the description of why it may be necessary). It will by default use acct_stop_query for that, but it has a drawback that hung records will be marked as if they were closed correctly. This may not be suitable for accounting purposes. The special query mlc_stop_query is provided to override acct_stop_query. If we mark hung records with status=4, then the mlc_stop_query will look as follows: 

  mlc_stop_query UPDATE calls \
               SET status=4,\
                acct_session_time=unix_timestamp(now())-\
                                  unix_timestamp(event_date_time) \
               WHERE user_name='%C{User-Name}' \
                 AND status = 1 \
                 AND acct_session_id='%C{Acct-Session-Id}' 

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.12 Rewrite functions -- `raddb/rewrite' 
The file `raddb/rewrite' contains definitions of Rewrite extension functions. For information regarding Rewrite extension language See section 11.2 Rewrite. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.13 Login Menus -- `raddb/menus' 
The menus is a way to allow user the choice between various services he could be provided. The menu functionality is enabled when Radius is compiled with `--enable-livingston-menus' option. 

A user is presented a menu after it is authenticated if the RHS of his profile record consists of a single A/V pair in the form: 

  Menu = <menu-name>

 



The menu files are stored in directory `raddb/menus'. 


5.13.1 A menu file syntax.     
5.13.2 An example of menu files     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.13.1 A menu file syntax. 
A menu file is a text file containing a menu declaration and any number of choice descriptions. The menus can be nested to an arbitrary depth. 

A comment is introduced by a `#' character. All characters from this one up to the end of line are discarded. 

The menu declaration is contained between the words `menu' and `end'. Each of these must be the only word on a line and must start in column 1. 

Choice descriptions follow the menu declaration. Each description starts with a line containing choice identifier. A choice identifier is an arbitrary word identifying this choice, or a word `DEFAULT'. It is followed by comma-separated list of A/V pairs which will be returned to the server when a user selects this choice. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.13.2 An example of menu files 

Single-Level Menu 
Suppose the following file is stored under `raddb/menus/menu1': 

  menu
        *** Welcome EEE user! ***
Please select an option:

        1. Start CSLIP session
        2. Start PPP session
        3. Quit

        Option:
end
# CSLIP choice
# Framed-IP-Address of 255.255.255.254 indicates that the NAS should
# select an address for the user from its own IP pool.
1
        Service-Type = Framed-User,
        Framed-Protocol = SLIP,
        Framed-IP-Address = 255.255.255.254,
        Termination-Menu = "menu1"
# PPP choice
2
        Service-Type = Framed-User,
        Framed-Protocol = PPP,
        Framed-IP-Address = 255.255.255.254,
        Termination-Menu = "menu1"
# A special menu EXIT means abort the session
3
        Menu = "EXIT"
# Return to this menu if no valid choice have been entered 
DEFAULT
        Menu = "menu1"

 


Now, suppose the `raddb/users' contains the following profile entry: 

  DEFAULT Auth-Type = System
        Menu = "menu1"

 


and user `jsmith' has a valid system account and tries to log in from some NAS. Upon authenticating the user, the Radius server sees that his reply pairs contain the Menu attribute. Radius then sends Access-Challenge packet to the NAS with the text of the menu in it. The `jsmith' then sees on his terminal: 

          *** Welcome EEE user! ***
Please select an option:

        1. Start CSLIP session
        2. Start PPP session
        3. Quit

        Option:

 
He then enters `2'. The NAS sends the Access-Request packet to the server, which sees that user wishes to use option 2 and replies to the NAS with an Access-Accept packet containing the following attributes: 

          Service-Type = Framed-User,
        Framed-Protocol = PPP,
        Framed-IP-Address = 255.255.255.254,
        Termination-Menu = "menu1"

 


The Termination-Menu in this list makes sure the same process will continue when `jsmith' logs out, i.e. he will be presented the same menu again until he enters choice `3' which will disconnect him. 


Nested menus 
In this example, the `other' choice refers to the menu above. 

  menu
        *** Welcome here! ***
Please enter an option:
        ppp     ---     Start PPP session
        telnet  ---     Begin guest login session
        other   ---     Select other option

        Enter your choice:
end
ppp
        Service-Type = Framed-User,
        Framed-Protocol = PPP
telnet
        Service-Type = Login-User,
        Login-IP-Host = 10.11.11.7,
        Login-Service = Telnet,
        Login-TCP-Port = 23
other
        Menu = "menu1"
DEFAULT
        menu = "menu2"

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.14 Macro Substitution 
Some statements in the configuration files need to use the actual values of the attributes supplied with the request. These are: 


Exec-Program and Exec-Program-Wait assignments in `users' database 
SQL query templates in `sqlserver' 
In these statements the following macros are replaced by the value of corresponding attributes: 


%Cnum 
(num is a decimal number). This variable is replaced by the value of attribute number `num'. The attribute is looked up in the incoming request pairlist. 
%C{attr-name} 
This is replaced by the value of attribute named `attr-name'. The attribute is looked up in the incoming request pairlist. 
%Rnum 
(num is a decimal number). This variable is replaced by the value of attribute number `num'. The attribute is looked up in the reply pairlist. 
%R{attr-name} 
This is replaced by the value of attribute named `attr-name'. The attribute is looked up in the reply pairlist. 
%D 
This is replaced by current date/time (localtime). 
%G 
This is replaced by current date/time in GMT. 
The exact substitution procedure varies depending on the type of the attribute referenced by macro. If the attribute is of string or date type, radiusd first checks if the resulting substitution should be quoted. It does so by looking at the character immediately preceeding `%'. If it is a single or double quote, then radiusd assumes the macro must be quoted and replaces it by an appropriately modified attribute value. The purpose of the modification is to ensure that no characters within the expanded string would conflict with the quoting characters. In particular, radiusd searches the attribute value for any of the characters `\', `'', `"' and prepends a `\' to any occurrence of these. For example, suppose that attribute NAS-Identifier has the value `A "new" host'. Then: 

  nasid=%C{NAS-Identifier} ==> nasid=A "new" host
nasid="%C{NAS-Identifier}" ==> nasid="A \"new\" host"
nasid=%\C{NAS-Identifier} ==> nasid=A \"new\" host

 


The last example illustrates the use of backslash character to force string quoting in the absense of explicit quotation marks. 

If an integer attribute reference is quoted, radiusd looks up the string translation of its value in the dictionary (see section 5.2.8 VALUE Statement) and uses this string as a replacement. If no translation is found, the numeric value is used. The following example assumes that the value of Acct-Terminate-Cause attribute is 10: 

  reason=%C{Acct-Terminate-Cause} ==> reason=10
reason='%C{Acct-Terminate-Cause}' ==> reason='NAS-Request'
reason=%\C{Acct-Terminate-Cause} ==> reason=NAS-Request

 


Again, a backslash after percent sign can be used to force dictionary lookup. 

<FIXME> The quoting rules are not flexible enough. For example, a string 'isn't it a string' may be produced, but backslash escape within a singly quoted string is useless in most implementations (e.g. in shell or SQL). Besides, quoting unconditionally replaces unprintable characters with their octal values, where the notion of "unprintable" is determined by the current locale. Is it always the right thing to do? </> 

The "`{}' form" allows to specify default value for the substitution. The default value will be used when no such attribute is encountered in the pairlist. The syntax for specifying the default value resembles that of shell environment variables. 

The substitution %C{attr-name:-defval} is expanded to the value of attr-name attribute, if it is present in the request pairlist, and to defval otherwise. For example: 

          %C{Acct-Session-Time:-0}

 


will return the value of Acct-Session-Time attribute or 0 if it doesn't exist in the request pairlist. 

<FIXME> Should the quoting rules apply for defval as well? I'd say they should... </> 

The substitution %C{attr-name:=defval} is expanded to the value of attr-name attribute. If this attribute is not present in the request pairlist, it will be created and assigned the value defval. E.g.: 

          %C{Acct-Session-Time:=0}

 


The substitution %C{attr-name:?message} is expanded to the value of attr-name attribute, if it is present. Otherwise the diagnostic message "attr-name: message" is issued to the log error channel, and string "message" is returned. 

The substitution %C{attr-name:+retval} is expanded to empty string if the attribute attr-name is present in the referenced pairlist. Otherwise it is expanded to retval. 

You can also use the following shortcuts: 


%p 
Port number 
%n 
NAS IP address 
%f 
Framed IP address 
%u 
User name 
%c 
Callback-Number 
%i 
Calling-Station-Id 
%t 
MTU 
%a 
Protocol (SLIP/PPP) 
%s 
Speed (Connect-Info attribute) 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

6. Request Comparison Methods 
The basic notions about comparison of the incoming requests and why it is necessary were given in 3.4.1 Checking for Duplicate Requests. This chapter concentrates on extended methods of request comparison and on the configuration issues. 


6.1 Extended Comparison     
6.2 Fine-Tuning the Request Queue     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

6.1 Extended Comparison 
The default comparison method may fail to recognize duplicate requests. if the originating NAS has modified the request authenticator or request identifier before retransmitting the request. If you happen to use such NASes, you will have to enable extended request comparison to compensate for their deficiencies. 

The extended request comparison consists in comparing the contents of both requests. However, blindly comparing each A/V pair from both requests won't work, since many attributes do change their values between successive retransmits. Therefore, radiusd uses only comparable attribute, i.e. a user-defined subset of such attributes that can safely be used in comparison. Thus, extended request comparison works as follows: 


The comparable attributes are extracted from each request. They form two sorted attribute lists. 
If lengths of both lists differ, the requests are considered different. 
Otherwise, the value of each A/V pair from the first list is compared against that of the corresponding A/V pair from the second list. If at least one A/V pair differs, then the requests are considered different. Notice, that values of Password and CHAP-Password are decoded prior to comparison. 
To use the extended comparison, follow the procedure below: 


Select user-defined attribute properties. 
The syntax of dictionary file allows for nine user-defined properties, denoted by characters `1' through `9'. You should select one of them to mark comparable attributes for authentication and another one to mark those for accounting. It is strongly suggested that you use PROPERTY statement in your main dictionary file (see section 5.2.7 PROPERTY statement), instead of modifying ATTRIBUTE statements in the underlying dictionary files. 

See section 5.2.4 ATTRIBUTE statement, for detailed description of attribute property flags. 


To enable the extended comparison for requests coming from any NAS, declare extended comparison flags in `raddb/config'. 
To enable the extended comparison for authentication requests, add to your auth block the statement 

          compare-attribute-flag flag;

 


The flag is the same symbol you used in the dictionary to mark comparable attributes for authentication. 

To enable the extended comparison for accounting requests, insert compare-attribute-flag statement into the acct block. 


To enable the extended comparison for requests coming from selected NASes, declare extended comparison flags in `raddb/naslist'. 
Add the following statement to the declaration of those NASes, that require using the extended comparison (in flags column): 

          compare-auth-flag=flag,compare-acct-flag=flag

 


See section 5.4 NAS List -- `raddb/naslist', for a description of naslist file syntax. 


6.1.1 An example of extended comparison configuration     
6.1.2 List of attributes that can be declared comparable.     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

6.1.1 An example of extended comparison configuration 
In this example configuration, the user-defined flag `1' marks authentication comparable attributes, and the flag `2' marks the accounting comparable attributes. 


`raddb/dictionary' 
  PROPERTY       User-Name               +12
PROPERTY       Password                +1
PROPERTY       NAS-Port-Id             +12
PROPERTY       State                   +1
PROPERTY       Called-Station-Id       +12
PROPERTY       Calling-Station-Id      +12
PROPERTY       Acct-Status-Type        +2
PROPERTY       Acct-Session-Id         +2
PROPERTY       Acct-Session-Time       +2

 



`raddb/config' 
          auth {
                max-requests 127;
                request-cleanup-delay 2;
                compare-attribute-flag 1;
        };
        acct {
                max-requests 127;
                request-cleanup-delay 2;
                compare-attribute-flag 2;
        };                

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

6.1.2 List of attributes that can be declared comparable. 
The following attributes can be declared as comparable: 


User-Name 
Password 
CHAP-Password 
NAS-Port-Id 
State 
Called-Station-Id 
Calling-Station-Id 
NAS-Identifier 
Acct-Status-Type 
Acct-Session-Id 
Acct-Session-Time 
User-UID 
User-GID 
Notice that this list is by no means an exhaustive one. Depending on a particular NAS other attributes may be safe to be used in comparisons, or, vice-versa, some attributes from this list may not be used. You should carefully analyze packets coming from your NAS before deciding which attributes to mark as comparable. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

6.2 Fine-Tuning the Request Queue 
As described in 3.4.1 Checking for Duplicate Requests, each request is added to the request queue when radiusd starts processing it and is removed from there a certain amount of time after its processing was finished. The configuration parameter request-cleanup-delay defines how long each already processed request is kept in the queue. Its value must be synchronized with the NAS settings. 

Each NAS allows to configure two parameters: 

Ntimeout 
The amount of time in seconds during which the NAS is waiting for a response from radius server. 
Nretries 
The number of times the NAS tries to re-send the request if it received no response from the radius server. 
Of course, these parameters are named differently for different makes of NASes. Refer to your NAS documentation to find out where these values are configured. 

In general, these parameters must satisfy the following relation: 

       request-cleanup-delay = Nretries * Ntimeout + const

 


where const is an empirical constant that depends on the average time of processing a single request. Usually its value lies between 0 and 10 seconds. 

For example, if the configuration of your NAS sets 

     Nretries = 3
   Ntimeout = 10

 


then your raddb/config should contain: 

  auth { 
        request-cleanup-delay 40;
};
acct { 
        request-cleanup-delay 40;
};

 


Notice the duplication of request-cleanup-delay: radiusd uses distinct values for authentication and accounting requests, however most existing NASes do not make such distinction. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7. Authentication 
An Authentication Type specifies which credentials the user is required to supply in order to be authenticated and where the user's authentication data are stored. It is defined by the value of Auth-Type attribute in LHS of a `users' entry. 


7.1 Accept Authentication Type    Accept unconditionally. 
7.2 Reject Authentication Type    Reject unconditionally. 
7.3 Local Password Authentication Type    Authenticate using plaintext password. 
7.4 Encrypted Password Authentication Type    Authenticate using MD5 encrypted password. 
7.5 System Authentication Type    Authenticate using system account. 
7.6 SQL Authentication Type    Authenticate using SQL. 
7.7 PAM Authentication Type    Authenticate using PAM. 
7.8 Defining Custom Authentication Types     
7.9 Multiple Login Checking    Checking for Simultaneous Logins. 
7.10 Controlling Authentication Probes     




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.1 Accept Authentication Type 
Accept is the simplest authentication type. Users with this authentication type will be authenticated successfully without checking any credentials. Actually this means that only username is required for authentication. 

This authentication type is used for each `users' entry, whose LHS contains 

  Auth-Type = Accept

 


This authentication type can be used for guest accounts, e.g. the following profile in `users': 

  guest   Auth-Type = Accept,
                Simultaneous-Use = 10
        Service-Type = Framed-User,
                Framed-Protocol = PPP

 


allows up to 10 simultaneous guest PPP accounts. To log in using such guest account it is sufficient to use username `guest' and any password. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.2 Reject Authentication Type 
The Reject authentication type causes the request to be rejected unconditionally. It can be used to disable a user account (For another method of disabling user accounts, see section 5.10 List of Blocked Users -- `raddb/access.deny'). 

This authentication type is used for each `users' entry, whose LHS contains 

  Auth-Type = Reject

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.3 Local Password Authentication Type 
The Local Password authentication type allows to keep plaintext user passwords. Although the use of this authentication type is strongly discouraged for security reasons, this is the only authentication type that can be used with CHAP authentication. 

There are two ways of using this authentication type 


Specifying Passwords in users File. 
To keep the plaintext passwords in `users' file, the profile entry must follow this pattern: 
  user-name  Auth-Type = Local,
                     User-Password = plaintext

 


The plaintext is the user's plaintext password. Obviously, user-name may not be DEFAULT nor BEGIN. 


Specifying Passwords in SQL Database. 
  user-name   Auth-Type = Local,
                      Password-Location = SQL

 


When the user is authenticated using such profile, its password is retrieved from the authentication database using auth_query. The configuration of SQL authentication is described in detail in 5.11.2 Authentication Server Parameters. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.4 Encrypted Password Authentication Type 
The Encrypted Password type allows to keep user's passwords encrypted via DES or MD5 algorithm. There are two ways of using this authentication type. 


Specifying Passwords in users File. 
  user-name  Auth-Type = Crypt-Local,
                     User-Password = crypt-pass

 


The Crypt-Password is a shortcut for the above notation: 

  user-name  Crypt-Password = crypt-pass

 



Specifying Passwords in SQL Database. 
  user-name   Auth-Type = Crypt-Local,
                      Password-Location = SQL

 


Using this profile, the user's password is retrieved from the authentication database using auth_query. The configuration of SQL authentication is described in detail on 5.11.2 Authentication Server Parameters. 

The shortcut for this notation is Auth-Type = SQL. 

In any case, the passwords used with this authentication type must be either DES or MD5 hashed. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.5 System Authentication Type 
The System authentication type requires that the user have a valid system account on the machine where the radius server is running. The use of this type is triggered by setting 

  Auth-Type = System

 


in the LHS of a `users' entry. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.6 SQL Authentication Type 
Setting Auth-Type = SQL or Auth-Type = Mysql in the LHS of a `users' entry is a synonym for 

  Auth-Type = Crypt-Local, Password-Location = SQL

 


and is provided as a shortcut and for backward compatibility with previous versions of GNU Radius. 

For description of SQL authentication, see 7.4 Encrypted Password Authentication Type. The configuration of SQL subsystem is described in 5.11 SQL Configuration -- `raddb/sqlserver'. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.7 PAM Authentication Type 
PAM authentication type indicates that a user should be authenticated using PAM (Pluggable Authentication Module) framework. The simplest way of usage is: 

  Auth-Type = PAM

 


Any user whose `users' profile contains the above, will be authenticated via PAM, using service name `radius'. If you wish to use another service name, set it using Auth-Data attribute, e.g.: 

  Auth-Type = PAM,
    Auth-Data = pam-service

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.8 Defining Custom Authentication Types 
The are three ways to define custom authentication types: 


Write a PAM module. 
Use a Guile procedure. 
Use an external program 
You can write a PAM module implementing the new authentication type. Then, specifying Auth-Type = PAM allows to apply it (see section 7.7 PAM Authentication Type). 

Alternatively, you may write a Scheme procedure implementing the new authentication type. To apply it, use Scheme-Procedure attribute in RHS. The Auth-Type = Accept can be used in LHS if the whole authentication burden is to be passed to the Scheme procedure. For example, if one wrote a procedure my-auth, to apply it to all users, one will place the following profile in his `users' file: 

  DEFAULT  Auth-Type = Accept
         Scheme-Procedure = "my-auth"

 


For a discussion of how to write Scheme authentication procedures, See section 11.3.2 Authentication with Scheme. 

The third way to implement your own authentication method is using an external program. This is less effective than the methods described above, but may be necessary sometimes. To invoke the program, use the following statement in the RHS of `users' entry: 

  Exec-Program-Wait = "progname args"

 


The progname must be the full path to the program, args --- any arguments it needs. The usual substitutions may be used in args to pass any request attributes to the program (see section 5.14 Macro Substitution). 

For a detailed description of Exec-Program-Wait attribute and an example of its use, see 14.3.7 Exec-Program-Wait. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.9 Multiple Login Checking 
The number of sessions a user can have open simultaneously can be restricted by setting Simultaneous-Use attribute in the user's profile LHS (see section 14.3.25 Simultaneous-Use). By default the number of simultaneous sessions is unlimited. 

When a user with limited number of simultaneous logins authenticates himself, Radius counts the number of the sessions that are already opened by this user. If this number is equal to the value of Simultaneous-Use attribute the authentication request is rejected. 

This process is run in several stages. First, Radius retrieves the information about currently opened sessions from one of its accounting databases. Then, it verifies whether all these sessions are still active. This pass is necessary since an open entry might be a result of missing Stop request. Finally, the server counts the sessions and compares their count with the value of Simultaneous-Use attribute. 

The following subsections address each stage in detail. 


7.9.1 Retrieving Session Data     
7.9.2 Verifying Active Sessions     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.9.1 Retrieving Session Data 
Radius retrieves the list of sessions currently opened by the user either from the system database (see section 8.1 System Accounting), or from the SQL database (see section 8.3 SQL Accounting). The system administrator determines which method to use. 

By default, system accounting database is used. Its advantages are simplicity and ease of handling. It has, however, a serious deficiency: the information is kept in the local files. If you run several radius servers, each of them has no easy way of knowing about the sessions initiated by other servers. 

This problem is easy to solve if you run SQL accounting (see section 8.3 SQL Accounting). In this case, each radius server stores the data in your SQL database and can easily retrieve them from there. 

To enable use of SQL database for multiple login checking, do the following: 

In your `raddb/config' file set: 

  mlc {
    method sql;
};

 


In your `raddb/sqlserver' file, specify the queries for retrieving the information about open sessions and, optionally, a query to close an existing open record. 

There are two queries for retrieving the information: mlc_user_query returns the list of sessions opened by the user, mlc_realm_query returns the list of sessions opened for the given realm. Each of them should return a list of 5-element tuples(4): 

  user-name, nas-ip-address, nas-port-id, acct-session-id

 


Here is an example of mlc_user_query and mlc_realm_query: 

  mlc_user_query SELECT user_name,nas_ip_address,\
                      nas_port_id,acct_session_id \
               FROM calls \
               WHERE user_name='%C{User-Name}' \
               AND status = 1

mlc_realm_query SELECT user_name,nas_ip_address,\
                       nas_port_id,acct_session_id \
                FROM calls \
                WHERE realm_name='%C{Realm-Name}'     

 


Apart from these two queries you may also wish to provide a query for closing a hung record. By default, radiusd will use acct_stop_query. If you wish to override it, supply a query named mlc_stop_query, for example: 

  mlc_stop_query UPDATE calls \
               SET status=4,\
                acct_session_time=unix_timestamp(now())-\
                                  unix_timestamp(event_date_time) \
               WHERE user_name='%C{User-Name}' \
                 AND status = 1 \
                 AND acct_session_id='%C{Acct-Session-Id}' 

 


See section 5.11.4.1 Writing SQL Accounting Query Templates, for detailed information on how to write these queries. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.9.2 Verifying Active Sessions 
Whatever database radiusd uses, an open entry in it does not necessary mean that the corresponding session is still being active. So, after retrieving the information about user sessions, Radius verifies on corresponding NASes whether these are actually active. 

For each entry in the session list, if its NAS acknowledges the session, the session count is incremented. Otherwise, such entry is marked as closed in the database and is not counted. 

There may also be cases when the NAS is unreachable due to some reasons. In such cases the Radius behavior is determined by the value of checkrad-assume-logged in `config' file auth statement (raddb/config). If the value is yes, Radius assumes the session is still active and increases the session count, otherwise it proceeds as if the NAS returned negative reply. 

To query a NAS, Radius first looks up its type and additional parameters in `naslist' file (see section 5.4 NAS List -- `raddb/naslist'). There are two predefined NAS types that cause Radius to act immediately without querying tne NAS: the special type `true' forces Radius to act as if the NAS returned 1, the type `false' forces it to act as if the NAS returned 0. If the type is neither of this predefined types, Radius uses it as a look up key into the `nastypes' file (see section 5.5 NAS Types -- `raddb/nastypes') and tries to retrieve an entry which has matching type. If such entry does not exist, Radius issues the error message and acts accordingly to the value of configuration variable checkrad-assume-logged. Otherwise, Radius determines the query method to use from the second field of this entry, and constructs method arguments by appending arguments from the `naslist' entry to those of nastypes entry. Note, that the former take precedence over the latter, and can thus be used to override default values specified in `nastypes'. 

Having determined the query method and its argument, Radius queries NAS and analyzes its output by invoking a user-supplied Rewrite function. The function to use is specified by the function= argument to the method. It is called each time a line of output is received from the NAS (for finger queries) or a variable is received (for SNMP queries). The process continues until the function returns 1 or the last line of output is read or a timeout occurs whichever comes first. 

If the user-function returns 1 it is taken to mean the user's session is now active at the NAS, otherwise, if it replies 0 or if the end of output is reached, it is taken to mean the user's session is not active. 

The syntax conventions for user-supplied functions are described in detail in 11.2.5 Login Verification Functions. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

7.10 Controlling Authentication Probes 
Authentication probe is an attempt of a user to use other user's account, by guessing his password. The obvious indication of an authentication probe is appearence of several consecutive authentication failures for the same user. Of course, if the intruder is given sufficient number of such probes he will sooner or later succeed in finding the actual password. The conventional method to prevent this from occurring is to keep failure counters for each user and to lock the account when its failure countrer reaches a predefined limit. Notice that a legitimate user may fail (sometimes even several times in sequence) in entering his password so, two important points should always be observed. First, failure counters record the number of consecutive authentication failures and they are reset after each successive authentication. Secondly, the maximum number of allowed consecutive failures should be set sufficiantly high. 

The version 1.3 offers two ways for controlling authentication probes: using external programs and using special SQL queries. 

To control authentication probes using external programs, use the combination of Exec-Program-Wait and Auth-Failure-Trigger. The program specified by Auth-Failure-Trigger is executed each time an authentication attempt failed. When both attributes are used together, the program invoked by Auth-Failure-Trigger can update the failure counter, and the one invoked by Exec-Program-Wait can compare the counter value with the predefined limit and reject authentication when both values become equal. Such approach is most useful in conjunction with BEGIN profile. 

Let's suppose the program `/sbin/check_failure' accepts a user name and returns 1 if the failure counter for this user has reached maximum allowed value. Otherwise it returns 0 and clears the counter. Another program, `/sbin/count_failure' increases failure counter value for the given user name. Assuming our basic authentication type is `PAM', the `raddb/users' file will look as follows: 

  BEGIN   NULL
        Exec-Program-Wait = "/sbin/check_failure  %C{User-Name}",
        Auth-Failure-Trigger = "/sbin/count_failure %C{User-Name}",
                Fall-Through = Yes

DEFAULT Auth-Type = PAM
        Service-Type = Framed-User,
                Framed-Protocol = PPP

[... Other profiles ...]                

 


The BEGIN profile will be executed before any other profile. It will add to the RHS Exec-Program-Wait and Auth-Failure-Trigger attributes and then radiusd will proceed to finding a matching profile (due to Fall-Through attribute). When such profile is found, the user will be authenticated according to the method set up by the profile's Auth-Type attribute. If authentication fails, `/sbin/count_failure' will be called and the user name passed to it as the argument. Otherwise, `/sbin/check_failure' will be invoked. 

To complete the example, here are working versions of both programs. Failure counters for each user name are kept in separate file in `/var/log/radius/fails' directory. Both programs are written in bash. 


The /sbin/count_failure program 
  #! /bin/bash

test $# -eq 1 || exit 1

MAXFAIL=8
REGDIR=/var/log/radius/fails

if [ -r "$REGDIR/$1" ]; then
  read COUNT < "$REGDIR/$1"
  COUNT=$((COUNT+1))
else
  COUNT=1
fi
echo $COUNT > "$REGDIR/$1"      
# End of /sbin/count_failure

 



The /sbin/check_failure program 
  #! /bin/bash

test $# -eq 1 || exit 1

MAXFAIL=8
REGDIR=/var/log/radius/fails

if [ -r "$REGDIR/$1" ]; then
  read COUNT < "$REGDIR/$1"
  if [ $COUNT -ge $MAXFAIL ]; then
    echo "Reply-Message=\"Too many login failures. Your account is locked\""
    exit 1
  else
    rm "$REGDIR/$1"
  fi
fi
exit 0

# End of check_failure

 


Another way of controlling authentication probes is by using SQL database to store failure counters. Two queries are provided for this purpose in `raddb/sqlserver' file: auth_success_query is executed upon each successful authentication, and auth_failure_query is executed upon each authentication failure. Both queries are not expected to return any values. One obvious purpose of auth_failure_query would be to update failure counters and that of auth_success_query would be to clear them. The auth_query or group_query should then be modified to take into account the number of authentication failures. 

The default SQL configuration GNU Radius is shipped with provides a working example of using these queries. Let's consider this example. 

First, we create a special table for keeping authentication failure counters for each user: 

  CREATE TABLE authfail (
  # User name this entry refers to
  user_name           varchar(32) binary default '' not null,
  # Number of successive authentication failures for this user
  count               int,
  # Timestamp when this entry was last updated
  time                datetime DEFAULT '1970-01-01 00:00:00' NOT NULL,
  # Create a unique index on user_name
  UNIQUE uname (user_name)
);

 


The query auth_fail_query will increment the value of count column for the user in question: 

  auth_failure_query UPDATE authfail \
                   SET count=count+1,time=now() \
                   WHERE user_name='%C{User-Name}'

 


The query auth_success_query will clear count: 

  auth_success_query UPDATE authfail \
                   SET count=0,time=now() \
                   WHERE user_name='%C{User-Name}'

 


Now, the question is: how to use this counter in authentication? The answer is quite simple. First, let's create a special group for all the users whose authentication failure counter has reached its maximum value. Let this group be called `*LOCKED_ACCOUNT*'. We'll add the following entry to `raddb/users': 

  DEFAULT Group = "*LOCKED_ACCOUNT*",
                Auth-Type = Reject
        Reply-Message = "Your account is currently locked.\n\
Please, contact your system administrator\n"

 


which will reject all such users with an appropriate reply message. 

The only thing left now is to rewrite group_query so that it returns `*LOCKED_ACCOUNT*' when authfail.count reaches its maximum value. Let's say this maximum value is 8. Then the following query will do the job: 

  group_query       SELECT user_group FROM groups \
                  WHERE user_name='%u' \
                  UNION \
                  SELECT CASE WHEN (SELECT count > 8 FROM authfail \
                                                 WHERE user_name='%u')
                         THEN '*LOCKED_ACCOUNT*' END

 


The default configuration comes with these queries commented out. It is up to you to uncomment them if you wish to use SQL-based control over authentication failures. 

Notice the following important points when using this approach: 


Your SQL server must support UNION. Earlier versions of MySQL lacked this support, so if you run MySQL make sure you run a reasonably new version (at least 4.0.18). 

Both auth_failure_query and auth_success_query assume the database already contains an entry for each user. So, when adding a new user to the database, make sure to insert an appropriate record into authfails table, e.g. 
  INSERT INTO authfail VALUES('new-user',0,now());

 





--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

8. Accounting 
By default GNU Radius supports three types of accounting. Any additional accounting methods can be defined using extension mechanisms. 

The accounting methods are applied to a request in a following sequence: 


System accounting 
Detailed request accounting 
SQL accounting 
Custom accounting 
Any method can be enabled or disabled. Thus, you can even disable them all, thereby disabling accounting altogether. 

Notice, that the multiple login checking scheme relies on accounting being enabled. By default it uses system accounting, but can also be configured to use SQL accounting. So, if you disable system accounting and still wish to use reliable multiple login checking, make sure you configure radiusd to use SQL for this purpose. See section 7.9 Multiple Login Checking, for the detailed information about the subject. 

If any accounting type in this sequence fails, the accounting is deemed to fail and all subsequent methods are not invoked. 


8.1 System Accounting    UNIX style utmp/wtmp accounting. 
8.2 Detailed Request Accounting    Detailed requests. 
8.3 SQL Accounting    Accounting to SQL server. 
8.4 Defining Custom Accounting Types     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

8.1 System Accounting 
Radius keeps files `radutmp' and `radwtmp' in its logging directory and stores the accounting data there. The utilities radwho and radlast can be used to list information about users' sessions. <FIXME> Should they work if other mlc method is used? </> 

This accounting method is enabled by default. To disable it, use system no statement in `raddb/config'. See section 5.1.4 acct statement, for more information. Please notice that disabling this authentication method will disable multiple login checking as well. Refer to 7.9 Multiple Login Checking, for the detailed discussion of this. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

8.2 Detailed Request Accounting 
Radius stores the detailed information about accounting packets it receives in files `radacct/nasname/detail' (see section 2. Naming Conventions), where nasname is replaced with the short name of the NAS from the `raddb/naslist' file (see section 5.4 NAS List -- `raddb/naslist'). 

By default, this accounting type is always enabled, provided that `radacct' directory exists and is writable (see section 2. Naming Conventions). To turn the detailed accounting off, use the detail statement in the `config' file. For more information about it, see 5.1.4 acct statement. 

The accounting detail files consist of a record for each accounting request. A record includes the timestamp and detailed dump of attributes from the packet, e.g.: 

  Fri Dec 15 18:00:24 2000
        Acct-Session-Id = "2193976896017"
        User-Name = "e2"
        Acct-Status-Type = Start
        Acct-Authentic = RADIUS
        Service-Type = Framed-User
        Framed-Protocol = PPP
        Framed-IP-Address = 11.10.10.125
        Calling-Station-Id = "+15678023561"
        NAS-IP-Address = 11.10.10.11
        NAS-Port-Id = 8
        Acct-Delay-Time = 0
        Timestamp = 976896024
        Request-Authenticator = Unverified

Fri Dec 15 18:32:09 2000
        Acct-Session-Id = "2193976896017"
        User-Name = "e2"
        Acct-Status-Type = Stop
        Acct-Authentic = RADIUS
        Acct-Output-Octets = 5382
        Acct-Input-Octets = 7761
        Service-Type = Framed-User
        Framed-Protocol = PPP
        Framed-IP-Address = 11.10.10.125
        Acct-Session-Time = 1905
        NAS-IP-Address = 11.10.10.11
        NAS-Port-Id = 8
        Acct-Delay-Time = 0
        Timestamp = 976897929
        Request-Authenticator = Unverified

 


Notice that radiusd always adds two pseudo-attributes to detailed listings. Attribute Timestamp shows the UNIX timestamp when radiusd has received the request. Attribute Request-Authenticator shows the result of checking the request authenticator. Its possible values are: 


Verified 
The authenticator check was successful. 

Unverified 
The authenticator check failed. This could mean that either the request was forged or that the remote NAS and radiusd do not agree on the value of the shared secret. 

None 
The authenticator check is not applicable for this request type. 
Notice also that the so-called internal attributes by default are not logged in the detail file. Internal attributes are those whose decimal value is greater than 255. Such attributes are used internally by radius and cannot be transferred via RADIUS protocol. Examples of such attributes are Fall-Through, Hint and Huntgroup-Name. See section 14.3 Radius Internal Attributes, for detailed listing of all internal attributes. The special attribute flag l (lower-case ell) may be used to force logging of such attributes (see section 5.2.4 ATTRIBUTE statement). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

8.3 SQL Accounting 
The SQL accounting method is enabled when Radius is configured with `--enable-sql' option and the `sqlserver' file in its configuration directory is properly set up (see section 5.11 SQL Configuration -- `raddb/sqlserver'). 

This version of GNU Radius (1.3) supports MySQL and PostgreSQL servers. It also supports ODBC, which can be used to build interfaces to another database management systems. 

With this accounting method enabled, radiusd will store the information about accounting requests in the configured SQL database. The accounting method is fully configurable: the Radius administrator defines both the types of requests to be accounted and the information to be stored into the database (see section 5.11 SQL Configuration -- `raddb/sqlserver'). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

8.4 Defining Custom Accounting Types 
If the built-in accounting methods do not meet your requirements, you can implement your own. There are two ways of doing so: 


Using a Guile procedure. 
Using an external program 
To use a Guile procedure for accounting, the name of the procedure must be specified as a value to the Scheme-Acct-Procedure attribute in the RHS list of a `hints' entry, e.g.: 

  DEFAULT NULL Scheme-Acct-Procedure = "my-acct"

 


For a detailed description of Scheme accounting procedures, see section 11.3.3 Accounting with Scheme. 

Another way of implementing your own accounting method is using an external program. This is less effective than the methods described above, but may be necessary sometimes. To invoke the program, use the following statement in the LHS of the `hints' entry: 

  Acct-Ext-Program = "progname args"

 


The progname must be the full path to the program, and args any arguments it needs. The usual substitutions may be used in args to pass any request attributes to the program (see section 5.14 Macro Substitution). 

For a detailed description of Acct-Ext-Program, see section 14.3.1 Acct-Ext-Program. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

9. Logging 
GNU Radius reports every event worth mentioning. The events are segregated by their severity level. Radius discerns the following levels (in order of increasing severity): 


Debug 
The debug messages (10.2 Debugging). 

Auth 
Under this level every authentication attempt is logged. This is enabled by setting   level auth;

 
in the category auth statement of the `config' file. 

Proxy 
Messages regarding proxy requests (see section 3.4.2 Proxying). 

Info 
Informational messages. 

Notice 
Normal, but significant conditions. 

Warning 
Warning conditions. These mean some deviations from normal work. 

Error 
Error conditions. Usually these require special attention. 

CRIT 
Critical conditions due to which Radius is no longer able to continue working. These require urgent actions from the site administrator. 

By default, all messages in all levels are output to the file `radlog/radius.log'. In addition, messages in level CRIT are also duplicated to the system console. These defaults can be overridden using logging statement in the `raddb/config' file. (See section logging statement, for the description of logging statement syntax; see section 2. Naming Conventions for information about the locations of different Radius configuration files.) 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

10. Problem Tracking 

10.1 Rule Tracing    Tracing rules. 
10.2 Debugging    Enabling full debugging information. 
10.3 Test Mode    Running radius in test mode. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

10.1 Rule Tracing 
If you have more than one entry in your `users' file it is not always obvious which of the entries were used for authentication. The authentication data flow becomes even harder to understand if there are some complex rules in the `hints' and `huntgroups' files. 

The rule tracing mode is intended to help you find out the exact order of the rules that each request matched during processing. The mode is toggled by trace-rules statement in auth or acct block of your `config' file. When rule tracing mode is on for a given type of requests, radiusd will display the data flow diagram for each processed request of this type. The diagram is output on info logging category, it represents the list of rules in reverse chronological order. Each rule is represented by its location in the form filename:line. To make the output more compact, if several rules appear in the same configuration file, their locations are listed as a comma-separated list of numbers after the file name. Furthermore, if the configuration files have the same path prefix, then only the first file name appears with the full prefix. 

Here is an example of trace rule diagram:   Oct 31 11:37:17 [28322]: Auth.info: (Access-Request foo 170 bar):
rule trace: /etc/raddb/users:157,22,3; huntgroups:72; hints:34

 


This diagram means, that the authentication request from server `foo' for user `bar' with ID 170 matched the following rules 

File name  Line number 
`/etc/raddb/hints'  34 
`/etc/raddb/huntgroups'  72 
`/etc/raddb/users'  3 
`/etc/raddb/users'  22 
`/etc/raddb/users'  157 


As a practical example, let's suppose you have the following setup. There are three classes of users: 


Users from group "root" are authenticated using system password database and get rlogin access to the server 192.168.10.1 
Users from group "staff" are also authenticated using system password database, but they are granted only telnet access to the server 192.168.10.2 
Finally, the rest of users is authenticated against SQL database and get usual PPP access. 
In addition, users from the first two classes are accounted using custom Scheme procedure staff-acct. 

The configuration files for this setup are showed below: 

Contents of `hints':   DEFAULT  Group = "root"
         Scheme-Acct-Procedure = "staff-acct",
                   Hint = "admin"

DEFAULT  Group = "staff"
         Scheme-Acct-Procedure = "staff-acct",
                   Hint = "staff"

 


Contents of file `users':   DEFAULT Auth-Type = SQL,
              Simultaneous-Use = 1
        Service-Type = Framed-User,
              Framed-Protocol = PPP

DEFAULT Hint = "admin",
             Auth-Type = System
        Service-Type = Login-User,
             Login-IP-Host = 192.168.0.1,              
             Login-Service = Rlogin
             
DEFAULT Hint = "staff",
              Auth-Type = System,
              Simultaneous-Use = 1
         Service-Type = Login-User,
              Login-IP-Host = 192.168.0.2,
              Login-Service = Telnet

 


Now, let's suppose that user `svp' is in the group `staff' and is trying to log in. However, he fails to do so and in radiusd logs you see: 

  Nov 06 21:25:24: Auth.notice: (Access-Request local 61 svp):
  Login incorrect [svp]

 


Why? To answer this question, you add to auth block of your `config' the statement 

  trace-rules yes;

 


and ask user `svp' to retry his attempt. Now you see in your logs: 

  Nov 06 21:31:24: Auth.notice: (Access-Request local 13 svp):
  Login incorrect [svp]
Nov 06 21:31:24: Auth.info: (Access-Request local 13 svp):
  rule trace: /etc/raddb/users:1, hints: 5

 


This means that the request for `svp' has first matched rule on the line 1 of file `hints', then the rule on line 1 of file `users'. Now you see the error: the entries in `users' appear in wrong order! After fixing it your `users' looks like: 

  DEFAULT Hint = "admin",
             Auth-Type = System
        Service-Type = Login-User,
             Login-IP-Host = 192.168.0.1,              
             Login-Service = Rlogin

DEFAULT  Hint = "staff",
              Auth-Type = System,
              Simultaneous-Use = 1
         Service-Type = Login-User,
              Login-IP-Host = 192.168.0.2,
              Login-Service = Telnet
             
DEFAULT Auth-Type = SQL,
              Simultaneous-Use = 1
        Service-Type = Framed-User,
              Framed-Protocol = PPP

 


Now, you ask `svp' to log in again, and see: 

  Nov 06 21:35:14: Auth.notice: (Access-Request local 42 svp):
  Login OK [svp]
Nov 06 21:35:14: Auth.info: (Access-Request local 42 svp):
  rule trace: /etc/raddb/users:7, hints: 5

 


Let's also suppose that user `plog' is not listed in groups "root" and "staff", so he is supposed to authenticate using SQL. When he logs in, you see in your logs: 

  Nov 06 21:39:05: Auth.notice: (Access-Request local 122 plog):
  Login OK [svp]
Nov 06 21:39:05: Auth.info: (Access-Request local 122 plog):
  rule trace: /etc/raddb/users:14

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

10.2 Debugging 
GNU Radius provides extensive debugging features. These are enabled either by the `--debug' (`-x') command line option to radiusd (see section 4. How to Start the Daemon.), or by the level statement in the debug category (see section logging statement). Both cases require as an argument a valid debug specification. 

A debug specification sets the module for which the debugging should be enabled and the debugging level. The higher the level is, the more detailed information is provided. The module name and level are separated by an equal sign. If the level is omitted, the highest possible level (100) is assumed. The module name may be abbreviated to the first N characters, in which case the first matching module is selected. Several such specifications can be specified, in which case they should be separated by commas. For example, the following is a valid debug specification:           proxy.c=10,files.c,config.y=1

 


It sets debug level 10 for module proxy.c, 100 for files.c, and 1 for config.y. 

The modules and debugging levels are subject to change from release to release. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

10.3 Test Mode 
Test mode is used to test various aspects of radius configuration, without starting the daemon. To enter test mode, run 

  radiusd -mt

 


You will see usual radiusd diagnostics and the following two lines: 

  ** TEST SHELL **
(radiusd) _

 


The string `** TEST SHELL **' indicates that radiusd has entered test mode, the string `(radiusd)' is the shell prompt, indicating that radiusd is waiting for your commands. 

The syntax of test shell command resembles that of Bourne shell: each command consists of a list of words separated by any amount of whitespace. Each word is either a sequence of allowed word characters (i.e. alphabetical characters, decimal digits, dashes and underscores), or any sequence of characters enclosed in a pair of double quotes. The very first word is a command verb, the rest of words are arguments to this command verb. A command verb may be used in its full form, in its abbreviated form (i.e. you may type only several first characters of the verb, the only condition being that they do not coincide with another command verb), or in it's short form. 

The first command you should know is help (or, in its short form, h). This command takes no arguments and displays the short summary of all the available commands. Here is an example of its output: 

  (radiusd) help
h       help                           Print this help screen
q       query-nas NAS LOGIN SID PORT [IP]
                                       Query the given NAS
g       guile                          Enter Guile
rs      rewrite-stack [NUMBER]         Print or set the Rewrite
                                       stack size
r       run-rewrite FUNCTION(args..)   Run given Rewrite function
s       source FILE                    Source the given Rewrite file
t       timespan TIMESPAN [DOW [HH [MM]]]
                                       Check the timespan interval
d       debug LEVEL                    Set debugging level
rd      request-define [PAIR [,PAIR]]  Define a request
rp      request-print                  Print the request
quit    quit                           Quit the shell

 


Each line of the output consists of three fields. The first field shows the short command form. The second one lists its full form and its arguments, optional arguments being enclosed in square brackets. The third field contains short textual description of the command. 


Test Shell Command: query-nas nas login sid port [ip] 
Test Shell Abbreviation: q 
Queries the given NAS about the session described by its arguments. This command is useful in testing simultaneous login verification (see section 7.9 Multiple Login Checking. Its arguments are 


nas 
Specifies the NAS to query. It cn be its short name as defined in `raddb/naslist', or its fully qualified domain name, or its IP address. 
login 
Name of the user whose session should be verified. 
sid 
Session ID. 
port 
Port number on the NAS. 
ip 
Framed IP address, assigned to the user. 
The command displays the following result codes: 


0 
The session is not active. 
1 
The session is active 
-1 
Some error occurred. 

Test Shell Command: guile 
Test Shell Abbreviation: g 
Enter Guile shell. The command is only available if the package has been compiled with Guile support. For more information, See section 11.3 Guile. 


Test Shell Command: rewrite-stack [number] 
Test Shell Abbreviation: rs 
Prints or sets the Rewrite stack size. 


Test Shell Command: run-rewrite function(args ...) 
Test Shell Abbreviation: r 
Runs given Rewrite function and displays its return value. Function arguments are specified in the usual way, i.e. as a comma-separated list of Rewrite tokens. 

If the function being tested operates on request contents (see section 11.2.4 Rewriting Incoming Requests), you may supply the request using request-define command (see below). 



Test Shell Command: source file 
Test Shell Abbreviation: s 
Reads and compiles ("source") the given Rewrite file. The command prints `0' if the file was compiled successfully. Otherwise, it prints `1' and any relevant diagnostics. 


Test Shell Command: timespan timespan [day-of-week [hour [minutes]]] 
Test Shell Abbreviation: t 
Checks whether the given time falls within the timespan interval. Its first argument, timespan, contains the valid radiusd timespan specification (see section 14.3.14 Login-Time). Rest of arguments define the time. If any of these is omitted, the corresponding value from current local time is used. 


day-of-week 
Ordinal day of week number, counted from 0. I.e.: Sunday -- 0, Monday -- 1, etc. 
hour 
Hours counted from 0 to 24. 
minutes 
Minutes. 
The following set of samples illustrates this command: 

  (radiusd) timespan Wk0900-1800
ctime: Tue Dec  2 16:08:47 2003
inside Wk0900-1800: 6720 seconds left

(radiusd) timespan Wk0900-1800 0
ctime: Sun Nov 30 16:09:03 2003
OUTSIDE Wk0900-1800: 60660 seconds to wait

(radiusd) timespan Wk0900-1800 0 12 30
ctime: Sun Nov 30 12:30:13 2003
OUTSIDE Wk0900-1800: 73800 seconds to wait

(radiusd) timespan Wk0900-1800 1 05 00
ctime: Mon Dec  1 05:00:33 2003
OUTSIDE Wk0900-1800: 14400 seconds to wait

(radiusd) timespan Wk0900-1800 1 09 10
ctime: Wed Jan  7 22:09:41 2004
OUTSIDE Wk0900-1800: 39060 seconds to wait

(radiusd) timespan Wk0900-1800 1 09 10
ctime: Mon Dec  1 09:10:44 2003
inside Wk0900-1800: 31800 seconds left

(radiusd) 

 




Test Shell Command: debug level 
Test Shell Abbreviation: d 
Set debugging level. Level is any valid debug level specification (see section 10.2 Debugging). 


Test Shell Command: request-define [pair [,pair]] 
Test Shell Abbreviation: rd 
Define a request for testing Rewrite functions. The optional arguments are a comma-separated list of A/V pairs. If they are omitted, the command enters interactive mode, allowing you to enter the desired A/V pairs, as in the following example: 

  (radiusd) request-define
Enter the pair list. End with end of file
[radiusd] User-Name = smith, User-Password = guessme
[radiusd] NAS-IP-Address = 10.10.10.1
[radiusd] NAS-Port-Id = 34
[radiusd] 
(radiusd) 

 


Notice that any number of A/V pairs may be specified in a line. To finish entering the request, either type an EOF character or enter an empty line. 



Test Shell Command: request-print 
Test Shell Abbreviation: rp 
Prints the request, defined by request-define. 

  (radiusd) request-print
    User-Name = (STRING) smith
    User-Password = (STRING) guessme
    NAS-IP-Address = (IPADDR) 10.10.10.1
    NAS-Port-Id = (INTEGER) 34
(radiusd) 

 




Test Shell Command: quit 
Immediately quits the shell. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11. Extensions 
The use of extension language allows extending the functionality of GNU Radius without having to modify its source code. The two extension languages supported are Rewrite and Scheme. Use of Rewrite is always enabled. Use of Scheme requires Guile version 1.4 or higher. 


11.1 Filters    Using external filter programs. 
11.2 Rewrite    The built-in extension language. 
11.3 Guile    Using Scheme. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.1 Filters 
The simplest way to extend the functionality of Radius is to use filters. A filter is an external program that communicates with Radius via its standard input and output channels. 


11.1.1 Getting Acquainted with Filters     
11.1.2 Declaring the Filter     
11.1.3 Invoking the Filter from a User Profile     
11.1.4 Adding Reply Attributes     
11.1.5 Accounting Filters     
11.1.6 Invoking the Accounting Filter     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.1.1 Getting Acquainted with Filters 
Suppose we wish to implement an authentication method based on the user name and the user's calling station ID. We have a database of user names with valid IDs, and the new method should authenticate a user only if the combination {user_name, id} is found in this database. 

We write a filter program that reads its standard input line by line. Each input line must consist of exactly two words: the user name and the calling station ID. For each input line, the program prints 0 if the {user_name, id} is found in the database and 1 otherwise. Let's suppose for the sake of example that the database is a plaintext file and the filter is written in a shell programming language. Then it will look like 

  #! /bin/sh

DB=/var/db/userlist

while read NAME CLID
do
    if grep "$1:$2" $DB; then
        echo "0"
    else
        echo "1"
    fi
done

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.1.2 Declaring the Filter 
Here is how this filter is declared in the `raddb/config' file: 

  filters {
    filter check_clid {
        exec-path "/usr/libexec/myfilter";
        error-log "myfilter.log";
        auth {
            input-format "%C{User-Name}
            %C{Calling-Station-Id}";
            wait-reply yes;
        };
    };        
};                        

 


Let's analyze this declaration line by line: 


filters { 
This keyword opens the filters declaration block. The block may contain several declarations. 


filter check_clid { 
This line starts the declaration of this particular filter and names it `check_clid'. 


exec-path "/usr/libexec/myfilter"; 
This line tells radiusd where to find the executable image of this filter. 


error-log "myfilter.log"; 
The diagnostic output from this filter must be redirected to the file `myfilter.log' in the current logging directory 


auth { 
This filter will process authentication requests. 


input-format "%C{User-Name} %C{Calling-Station-Id}"; 
Define the input line format for this filter. The %C{} expressions will be replaced by the values of the corresponding attributes from the incoming request (see section 5.14 Macro Substitution). 


wait-reply yes; 
radiusd will wait for the reply from this filter to decide whether to authenticate the user. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.1.3 Invoking the Filter from a User Profile 
To invoke this filter from the user profile, specify its name prefixed with `|' in the value of Exec-Program-Wait attribute, like this: 

  DEFAULT Auth-Type = System,
                Simultaneous-Use = 1
        Exec-Program-Wait = "|check_clid"

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.1.4 Adding Reply Attributes 
Apart from simply deciding whether to authenticate a user, the filter can also modify the reply pairs. 

  #! /bin/sh

DB=/var/db/userlist

while read NAME CLID
do
    if grep "$1:$2" $DB; then
        echo "0 Service-Type = Login, Session-Timeout = 1200"
    else
        echo "1 Reply-Message = 
              \"You are not authorized to log in\""
    fi
done

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.1.5 Accounting Filters 
Let's suppose we further modify our filter to also handle accounting requests. To discern between the authentication and accounting requests we'll prefix each authentication request with the word `auth' and each accounting request with the word `acct'. Furthermore, the input line for accounting requests will contain a timestamp. 

Now, our filter program will look as follows: 

  #! /bin/sh

AUTH_DB=/var/db/userlist
ACCT_DB=/var/db/acct.db

while read CODE NAME CLID DATE
do
    case CODE
    auth)
        if grep "$1:$2" $DB; then
            echo "0 Service-Type = Login, \
                  Session-Timeout = 1200"
        else
            echo "1 Reply-Message = \
                  \"You are not authorized to log in\""
        fi
    acct)
        echo "$CODE $NAME $CLID $DATE" >> $ACCT_DB
done

 


Its declaration in the `raddb/config' will also change: 

  filter check_clid {
    exec-path "/usr/libexec/myfilter";
    error-log "myfilter.log";
    auth {
        input-format "auth %C{User-Name} 
                      %C{Calling-Station-Id}";
        wait-reply yes;
    };
    acct {
        input-format "acct %C{User-Name} 
                      %C{Calling-Station-Id} %D";
        wait-reply no;
    };
};        

 


(The input-format lines are split for readability. Each of them is actually one line). 

Notice wait-reply no in the acct statement. It tells radiusd that it shouldn't wait for the response on accounting requests from the filter. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.1.6 Invoking the Accounting Filter 
To invoke the accounting filter, specify its name prefixed with a vertical bar character as a value of Acct-Ext-Program in our `raddb/hints' file. For example: 

  DEFAULT NULL
        Acct-Ext-Program = "|check_clid:

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2 Rewrite 
Rewrite is the GNU Radius extension language. Its name reflects the fact that it was originally designed to rewrite the broken request packets so they could be processed as usual (see section 11.2.4 Rewriting Incoming Requests). Beside this basic use, however, Rewrite functions are used to control various aspects of GNU Radius functionality, such as verifying the activity of user sessions, controlling the amount of information displayed in log messages, etc (see section 11.2.3 Interaction with Radius). 


11.2.1 Syntax Overview     
11.2.2 Quick Start     
11.2.3 Interaction with Radius     
11.2.4 Rewriting Incoming Requests     
11.2.5 Login Verification Functions     
11.2.6 Attribute Creation Functions     
11.2.7 Logging Hook Functions     
11.2.8 Full Syntax Description     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.1 Syntax Overview 
The syntax of Rewrite resembles that of C. Rewrite has two basic data types: integer and string. It does not have global variables; all variables are automatic. The only exceptions are the A/V pairs from the incoming request, which are accessible to Rewrite functions via the special notation %[attr]. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.2 Quick Start 
As an example, let's consider the following Rewrite function: 

  string
foo(integer i)
{
    string rc;
    if (i % 2)
        rc = "odd";
    else
        rc = "even";
    return "the number is " + rc;
}

 


The function takes an integer argument and returns the string `the number is odd' or `the number is even', depending on the value of i. This illustrates the fact that in Rewrite the addition operator is defined on the string type. The result of such operation is the concatenation of operands. 

Another example is a function that adds a prefix to the User-Name attribute: 

  integer
px_add()
{
        %[User-Name] = "pfx-" + %[User-Name];
        return 0;
}

 


This function manipulates the contents of the incoming request; its return value has no special meaning. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.3 Interaction with Radius 
A Rewrite function can be invoked in several ways, depending on its purpose. There are three major kinds of Rewrite functions: 


Functions used to rewrite the incoming requests. 
Functions designed for simultaneous login verification. 
Functions used to generate Radius attribute values. 
Logging hooks. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.4 Rewriting Incoming Requests 
The need for rewriting the incoming requests arises from the fact that some NASes are very particular about the information they send with the requests. There are cases when the information they send is hardly usable or even completely unusable. For example, a Cisco AS5300 terminal server used as a voice-over IP router packs a lot of information into its Acct-Session-Id attribute. Though the information stored there is otherwise relevant, it makes proper accounting impossible, since the Acct-Session-Id attributes in the start and stop packets of the same session become different, and thus Radius cannot determine the session start to which the given session stop request corresponds (see section 14.2.7 Acct-Session-Id). 

In order to cope with such NASes, GNU Radius is able to invoke a Rewrite function upon arrival of the packet and before processing it further. This function can transform the packet so that it obtains the form prescribed by RFCs and its further processing becomes possible. 

For example, in the case of the AS5300 router, a corresponding Rewrite function parses the Acct-Session-Id attribute; breaks it down into fields; stores them into proper attributes, creating them if necessary; and finally replaces Acct-Session-Id with its real value, which is the same for the start and stop records corresponding to a single session. Thus all the information that came with the packet is preserved, but the packet itself is made usable for proper accounting. 

A special attribute, Rewrite-Function, is used to trigger invocation of a Rewrite function. Its value is a name of the function to be invoked. 

When used in a `naslist' profile, the attribute causes the function to be invoked when the incoming request matches the huntgroup (see section 3.4.4 Huntgroups). For example, to have a function fixup invoked for each packet from the NAS 10.10.10.11, the following huntgroup rule may be used: 

  DEFAULT  NAS-IP-Address = 11.10.10.11
         Rewrite-Function = "fixup"

 


The Rewrite-Function attribute may also be used in a `hints' rule. In this case, it will invoke the function if the request matches the rule (see section 3.4.3 Hints). For example, this `hints' rule will cause the function to be invoked for each request containing the user name starting with `P': 

  DEFAULT  Prefix = "P"
         Rewrite-Function = "fixup"

 


Note that in both cases the attribute can be used either in LHS or in RHS pairs of a rule. 

The packet rewrite function must be declared as having no arguments and returning an integer value: 

  integer fixup()
{
}

 


The actual return value from such a function is ignored, the integer return type is just a matter of convention. 

The following subsection present some examples of packet rewrite functions. 


11.2.4.1 Examples of Various Rewrite Functions     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.4.1 Examples of Various Rewrite Functions 
The examples found in this chapter are working functions that can be used with various existing NAS types. They are taken from the `rewrite' file contained in distribution of GNU Radius. 


1. Port rewriting for MAX Ascend terminal servers 
Some MAX Ascend terminal servers pack additional information into the NAS-Port-Id attribute. The port number is constructed as XYYZZ, where X = 1 for digital, X = 2 for analog, YY is the line number (1 for first PRI/T1/E1, 2 for second, and so on), and ZZ is the channel number (on the PRI or channelized T1/E1). 

The following rewrite functions are intended to compute the integer port number in the range (1 .. portcnt), where portcnt represents the real number of physical ports available on the NAS. Such a port number can be used, for example, to create a dynamic pool of IP addresses (see section 14.1.8 Framed-IP-Address). 

  /*
 * decode MAX port number
 * input: P        --  The value of NAS-Port-Id attribute
 *        portcnt  --  number of physical ports on the NAS
 */
integer
max_decode_port(integer P, integer portcnt)
{
    if (P > 9999) {
        integer s, l, c;

        s = P / 10000;
        l = (P - (10000 * s))/100; 
        c = P - ((10000 * s) + (100 * l)); 
        return (c-1) + (l-1) * portcnt;
    }
    return P;
}

/*
 * Interface function for MAX terminal server with 23 ports.
 * Note that it saves the received NAS-Port-Id attribute in
 * the Orig-NAS-Port-Id attribute. The latter must be
 * defined somewhere in the dictionary
 */
integer
max_fixup()
{
    %[Orig-NAS-Port-Id] = %[NAS-Port-Id];
                                  # Preserve original data
    %[NAS-Port-Id] = max_decode_port(%[NAS-Port-Id], 23);
    return 0;
}

 



2. Session ID parsing for Cisco AS 5300 series 
Cisco VOIP IOS encodes a lot of other information into its Acct-Session-Id. The pieces of information are separated by `/' characters. The part of Acct-Session-Id up to the first `/' character is the actual session ID. 

On the other hand, its accounting packets lack NAS-Port-Id, though they may contain the vendor-specific pair with code 2 (vendor PEC 9), which is a string in the form `ISDN 9:D:999' (`9' represents any decimal digit). The number after the last `:' character can be used as a port number. 

The following code parses Acct-Session-Id attribute and stores the information it contains in various other attributes, generates a normal Acct-Session-Id, and attempts to generate a NAS-Port-Id attribute. 

  /* 
 * The port rewriting function for Cisco AS5300 used for
 * VoIP. This function is used to generate NAS-Port-Id pair
 * on the basis of vendor-specific pair 2. If the latter is
 * in the form "ISDN 9:D:999" (where each 9 represents a
 * decimal digit), then the function returns the number
 * after the last colon. This is used as a port number.
 */
integer
cisco_pid(string A)
{
    if (A =~ 
        ".*\([0-9][0-9]*\):
         [A-Z0-9][A-Z0-9]*:\([0-9][0-9]*\)") {
        return (integer)\2;
    }
    return -1;
}

/*
 * This function parses the packed session id.
 * The actual sid is the number before the first slash
 * character.  Other possibly relevant fields are also
 * parsed out and saved in the Voip-* A/V pairs. The latter
 * should be defined somewhere in the dictionary.
 * Note that the regular expression in this example
 * spans several lines for readability. It should be on one 
 * line in real file.
 */
string
cisco_sid(string S)
{
   if (S =~ "\(.[^/]*\)/[^/]*/[^/]*/\([^/]*\)/\([^/]*\)/
             \([^/]*\)/\([^/]*\)/\([^/]*\)/\([^/]*\)
             /\([^/]*\).*") {
        %[Voip-Connection-ID] = \2;
        %[Voip-Call-Leg-Type] = \3;
        %[Voip-Connection-Type] = \4;
        %[Voip-Connect-Time] = \5;
        %[Voip-Disconnect-Time] = \6;
        %[Voip-Disconnect-Cause] = \7;
        %[Voip-Remote-IP] = \8;
        return \1;
   } 
   return S;
}

/*
 * Normalize cisco AS5300 packets
 */
integer
cisco_fixup()
{
    integer pid;

    if ((pid = cisco_pid(%[Cisco-PRI-Circuit])) != -1) {
        if (*%[NAS-Port-Id])
            %[Orig-NAS-Port-Id] = %[NAS-Port-Id];
        %[NAS-Port-Id] = pid;
    }
    if (*%[Acct-Session-Id]) {
        %[Orig-Acct-Session-Id] = %[Acct-Session-Id];
        %[Acct-Session-Id] = cisco_sid(%[Acct-Session-Id]);
    }
    return 0;
}

 



3. User-name rewriting for NT machines 
Users coming from Windows NT machines often authenticate themselves as `NT_DOMAIN\username'. The following function selects the user-name part and stores it in the User-Name attribute: 

  integer
login_nt(string uname)
{
    integer i;
        
    if ((i = index(uname, '\\')) != -1)
        return substr(uname, i+1, -1);
    return uname;
}

integer
nt_rewrite()
{
    %[Orig-User-Name] = %[User-Name];
    %[User-Name] = login_nt(%[User-Name]);
    return 0;
}

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.5 Login Verification Functions 
A login verification function is invoked to process the output from the NAS. This process is described in 7.9 Multiple Login Checking. The function to be invoked for given NAS is defined by a function flag in the `raddb/nastypes' or `raddb/naslist' file (see section 5.5 NAS Types -- `raddb/nastypes'). It must be defined as follows: 


Function Template: integer check (string str, string name, integer pid, string sid) 

Its arguments are: 


str 
Input string. If the query method is finger, this is the string of output received from the NAS with trailing newline stripped off. If the query method is snmp, it is the received variable value converted to its string representation. 
name 
User name. 
pid 
Port ID of the session. 
sid 
Session ID. 
The function should return non-0 if its arguments match the user's session, and 0 otherwise. 


11.2.5.1 Examples of Login Verification Functions     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.5.1 Examples of Login Verification Functions 
As an example, let's consider the function for analyzing a line of output from a standard UNIX finger service. In each line of finger output the first field contains the user name; the third field, the The function must return 1 if the three fields match the input user name and port and session IDs: 

  integer
check_unix(string str, string name, integer pid, string sid)
{
    return field(str, 1) == name
           && field(str, 3) == pid
           && field(str, 7) == sid;
}

 


The next example is a function to analyze a line of output from an SNMP query returning a user name. This function must return 1 if the entire input line matches the user name: 

  integer
check_username(string str, string name, integer pid, string sid)
{
    return str == name;
}

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.6 Attribute Creation Functions 
These are the functions used to create Radius reply attributes. An attribute creation function can take any number of arguments. The type of its return is determined by the type of Radius attribute the value will be assigned to. To invoke the function, write its name in the A/V pair of the RHS in the `raddb/users' file, e.g.: 

  DEFAULT Auth-Type = SQL
        Service-Type = Framed-User,
            Framed-IP-Address = "=get_ip_addr(10.10.10.1)"

 


The function get_ip_addr will be invoked after successful authentication and it will be passed the IP 10.10.10.1 as its argument. An example of a useful function that can be invoked this way is 

  integer
get_ip_address(integer base)
{
    return base + %[NAS-Port-Id] - %[NAS-Port-Id]/16;
}

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.7 Logging Hook Functions 
A logging hook functions should be declared as follows: 


Function Template: string hook (integer reqtype, string nasid, integer reqid) 
reqtype 
Type of the request. It can be converted to string using request_code_string function (see section 11.2.8.7 Rewrite Built-in Functions). 
nasid 
NAS identifier from `raddb/naslist', or its host name if not declared there 
reqid 
Request identifier. 
Notice that the hook function shall not produce any side effects, in particular it shall not modify the incoming request in any way. 

Following is an example prefix hook function that formats the incoming request data: 

  string
compat_log_prefix(integer reqtype, string nas, integer id)
{
        string result;

        return "(" + request_code_string(reqtype) + " "
                   + nas + " " + (string)id + " " + %[User-Name] + ")";
}

 


Here is a sample log produced by radiusd before and after enabling this function: 

  Auth.notice: Login OK [jsmith]
...
Auth.notice: (AUTHREQ nas-2 251 jsmith): Login OK [jsmith]

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.8 Full Syntax Description 

11.2.8.1 Rewrite Data Types     
11.2.8.2 Rewrite Symbols     
11.2.8.3 Rewrite Identifiers     
11.2.8.4 Rewrite Declarations     
11.2.8.5 Rewrite Statements     
11.2.8.6 Regular Expressions     
11.2.8.7 Rewrite Built-in Functions     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.8.1 Rewrite Data Types 
There are only two data types: integer and string, the two being coercible to each other in the sense that a string can be coerced to an integer if it contains a valid ASCII representation of a decimal, octal, or hex number, and an integer can always be coerced to a string, the result of such coercion being the ASCII string that is the decimal representation of the number. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.8.2 Rewrite Symbols 
A symbol is a lexical token. The following symbols are recognized: 


Arithmetical operators 
These are `+', `-', `*', `/' representing the basic arithmetical operations, and `%' meaning remainder. 
Comparison operators 
These are: `==', `!=', `<', `<=', `>', `>=' with the same meaning they have in C. Special operators are provided for regular-expression matching. The binary operator `=~' returns true if its left-hand-side operand matches the regular expression on its right-hand side (see section 11.2.8.6 Regular Expressions). `!~' returns true if its left-hand-side operand does not match the regexp on its right-hand side. The right-hand-side operand of `!~' or `=~' must be a literal string, i.e., the regular expression must be known at compile time. 
Unary operators 
The unary operators are `-' and `+' for unary plus and minus, `!' for boolean negation, and `*' for testing for the existence of an attribute. 
Boolean operators 
These are `&&' and `||'. 
Parentheses `(' and `)' 
These are used to change the precedence of operators, to introduce type casts (type coercions), to declare functions, and to pass actual arguments to functions. 
Curly braces (`{' and `}') 
These are used to delimit blocks of code. 
Numbers 
Numbers follow the usual C convention for integers. A number consisting of a sequence of digits is taken to be octal if it begins with `0' (digit zero), and decimal otherwise. If the sequence of digits is preceded by `0x' or `0X', it is taken to be a hexadecimal integer. 
IP Numbers 
IP numbers are represented by a standard numbers-and-dots notation. IP numbers do not constitute a separate data type, rather they are in all respects similar to initeger numbers. 
Characters 
These follow the usual C convention for characters, i.e., they consist either of an ASCII character itself or of its value, enclosed in a pair of singlequotes. The character value begins with `\' (backslash) and consists either of three octal or of two hexadecimal digits. A character does not form a special data type; it is represented internally by an integer. 
Quoted strings 
These follow slightly modified C conventions for strings. A string is a sequence of characters surrounded by double quotes, as in `"..."'. In a string, the double quote character `"' must be preceeded by a backslash `\'. A `\' and an immediately following newline are ignored. Following escape sequences have special meaning: 

\a 
Audible bell character (ASCII 7) 
\b 
Backspace (ASCII 8) 
\e 
Escape character (ASCII 27) 
\f 
Form feed (ASCII 12) 
\n 
Newline (ASCII 10) 
\r 
Carriage return (ASCII 13) 
\t 
Horizontal tab (ASCII 9) 
\\ 
Backslash 
\ooo 
(`o' represents an octal digit) A character whose ASCII value is represented by the octal number `ooo'. 
\xHH 
\XHH 
(`H' represents a hex digit) A character whose ASCII value is represented by the hex number `HH'. 
\( 
Two characters `\('. 
\) 
Two characters `\)'. 
If the character following the backslash is not one of those specified, the backslash is ignored. 


Attribute values 
The incoming request is passed implicitly to functions invoked via the Rewrite-Function attribute. It is kept as an associative array, whose entries can be accessed using the following syntax: 
  `%[' attribute-name `]'
`%[' attribute-name `]' `(' n `)'

 


The first form returns the value of the attribute attribute-name. Here attribute-name should be a valid Radius dictionary name (see section 5.2 Dictionary of Attributes -- `raddb/dictionary'). 

The second form returns the value of the nth attribute of type attribute-name. The index n is counted from zero, so 

          %[attribute-name](0)

 


is equivalent to 

          %[attribute-name]

 



Identifiers 
Identifiers represent functions and variables. These are described in the next sub-subsection. 
Regexp group references 
A sequence of characters in the form 
  `\number'

 



refers to the contents of parenthesized group number number obtained as a result of the last executed `=~' command. The regexp group reference has always string data type. For example: 

  string
basename(string arg)
{
    if (arg =~ ".*/\(.*\)\..*")
        return \1;
    else
        return arg;
}

 


This function strips from arg all leading components up to the last slash character, and all trailing components after the last dot character. It returns arg unaltered if it does not contain slashes and dots. It is roughly analogous to the system basename utility. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.8.3 Rewrite Identifiers 
A valid identifier is a string of characters meeting the following requirements: 


It starts with either a lower- or an uppercase letter of the Latin alphabet or either of the following symbols: `_', `$'. 
It consists of alphanumeric characters, underscores(`_'), and dollar signs (`$'). 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.8.4 Rewrite Declarations 

Function declarations 
A Rewrite function is declared as follows: 

  type function-name (parameter-list)

 


where type specifies the return type of the function, function-name declares the symbolic name of the function, and parameter-list declares the formal parameters to the function. It is a comma-separated list of declarations in the form 

  type parm-name

 


type being the parameter type, and parm-name being its symbolic name. Both function-name and parm-name should be valid identifiers. 


Variable declarations 
There are no global variables in Rewrite. All variables are local. The local variables are declared right after the opening curly brace (`{') and before any executable statements. The declaration syntax is 

  type ident_list ;

 


Here ident_list is either a valid Rewrite identifier or a comma-separated list of such identifiers. 

Note that, unlike in C, no assignments are allowed in variable declarations. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.8.5 Rewrite Statements 
The Rewrite statements are: expressions, assignments, conditional statements, and return statements. A statement is terminated by a semicolon. 


Expressions 
An expression is one of the following: 


A variable identifier 
A type coercion expression 
An arithmetic expression 
A boolean expression 
An assignment 
A function call 
A delete statement 

Type coercion 
The type coercion is like a type cast in C. Its syntax is 
  `(' type `)' ident

 


The result of type coercion is as follows: 

type  Variable type  Resulting conversion 
integer integer  No conversion. This results in the same integer value.  
integer string  If the string value of the variable is a valid ASCII representation of the integer number (either decimal, octal, or hex), it is converted to the integer; otherwise the result of the conversion is undefined.  
string integer  The ASCII representation (in decimal) of the integer number.  
string string  No conversion. This results in the same string value.  



Assignment 
An assignment is 
  ident = expression ;

 


The variable ident is assigned the value of expression. 


Function calls 
These take the form 
  ident ( arg-list )

 


where ident is the identifier representing the function, and arg-list is a comma-separated list of expressions supplying actual arguments to the function. The number of the expressions must correspond exactly to the number of formal parameters in the function definition. The function that ident references can be either a compiled function or a built-in function. 


`delete' statement 
The `delete' statement is used to delete an attribute or attributes from the incoming request. Its syntax is: 

  delete attribute-name;
delete attribute-name(n);

 


The first variant deletes all the attributes of the given type. The second variant deletes only the nth occurrence of the matching attribute. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.8.6 Regular Expressions 
Rewrite uses POSIX regular expressions (See section `Regular Expression Library' in Regular Expression Library, for the detailed description of these). 

You control the exact type of regular expressions by the use of the pragmatic comment regex. Its syntax is as follows: 

  #pragma regex option-list

 


Option-list is a whitespace-separated list of options. Each option is one of the following words prefixed with `+' or `-': 


extended 
Use POSIX extended regular expression syntax when interpreting regular expressions. 

icase 
Do not differentiate case. Subsequent regular expression comparisons will be case insensitive. 
newline 
Match-any-character operators don't match a newline. 
A non-matching list (`[^...]') not containing a newline does not match a newline. 

Match-beginning-of-line operator (`^') matches the empty string immediately after a newline. 

Match-end-of-line operator (`$') matches the empty string immediately before a newline. 


Prefixing an option with `+' means to enable the corresponding behavior. Prefixing it with `-' means to disable it. Thus, the following statement: 

  #pragma regex +extended +icase

 


will enable extended POSIX regular expressions using case-insensitive comparison. 

Using the following comment: 

  #pragma regex -extended 

 


will switch to the basic POSIX regular expressions. 

The settings of a regex pragmatic comment remain in force up to the end of current source file, or to the next regex comment, whichever occurs first. 

For compatibility with previous versions, GNU Radius uses the following defaults: 

  #pragma regex -extended -icase -newline

 


i.e. all regular expressions are treated as basic POSIX, comparison is case-sensitive. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.2.8.7 Rewrite Built-in Functions 
The following built-in functions are provided: 


Function: integer length (string s) 
Returns the length of the string s. 

  length("string") => 6

 




Function: integer index (string s, integer c) 
Returns the index of the first occurrence of the character c in the string s. Returns -1 if no such occurrence is found. 

  index("/raddb/users", 47) => 0

index("/raddb/users", 45) => -1

 




Function: integer rindex (string s, integer i) 
Returns the index of the last occurrence of the character c in the string s. Returns -1 if no such occurrence is found. 

  rindex("/raddb/users", 47) => 6

 




Function: string substr (string s, integer start, integer length) 
Returns the substring of s of length at most length starting at position start. 

  substr("foo-bar-baz", 3, 5) => "-bar-"

 



All character positions in strings are counted from 0. 


Function: string field (string buffer, integer n) 
This function regards the buffer argument as consisting of fields separated with any amount of whitespace. It extracts and returns the nth field. n is counted from 1. 

  field("GNU's not UNIX", 1) => "GNU's"
field("GNU's not UNIX", 2) => "not"
field("GNU's not UNIX", 3) => "UNIX"
field("GNU's not UNIX", 4) => ""

 




Function: integer logit (string msg) 
Outputs its argument to the Radius log channel info. Returns 0. For debugging purposes. 


Function: integer inet_aton (string str) 
Converts the Internet host address str from the standard numbers-and-dots notation into the equivalent integer in host byte order. 

  inet_aton("127.0.0.1") => 2130706433

 




Function: string inet_ntoa (integer ip) 
Converts the Internet host address ip given in host byte order to a string in standard numbers-and-dots notation. 

  inet_ntoa(2130706433) => "127.0.0.1"

 




Function: integer htonl (integer n) 
Converts the integer n, regarded as long, from host to network byte order. 


Function: integer ntohl (integer n) 
Converts the integer n, regarded as long, from network to host byte order. 


Function: integer htons (integer n) 
Converts the integer n, regarded as short, from host to network byte order. 


Function: integer ntohs (integer n) 
Converts the integer n, regarded as short, from network to host byte order. 


Function: string gsub (string regex, string repl, string str) 
For each substring matching the regular expression regex in the string str, substitute the string repl, and return the resulting string. 

  gsub("s","S","strings")
    => "StringS"
gsub("[0-9][0-9]*","N","28 or 29 days")
    => "N or N days"
gsub("[()'\"]","/","\"a\" (quoted) 'string'")
    => "/a/ /quoted/ /string/"

 




Function: string qprn (string str) 
Replace all non-printable characters in string S by their corresponding hex value preceeded by a percent sign. Return the resulting string. Printable are alphabetical characters, decimal digits and dash (`-'). Other characters are considered non-printable. For example: 

  qprn("a string/value") => "a%20string%2Fvalue"

 




Function: string quote_string (string str) 
Replace all non-printable characters in string str by their three-digit octal code prefixed with a backslash, or by their C escape notation, as appropriate. Non-printable characters depend on the locale settings. For example, suppose that the current locale is set to ISO-8859-1 (a so called "Latin-1" character set) and -!- represents a tab character. Then: 

  quote_string("Francois contains non-!-printable chars")
  => "Fran\347ois contains non\tprintable chars"

 




Function: string unquote_string (string str) 
Replace C escape notations in string str with corresponding characters using current locale. For example, for ISO-8859-1 locale: 

  unquote_string("Fran\347ois") => "Francois"

 




Function: string toupper (string str) 
Returns the copy of the string str with all alphabetical characters converted to upper case. For example: 

  toupper("a-string") => "A-STRING"

 




Function: string tolower (string str) 
Returns the copy of the string str with all alphabetical characters converted to lower case. For example: 

  tolower("A-STRING") => "a-string"

 




Function: string request_code_string (integer code) 
Converts integer RADIUS request code to its textual representation as per RFC 3575. This function is useful in logging hooks (see section 5.1.2.1 Logging hooks). 

  request_code_string(4) => "Accounting-Request"

 




Native Language Support 
The native language support is provided via the functions described below. These functions are interfaces to GNU gettext library. For the information about general concepts and principles of Native Language Support, please refer to section `gettext' in GNU gettext utilities. 

The default current textual domain is `radius'. 


Function: string textdomain (string domain) 
Sets the new value for the current textual domain. This domain is used by the functions gettext and ngettext. Returns the name of the previously used domain. 


Function: string gettext (string msgid) 
Function: string _ (string msgid) 
The function returns the translation of the string msgid if it is available in the current domain. If it is not available, the argument itself is returned. 

The second form of this function provides a traditional shortcut notation. 

For a detailed description of the GNU gettext interface, refer to section `Interface to gettext' in GNU gettext utilities. 



Function: string dgettext (string domain, string msgid) 
Returns the translation of the string msgid if it is available in the domain domain. If it is not available, the argument itself is returned. 


Function: string ngettext (string msgid_singular, string msgid_plural, integer number) 
The ngettext function is used to translate the messages that have singular and plural forms. The msgid_singular parameter must contain the singular form of the string to be converted. It is also used as the key for the search in the catalog. The msgid_plural parameter is the plural form. The parameter number is used to determine the plural form. If no message catalog is found msgid_singular is returned if number == 1, otherwise msgid_plural. 

For a detailed description of the GNU gettext interface for the plural translation, refer to section `Additional functions for plural forms' in GNU gettext utilities. 




Function: string dngettext (string domain, string msg_sing, string msg_plur, integer number) 
Similar to ngettext, but searches translation in the given domain. 


Request Accessors 
The following functions are used to read some internal fields of a RADIUS request. 


Function: Integer request_source_ip () 
Returns source IP address of the currently processed request. This function can be used to add NAS-IP-Address attribute to the requests lacking one, e.g.: 

  integer
restore_nas_ip()
{
        if (!*%[NAS-IP-Address])
                %[NAS-IP-Address] = request_source_ip();
        return 0;
}

 




Function: Integer request_source_port () 
Returns the source UDP port. 


Function: Integer request_id () 
Returns the request identifier. 


Function: Integer request_code () 
Returns the request code. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.3 Guile 
The name Guile stands for GNU's Ubiquitous Intelligent Language for Extensions. It provides a Scheme interpreter conforming to the R4RS language specification. This section describes use of Guile as an extension language for GNU Radius. It assumes that the reader is sufficiently familiar with the Scheme language. For information about the language, refer to section `Top' in Revised(4) Report on the Algorithmic Language Scheme. For more information about Guile, see section `Overview' in The Guile Reference Manual. 

Scheme procedures can be called for processing both authentication and accounting requests. The invocation of a Scheme procedure for an authentication request is triggered by the Scheme-Procedure attribute; the invocation for an accounting request is triggered by the Scheme-Acct-Procedure attribute. The following sections address these issues in more detail. 


11.3.1 Data Representation     
11.3.2 Authentication with Scheme     
11.3.3 Accounting with Scheme     
11.3.4 Radius-Specific Functions     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.3.1 Data Representation 
A/V pair lists are the main object Scheme functions operate upon. Scheme is extremely convenient for representation of such objects. A Radius A/V pair is represented by a Scheme pair; e.g., 

          Session-Timeout = 10

 


is represented in Guile as 

          (cons "Session-Timeout" 10)

 


The car of the pair can contain either the attribute dictionary name or the attribute number. Thus, the above pair may also be written in Scheme as 

          (cons 27 10)

 


(because Session-Timeout corresponds to attribute number 27). 

Lists of A/V pairs are represented by Scheme lists. For example, the Radius pair list 

          User-Name = "jsmith",
                User-Password = "guessme",
                NAS-IP-Address = 10.10.10.1,
                NAS-Port-Id = 10

 


is written in Scheme as 

          (list
          (cons "User-Name" "jsmith")
          (cons "User-Password" "guessme")
          (cons "NAS-IP-Address" "10.10.10.1")
          (cons "NAS-Port-Id" 10))

 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.3.2 Authentication with Scheme 
The Scheme procedure used for authentication must be declared as follows: 


Function Template: auth-function request-list check-list reply-list 
Its arguments are: 
request-list 
The list of A/V pairs from the incoming request 
check-list 
The list of A/V pairs from the LHS of the profile entry that matched the request 
reply-list 
The list of A/V pairs from the RHS of the profile entry that matched the request 
The function return value determines whether the authentication will succeed. The function must return either a boolean value or a pair. The return of #t causes authentication to succeed. The return of #f causes it to fail. 

For a function to add something to the reply A/V pairs, it should return a pair in the form 

      (cons return-code list)

 


where return-code is a boolean value of the same meaning as described above. list is a list of A/V pairs to be added to the reply list. For example, the following function will always deny the authentication, returning an appropriate message to the user: 

  (define (decline-auth request-list check-list reply-list)
  (cons #f
        (list
         (cons "Reply-Message"
               "\r\nSorry, you are not
                allowed to log in\r\n"))))

 


As a more constructive example, let's consider a function that allows the authentication only if a user name is found in its internal database: 

  (define staff-data
  (list
   (list "scheme"
         (cons
          (list (cons "NAS-IP-Address" "127.0.0.1"))
          (list (cons "Framed-MTU" "8096")))
         (cons
          '()
          (list (cons "Framed-MTU" "256"))))))
  
(define (auth req check reply)
  (let* ((username (assoc "User-Name" req))
         (reqlist (assoc username req))
         (reply-list '()))
    (if username
        (let ((user-data (assoc (cdr username) staff-data)))
          (rad-log L_INFO (format #f "~A" user-data))
          (if user-data
              (call-with-current-continuation
               (lambda (xx)
                 (for-each
                  (lambda (pair)
                    (cond
                     ((avl-match? req (car pair))
                      (set! reply-list (avl-merge
                                        reply-list
                                        (cdr pair)))
                      (xx #t))))
                  (cdr user-data))
                 #f)))))
    (cons
     #t
     reply-list)))

 


To trigger the invocation of the Scheme authentication function, assign its name to the Scheme-Procedure attribute in the RHS of a corresponding `raddb/users' profile. For example: 

  DEFAULT Auth-Type = SQL
        Scheme-Procedure = "auth"

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.3.3 Accounting with Scheme 
The Scheme accounting procedure must be declared as follows: 


Function Template: acct-function-name request-list 
Its argument is: 
request-list 
The list of A/V pairs from the incoming request 
The function must return a boolean value. The accounting succeeds only if it has returned #t. 

Here is an example of a Scheme accounting function. The function dumps the contents of the incoming request to a file: 

  (define radius-acct-file "/var/log/acct/radius")

(define (acct req)
  (call-with-output-file radius-acct-file
    (lambda (port)
      (for-each (lambda (pair)
                  (display (car pair) port)
                  (display "=" port)
                  (display (cdr pair) port)
                  (newline port))
                req)
      (newline port)))
  #t)

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

11.3.4 Radius-Specific Functions 

Scheme Function: avl-delete av-list attr 
Delete from av-list the pairs with attribute attr. 


Scheme Function: avl-merge dst src 
Merge src into dst. 


Scheme Function: avl-match? target list 
Return #t if all pairs from list are present in target. 


Scheme Function: rad-dict-name->attr name 
Return a dictionary entry for the given attribute name or #f if no such name was found in the dictionary. 

A dictionary entry is a list in the form 


Scheme List: dict-entry name-string attr-number type-number vendor 

where the arguments are as follows: 


name-string 
The attribute name 
value-number 
The attribute number 
type-number 
The attribute type 
vendor 
The vendor PEC, if the attribute is a vendor-specific one, or #f otherwise. 

Scheme Function: rad-dict-value->name attr value 
Returns the dictionary name of the given value for an integer-type attribute attr, which can be either an attribute number or its dictionary name. 


Scheme Function: rad-dict-name->value attr value 
Convert a symbolic attribute value name into its integer representation. 


Scheme Function: rad-dict-pec->vendor pec 
Convert a PEC to the vendor name. 


Scheme Function: rad-log-open prio 
Open Radius logging to the severity level prio. 


Scheme Function: rad-log-close 
Close a Radius logging channel opened by a previous call to rad-log-open. 


Scheme Function: rad-rewrite-execute-string string 
Interpret string as an invocation of a function in Rewrite language and execute it. 

Return value: return of the corresponding Rewrite call, translated to the Scheme data type. 



Scheme Function: rad-rewrite-execute arglist 
Execute a Rewrite language function. (car arglist) is interpreted as a name of the Rewrite function to execute, and (cdr arglist) as a list of arguments to be passed to it. 

Return value: return of the corresponding Rewrite call, translated to the Scheme data type. 



Scheme Function: rad-openlog ident option facility 
Scheme interface to the system openlog() call. 


Scheme Function: rad-syslog prio text 
Scheme interface to the system syslog() call. 


Scheme Function: rad-closelog 
Scheme interface to the system closelog() call. 


Scheme Function: rad-utmp-putent status delay list radutmp_file radwtmp_file 
Write the supplied data into the radutmp file. If radwtmp_file is not nil, the constructed entry is also appended to wtmp_file. 

list is: 


Scheme List: utmp-entry user-name orig-name port-id port-type session-id caller-id framed-ip nas-ip proto 


user-name 
The user name 
orig-name 
The original user name from the request 
port-id 
The value of the NAS-Port-Id attribute 
port-type 
A number or character indicating the port type 
session-id 
The session ID 
caller-id 
The value of the Calling-Station-Id attribute from the request 
framed-ip 
The framed IP assigned to the user 
nas-ip 
The NAS IP 
proto 
A number or character indicating the type of the connection 

--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12. Utility Programs 

Controlling who and when was logged in 
12.1 radwho    Show who is logged in by radius now. 
12.2 radlast    Show the history of logins by radius. 

Maintenance commands 
12.3 radzap    Modify the login records. 
12.4 radgrep    Quickly find the login record. 
12.5 radping    Ping the remote machine by the username. 
12.6 radauth    Check if a user can be authenticated. 
12.7 radctl    Radctl monitor. 
12.8 builddbm    Create DBM version of the `raddb/users' file. 

Guile interface 
12.9 radscm: A Guile Interface to Radius Functions    A Guile interface to radius functions. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.1 radwho 
Radwho displays the list of users currently logged in by the Radius server. 

Default output information is made compatible with that of the standard UNIX finger(1) utility. For each user the following information is displayed: login name, name, connection protocol, NAS port, login date, NAS name, assigned IP or corresponding network name. 

When used with `-l' option, the long output format is used. In this format the following information is output: 


`Login' 
Login name of the user 
`SessionID' 
Unique session ID assigned by the terminal server. 
`Proto' 
Connection prototype. 
`Port' 
Port number 
`When' 
Login date and time 
`From' 
Name of the NAS that accepted the connection. 
`Location' 
Framed IP or the corresponding network name. 
`Caller' 
Caller station ID ad reported by the NAS. 
`Duration' 
Duration of the session. 

12.1.1 radwho Command Line Options    Command line options. 
12.1.2 radwho Format Strings     
12.1.3 radwho Predefined Formats     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.1.1 radwho Command Line Options 
The following command line options can be used to modify the behavior of the program: 


`-A' 
`--all' 
Display the information about logged-out users as well. The logged-out users are shown with `Proto' field set to HUP. 

`-c' 
`--calling-id' 
Display the calling station ID in the second column. Equivalent to `--format clid'. 

`-d NAME' 
`--directory NAME' 
Set the Radius configuration directory name. 

`-D fmt' 
`--date-format fmt' 
Set the date representation. Th fmt is usual strftime(3) format string. It defaults to %a %H:%M, i.e. the abbreviated weekday name according to the current locale, and the hour and the minutes as two-digit decimal numbers. 

`-e STRING' 
`--empty STRING' 
Display any empty field as STRING. This is useful when the output of radwho is fed to some analyzing program, as it helps to keep the same number of columns on each line of output. 

`-F' 
`--finger' 
Start in fingerd mode. In this mode radwho emulates the behavior of the fingerd(8) utility. Use this option if starting radwho from the `/etc/inetd.conf' line like this (5): 
  finger stream tcp nowait nobody /usr/sbin/radwho
radwho -fL

 


This mode is also enabled by default if radwho notices that its name (argv[0]) is `fingerd' or `in.fingerd'. 


`-H' 
`--no-header' 
Don't display header line. 

`-i' 
`--session-id' 
Display session ID instead of GECOS in the second column. Equivalent to `--format sid'. 

`-I' 
`--ip-strip-domain' 
Display hostnames without domain part. 

`-u' 
`--local-also' 
Display information about local users from the system `utmp' file. May prove useful when running radwho as a finger daemon. 

`-n' 
`--no-resolve' 
Do not resolve IP. 

`-o format' 
`--format format' 
Select customized output format. This can also be changed by setting the value of environment variable RADWHO_FORMAT. The format is either a symbolic name of one of the predefined formats or a format specification (see next subsection). 

`-s' 
`--secure' 
Run in secure mode. Queries without a user name are rejected. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.1.2 radwho Format Strings 
A format string controls the output of every record from `radutmp'. It contains two types of objects: ordinary characters, which are simply copied to the output, and format specifications, each of which causes output of a particular piece of information from the `radutmp' record. 

Each format specification starts with an opening brace and ends with a closing brace. The first word after the brace is the name of the format specification. The rest of words are positional arguments followed by keyword arguments. Both are optional. The keyword arguments begin with a colon and must follow the positional arguments. 

The full list of format specifications follows. 


Format Spec: newline [count] 
Causes the newline character to be output. If the optional count is supplied, that many newlines will be printed 


Format Spec: tab [num] 
Advance to the next tabstop in the output stream. If optional num is present, then skip num tabstops. Each tabstop is eight characters long. 

The following specifications output particular fields of a `radutmp' record. They all take two positional arguments: width and title. 

The first argument, width sets the maximum output length for this specification. If the number of characters actually output is less than the width, they will be padded with whitespace either to the left or to the right, depending on the presence of the :right keyword argument. If the number of characters is greater than width, they will be truncated to fit. If width is not given, the exact data are output as is. 

The second argument, title, gives the title of this column for the heading line. By default no title is output. 

Every field specification accepts at least two keyword arguments. The keyword :right may be used to request alignment to the right for the data. This keyword is ignored if width is not given. 

The keyword :empty followed by a string causes radwho to output that string if the resulting value for this specification would otherwise be empty. 


Format Spec: login width title [:empty repl][:right] 
Print the user login name. 


Format Spec: orig-login width title [:empty repl][:right] 
Print original login name as supplied with the request. 


Format Spec: gecos width title [:empty repl][:right] 
The GECOS field from the local `/etc/passwd' corresponding to the login name. If the user does not have a local account, his login name is output. 


Format Spec: nas-port width title [:empty repl][:right] 
NAS port number 


Format Spec: session-id width title [:empty repl][:right] 
The session ID. 


Format Spec: nas-address width title [:empty repl][:right][:nodomain] 
The NAS name or IP. 

The :nodomain keyword suppresses the output of the domain part of the name, i.e., the hostname is displayed only up to the first dot. 



Format Spec: framed-address width title [:empty repl][:right][:nodomain] 
Framed IP assigned to the user, if any. 

The :nodomain keyword suppresses the output of the domain part of the name, i.e. the hostname is displayed only up to the first dot. 



Format Spec: protocol width title [:empty repl][:right] 
Connection protocol as reported by Framed-Protocol attribute. If the symbolic value is found in the dictionary file, it will be displayed. Otherwise, the numeric value will be displayed as is. 


Format Spec: time width title [:empty repl][:right][:format date-format] 
Date and time when the session started. 

The :format keyword introduces the strftime format string to be used when converting the date for printing. The default value is %a %H:%M. 



Format Spec: duration width title [:empty repl][:right] 
Total time of the session duration. 


Format Spec: delay width title [:empty repl][:right] 
Delay time (see section 14.2.2 Acct-Delay-Time). 


Format Spec: port-type width title [:empty repl][:right] 
Port type as reported by the value of the NAS-Port-Type attribute. If the symbolic value is found in the dictionary file, it will be displayed. Otherwise, the numeric value will be displayed as is. 


Format Spec: clid width title [:empty repl][:right] 
The calling station ID. 


Format Spec: realm width title [:empty repl][:right][:nodomain] 
If the request was forwarded to a realm server, print the symbolic name of the realm from the `raddb/realms' file. If no symbolic name is found, print the remote server IP or hostname. In the latter case, the :nodomain keyword may be used to suppress the output of the domain part of the name, i.e. to display the hostname only up to the first dot. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.1.3 radwho Predefined Formats 
The predefined formats are: 


`default' 
Default output format. Each record occupies one line. The fields output are: login name, GECOS name, connection protocol, port number, time when the connection was initiated, NAS IP, and assigned framed IP. This corresponds to the following format specification (split in several lines for readability: 
  (login 10 Login) (gecos 17 Name) \
(protocol 5 Proto) (nas-port 5 TTY) \
(time 9 When) (nas-address 9 From) \
(framed-address 16 Location)

 



`sid' 
The same as `default', except that the session ID is output in the second column. 

`clid' 
The same as `default', except that the calling station ID is output in the second column. 

`long' 
Outputs all information from each `radutmp' record. It is equivalent to specifying the following format string: 
  (login 32 Login) (session-id 32 SID) \
(protocol 5 Proto) (nas-port 5 Port) \
(time 27 Date) (nas-address 32 NAS) \
(clid 17 CLID) (duration 7 Duration) \
(framed-address 16 Location) (realm 16 Realm)

 



`gnu' 
Each `radutmp' record is represented as a table. It is equivalent to specifying the following format string: 
  User: (login)(newline)\
In real life: (gecos)(newline)\
Logged in: (time)(newline)\
NAS: (nas-address)(newline)\
Port: (nas-port)(newline)\
CLID: (clid)(newline)\
Protocol: (protocol)(newline)\
Session ID: (session-id)(newline)\
Uptime: (duration)(newline)\
Assigned IP: (framed-address)(newline)\
Realm: (realm)(newline)"

 





--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.2 radlast 
The radlast utility lists sessions of specified users, NASes, NAS ports, and hosts, in reverse time order. By default, each line of output contains the login name, the NAS short name and port number from where the session was conducted, the host IP or name, the start and stop times for the session, and the duration of the session. If the session is still continuing, radlast will so indicate. 

When the `-l' option is specified, radlast produces long output. It includes following fields: 


Login name 
NAS short name 
Port number 
Connection protocol 
Port type 
Session ID 
Caller ID 
Framed IP address 
Session Start Time 
Session Stop Time 
Duration of the Session 

12.2.1 radlast Command Line Options    Command line options. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.2.1 radlast Command Line Options 
Use following command line options to control the behavior of the radlast utility: 


`-number' 
`-c number' 
`--count number' 
When given this option, radlast will output at most this many lines of information. 

`-f' 
`--file name' 
Read the specified file instead of the default `/var/log/radwtmp'. 

`-h hostname' 
`--host hostname' 
Report the logins from given host. Host can be either a name or a dotted-quad Internet address. 

`-n shortname' 
`--nas shortname' 
Report the logins from the given NAS. 

`-l' 
`--long-format' 
Long output format. Report all the information stored in `radwtmp' file. 

`-p number' 
`--port number' 
Report the logins on a given port. The port may be specified either fully or abbreviated, e.g. radlast -p S03 or radlast -p 3. 

`-s' 
`--show-seconds' 
Report the duration of the login session in seconds instead of the default days, hours, and minutes. 

`-t' 
The same as `-p'. This flag is provided for compatibility with last(1). 

`-w' 
`--wide' 
Widen the duration field to show seconds as well as the default days, hours and minutes. 

If multiple arguments are given, the logical OR operation between them is assumed, i.e., the information selected by each argument is printed. This, however, does not apply to the `-c' option. That option is always combined with the rest of command line by logical AND. 

The pseudo-user `~reboot' logs in on every reboot of the network access server. 

If radlast is interrupted, it indicates to what date the search had progressed. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.3 radzap 
radzap searches the Radius accounting database for matching login records and closes them. 

At least one of the options `-n', `-p', or the user name must be specified. If they are used in conjunction, they are taken as if joined by the logical AND operation. 

radzap operates in two modes: silent and confirm. The silent mode is enabled by default. When run in this mode, radzap deletes every record that matches the search conditions given. 

In confirm mode radzap will ask for a confirmation before zapping each matching record. Any line beginning with a `y' is taken as a positive response; any other line is taken as a negative response. 

The confirm mode is toggled by the command line option `-c'. 


Syntax 
  radzap [options] [username]

 


Options are: 


`-c' 
`--confirm' 
Enable confirm mode. 
`-d dir' 
`--directory dir' 
Specify alternate configuration directory. Default is `/usr/local/etc/raddb'. 
`-f file' 
`--file file' 
Operate on file instead of the default `RADLOG/radutmp'. 
`-l dir' 
`--log-directory dir' 
Search the file `radutmp' in the given directory. 
This option is deprecated. It is currently retained for backward compatibility with previous versions. 

`-q' 
`--quiet' 
Disable confirm mode. 
`-h' 
`--help' 
Display a short help summary, and exit. 
`-n name' 
`--nas name' 
Specify NAS name to zap user from. 
`-p port' 
`--port port' 
Specify the port number of the session to be zapped. The port number can be specified either in its full form, e.g. radzap -p S02, or in its short form, e.g. radzap -p 2. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.4 radgrep 
This utility allows one to quickly look up the user in the Radius accounting database, using a regular expression match. radgrep scans the output of radwho utility and outputs only the lines that match given regular expressions. 


Syntax 
radgrep accepts two sets of options separated by `--' (double hyphen). The first subset is passed as the command line to the radwho utility. The second one is passed to grep. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.5 radping 
This utility is a shell program that determines the user's framed IP and runs ping on that address. 


Syntax 
  radping username
radping -c calling-station-id

 

The second way of invoking the program allows one to use the calling station ID to indicate the user. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.6 radauth 
The radauth utility sends the Radius server an Access-Request packet and displays the result it gets. If the server responds with Access-Accept radauth can also send an Accounting-Request thereby initiating user's session. 

The utility is a radtest program. See section 13.2.12 Sample Radtest Program, for the detailed discussion of its internals. 


Invocation 
  radauth [options] [command] user-name [password]

 


Options are: 


`-v' 
Print verbose descriptions of what is being done. 

`-n nas-ip' 
Set NAS IP address 

`-s sid' 
Set accounting session ID 

`-P port' 
Set NAS port number. 
<FIXME> GNU long options are not yet supported </> 

Valid commands are: 


auth 
Send only Access-Request. This is the default. 

acct 
Send Access-Request. If successfull, send Accounting-Request with Acct-Status-Type = Start. 

start 
Send Accounting-Request with Acct-Status-Type = Start. 

stop 
Accounting-Request with Acct-Status-Type = Stop. 
The program determines which Radius server to use, the authentication port number, and the shared secret, following the procedure common to all client scripts (see section 13.1 Client Configuration). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.7 radctl 
Radctl is a control interface to the radiusd daemon. It allows the user running it to query radiusd about various aspects of its work and to issue administrative commands to it. The syntax is 

  radctl command [args]

 


where command is a command telling radctl which actions to take, and args are optional arguments to the command. Only one command can be specified per invocation. 

The valid commands are as follows: 


start [args] 
If radiusd is not running already, it is started. When present, args are passed as the command line to the server. 

stop 
Stops running radiusd. 

restart [args] 
Stops the server and then starts it again. When present, args are passed as the command line to the server. 

reload 
Causes the running radiusd server to reread its configuration files. 

dumpdb 
Tells radiusd to dump its user hash table into the file `radlog/radius.parse'. This can be used for debugging configuration files. 

which 
Reports the running version of radiusd. This command shows the line of ps(1) describing the running copy of radiusd program. The exact look depends on the version of operating system you are running. Please refer to "man ps" for more detail on ps output. 
Here is an example of what radctl which prints on GNU/Linux: 

  19692 ?        01:53:11 radiusd

 


Here, first field is the PID of the process, second field (`?') indicates that the running program has detached from the controlling terminal, the third field gives total amount of CPU time used by the program, and, finally, the last field shows the full name under which the command was invoked. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.8 builddbm 

Usage 
builddbm converts the plaintext Radius users database into DBM files. Some versions of the Radius daemon have used this to speed up the access to the users database. However, with GNU Radius things go the other way around. The server reads the entire plaintext database, converts it into internal form, and stores into a hash table, which provides for fast access. Actually, using a DBM version of the users database slows down the access unless the machine that runs the Radius daemon is short of address space for the daemon to store the users database. 

Syntax 
When used without arguments, the builddbm utility attempts to convert the file `raddb/users' to `raddb/users.db' or to the pair `raddb/users.pag', `raddb/users.dir', depending on the version of the DBM library used. 
If used with one argument, that argument is taken as the name of the plaintext database file to operate upon. 

Use the following command line options to modify the operation of buildbm: 


`-d dir' 
Specifies alternate directory for the Radius configuration files. This defaults to `/usr/local/etc/raddb'. 

`-h' 
Outputs short usage summary and exits with 0 exit code. 

--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

12.9 radscm: A Guile Interface to Radius Functions 
radscm is a Scheme interpreter based on Guile with the addition of special functions and variables for communicating with radiusd. This chapter concentrates on the special features provided by radscm. Refer to Guile documentation for information about Scheme and Guile (see section `Overview' in The Guile Reference Manual). 


Variables 

Variable: %raddb-path 
A path to the Radius configuration directory. 


Function: rad-server-list 
A list of radius servers. Each element of the list is: 

  (list id-str host-str secret-str auth-num acct-num
      cntl-num)

 


where the arguments are as follows: 

id-str  Server ID 
host-str  Server hostname or IP 
secret-str  Shared secret key to use 
auth-num  Authentication port number 
acct-num  Accounting port number 
cntl-num  Control channel port number 
Thus, each entry can be used as an argument to rad-client-set-server or rad-client-add-server. 


Functions 

Function: rad-send-internal port-number code-number pair-list 
Sends the request to currently selected server. Arguments are: 


port-number 
Port number to use. These values are allowed: 
0  Authentication port 
1  Accounting port 
2  Control port 
The actual port numbers are those configured for the given server. 

code-number 
Request code. 
pair-list 
List of attribute-value pairs. Each pair is either           (cons attr-name-str value)

 
or           (cons attr-number value)

 

Return: On success,           (list return-code-number pair-list)

 
On failure,           '()

 




Function: rad-send port-number code-number pair-list . verbose 
Sends a radius request. Actually it does the same work as rad-send-internal, but if verbose is specified, the verbose report about interaction with the radius server is printed. 


Function: rad-client-list-servers 
List currently configured servers. Two columns for each server are displayed: server ID and IP. 


Function: rad-get-server 
Returns the ID of the currently selected server. 


Function: rad-client-set-server list 
Selects for use the server described by list. Here list takes the form 

  (list id-str host-str secret-str auth-num acct-num
      cntl-num)

 
where the elements are as follows: 

id-str  Server ID 
host-str  Server hostname or IP 
secret-str  Shared secret key to use 
auth-num  Authentication port number 
acct-num  Accounting port number 
cntl-num  Control channel port number 



Function: rad-client-add-server list 
Adds the server described by list to the list of active servers. Here list takes the form 

  (list id-str host-str secret-str auth-num acct-num
      cntl-num)

 


where the elements are as follows: 

id-str  Server ID 
host-str  Server hostname or IP 
secret-str  Shared secret key to use 
auth-num  Authentication port number 
acct-num  Accounting port number 
cntl-num  Control channel port number 



Function: rad-read-no-echo prompt-str 
Prints the given prompt-str, disables echoing, reads a string up to the next newline character, restores echoing, and returns the string entered. This is the interface to the C getpass(3) function. 


Function: rad-client-source-ip ip-str 
Sets the IP to be used as source. ip-str can be either an IP in dotted-quad form or a hostname. 


Function: rad-client-timeout number 
Sets the timeout in seconds for waiting for a server reply. 


Function: rad-client-retry number 
Sets the number of retries for sending requests to a Radius server. 


Function: rad-format-code dest-bool code-number 
Format a radius reply code into a human-readable form. dest-bool has the same meaning as in format (see section `Formatted Output' in The Guile Reference Manual.) 


Function: rad-format-pair dest-bool pair 
Format a radius attribute-value pair for output. dest-bool has the same meaning as in format. pair is either                   (cons name-str value)

 
or                   (cons attr-number value)

 
where value may be of any type appropriate for the given attribute. 


Function: rad-print-pairs dest-bool pair-list 
Output the radius attribute-value pairs from pair-list. dest-bool has the same meaning as in format. pair-list is a list of pairs in the form 

                  (cons name-str value)

 
or 

                  (cons attr-number value)

 
where value may be of any type appropriate for the given attribute. 

All Reply-Message pairs from the list are concatenated and displayed as one. 



Function: rad-format-reply-msg pair-list . text 
Concatenate and print text from all Reply-Message pairs from pair-list. If text is specified, it is printed before the concatenated text. 


Function: rad-list-servers 
For each server from rad-server-list, print its ID and hostname or IP. 


Function: rad-select-server ID-STR 
Select the server identified by id-str as a current server. The server data are looked up in rad-server-list variable. 


Function: rad-add-server id-str 
Add the server identified by id-str to the list of current servers. The server data are looked up in rad-server-list variable. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13. Client Package 
Beside the Radius server and accompanying utilities, GNU Radius provides a set of utilities to be used as Radius clients. 

The following sections describe in detail the parts of the Radius client package. 


13.1 Client Configuration    Configuration file is common for all client utilities. 
13.2 radtest    Radius client shell. 
13.3 radsession    Send arbitrary requests to Radius server. 
13.4 nas.scm    A NAS implementation for GNU/Linux machines. 
13.5 pam_radius.so    A PAM module for authentication via Radius. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.1 Client Configuration 
All programs from the client package share the same configuration file: `raddb/client.conf'. The file uses simple line-oriented syntax. Empty lines are ignored; the command `#' introduces an end-of-line comment. 

The source IP is introduced with the source_ip statement. Its syntax is: 

  source_ip ip-addr

 


where ip-addr must be the IP in dotted-quad notation. 

The Radius server to send the requests to is introduced with server statement: 

  server name ip-addr secret auth-port acct-port

 


Its parts are: 


name 
The server name. It is reserved for further use. 
ip-addr 
The server IP. 
secret 
The shared secret to be used when sending requests to this server. 
auth-port 
The authentication port number. 
acct-port 
The accounting port number. 
If several server statement are present, they are tried in turn until one of them replies to the request. 

The amount of time a client program waits for the reply from a server is configured using the timeout statement: 

  timeout number

 


If the program does not receive any response within number seconds, it assumes the server does not respond and either retries the transmission or tries the next available server. The number of retries is set with the retry statement: 

  retry number

 


The example `raddb/client.conf' follows: 

  server first 10.11.10.1 secret 1645 1646
server second 10.11.10.1 secret 1645 1646
source_ip 127.0.0.1
timeout 3
retry 5

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2 radtest 
Radtest is a radius client shell, providing a simple and convenient language for sending requests to RADIUS servers and analyzing their reply packets. 


13.2.1 Invoking radtest     
13.2.2 Literal Values     
13.2.3 Reserved Keywords     
13.2.4 Variables     
13.2.5 Positional Parameters     
13.2.6 Expressions     
13.2.7 Function Definitions     
13.2.8 Interacting with Radius Servers     
13.2.9 Conditional Statements     
13.2.10 Loops     
13.2.11 Built-in Primitives     
13.2.12 Sample Radtest Program     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.1 Invoking radtest 
(This message will disappear, once this node revised.) 

`-a variable=value' 
`--assign=variable=value' 
Assign a value to variable. See section 13.2.4.5 Assignment Options, for a detailed discussion. 

`-f file' 
`--file=file' 
Read input from file. Stops further processing of the command line. 

`-i' 
`--no-interactive' 
Disable interactive mode. 

`-n' 
`--dry-run' 
Check the input file syntax and exit. 

`-q' 
`--quick' 
Do not read the configuration file. <FIXME> </> 

`-r number' 
`--retry=number' 
Set number of retries. 

`-s server' 
`--server=server' 
Set radius server parameters. 

`-t number' 
`--timeout=number' 
Set timeout 

`-v' 
`--verbose' 
Verbose mode 

`-x debugspec' 
`--debug=debugspec' 
Set debugging level 

`-d dir' 
`--directory dir' 
Specify alternate configuration directory. Default is `/usr/local/etc/raddb'. 

`-L' 
`--license' 
Print license and exit. 

`-?' 
`--help' 
Print short usage summary 

`--usage' 
Print even shorter usage summary. 

`-V' 
`--version' 
Print program version. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.2 Literal Values 
There are four basic data types in radtest language: integer, ipaddr, string and avlist. 


13.2.2.1 Numeric Values    Integers and IP addresses. 
13.2.2.2 Character Strings     
13.2.2.3 Lists of A/V pairs     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.2.1 Numeric Values 
Integer means a signed integer value in the range -2147483648..2147483647. 

Ipaddr is an unsigned integer value suitable for representing IPv4 addresses. These can be input either as decimal numbers or as IP addresss in usual "dotted-quad" notation. 

As a convenience measure, RADIUS request code names can be used in integer context. The following table lists currently defined request names with their integer codes: 

Access-Request  1 
Access-Accept  2 
Access-Reject  3 
Accounting-Request  4 
Accounting-Response  5 
Accounting-Status  6 
Password-Request  7 
Password-Ack  8 
Password-Reject  9 
Accounting-Message  10 
Access-Challenge  11 
Status-Server  12 
Status-Client  13 
Ascend-Terminate-Session  31 
Ascend-Event-Request  33 
Ascend-Event-Response  34 
Ascend-Allocate-IP  51 
Ascend-Release-IP  52 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.2.2 Character Strings 
String is an arbitrary string of characters. Any input token consisting of letters of Latin alphabet, decimal digits, underscores dashes and dots and starting with a Latin alphabet letter or underscores is considered a string. To input strings containing other letters, surround them by double quotes. The following are valid strings: 

  A-string
"String, containing white space"

 


The double quote character `"' must be preceeded by a backslash `\' if it is part of a string: 

  "Always quote \" character"

 


Generally speaking, `\' is an escape character, that alters the meaning of the immediately following character. If it is located at the end of the line, it allows to input newline character to strings: 

  "This string contains a \
newline character."

 


Other special escape sequences are: 


\a 
Audible bell character (ASCII 7) 
\b 
Backspace (ASCII 8) 
\e 
Escape character (ASCII 27) 
\f 
Form feed (ASCII 12) 
\n 
Newline (ASCII 10) 
\r 
Carriage return (ASCII 13) 
\t 
Horizontal tab (ASCII 9) 
\\ 
Backslash 
\ooo 
(`o' represents an octal digit) A character whose ASCII value is represented by the octal number `ooo'. 
\xHH 
\XHH 
(`H' represents a hex digit) A character whose ASCII value is represented by the hex number `HH'. 
If the character following the backslash is not one of those specified, the backslash is ignored. 

An important variant of string is a numeric string, or STRNUM for short. A numeric string is a string that can be converted to a number, for example "+2". This concept is used for type conversion between integer and string values. 

Another way to represent strings is using here document syntax. Its format is as follows: 

  <<[-]delimiter
  text
delimiter

 


Delimiter is any word you choose to delimit the text, text represent the text of the string. If delimiter is prepended by a dash, any leading tabulation characters will be removed from text. This allows for natural indentation of `here document' constructs. 

The `here document' construct is especially useful to represent strings containing embedded newlines, as shown in the example below: 

  print <<EOT
usage: foo [OPTIONS] [NAME...]
OPTIONS are:
  -h            Print this help list.
EOT

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.2.3 Lists of A/V pairs 
Avlist are whitespace or comma-separated lists of RADIUS attribute-value pairs. A syntax for A/V pair is 

  name op value

 


where name is attribute name, op is a comparison operator (`=', `!=', `<', `<=', `>', `>='), and value is any valid radtest data or expression. An A/V pair list must be enclosed in parentheses. This is an example of an A/V pair list consisting of two pairs: 

  ( User-Name = "test" NAS-IP-Address = 10.10.10.1 )

 


An empty pair list is represented by a pair of parentheses: (). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.3 Reserved Keywords 
The following keywords are reserved in radtest: 

  acct, and, auth, begin, break, case, continue, 
do, else, end, exit, expect, getopt, if,       
in, input, not, or, print, return, send,     
set, shift, while    

 


The reserved keywords may be used as variable names, provided that the following requrements are met: 


In assignment, these names are quoted using single quotes. 
  'case' = 1

 



When dereferencing, the use of curly braces is obligatory: 
  ${case} + 2

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.4 Variables 
Variables are means of storing data values at one point of your program for using them in another parts of it. Variables can be assigned either in the program itself, or from the radtest command line. 


13.2.4.1 Using Variables     
13.2.4.2 Variable Assignments     
13.2.4.3 Dereferencing Variables     
13.2.4.4 Accessing Elements of A/V Pair Lists     
13.2.4.5 Assignment Options     
13.2.4.6 Built-in Variables     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.4.1 Using Variables 
The name of a variable must be a sequence of letters, digits, underscores and dashes, but it may not begin with a digit or dash. Notice, that in contrast to the majority of programming languages, use of dashes (minus signs) is allowed in user names. This is because traditionally RADIUS attribute names contain dashes, so extending this practice to variable names makes radtest programs more consistent. On the other hand, this means that you should be careful when using minus sign as a subtraction operator (see minus-ambiguity). Case is significant in variable names: a and A are different variables. 

A name of a variable may coincide with one of radtest reserved keywords. See section 13.2.3 Reserved Keywords, for description on how to use such variables. 

A few variables have special built-in meanings (see section 13.2.4.6 Built-in Variables). Such variables can be assigned and accessed just as any other ones. All built-in variables names are entirely upper-case. 

Variables are never declared, they spring into existence when an assignment is made to them. The type of a variable is determined by the type of the value assigned to it. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.4.2 Variable Assignments 
An assignment stores a new value into a variable. It's syntax is quite straightforward: 

  variable = expression

 


As a result of the assignment, the expression is evaluated and its value is assigned to variable. If variable did not exist before the assignment, it is created. Otherwise, whatever old value it had before the assignment is forgotten. 

It is important to notice that variables do not have permanent types. The type of a variable is the type of whatever value it currently holds. For example: 

  foo = 1
print $foo => 1
foo = "bar"
print $foo => bar
foo = ( User-Name = "antonius" NAS-IP-Address = 127.0.0.1 )
print $foo => ( User-Name = "antonius" NAS-IP-Address = 127.0.0.1 )

 


Another important point is that in radtest, assignment is not an expression, as it is in many other programming languages. So C programmers should resist temptation to use assignments in expressions. The following is not correct: 

  x = y = 1

 


Finally, if the variable name coincides with one of radtest keywords, it must be enclosed in single quotes: 

  'case' = 1

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.4.3 Dereferencing Variables 
Dereferencing a variable means accessing its value. The simplest form of dereferencing is by prepending a dollar sign to the variable name: 

  foo = 1
print foo => foo
print $foo => 1

 


Notice, that in the example above, the first print statement understands foo as a literal string, whereas the second one prints the value of the variable. 

Dereferencing an undefined variable produces error message: 

  print $x error--> variable `x' used before definition

 


Optionally, the variable name may be surrounded by curly braces. Both $foo and ${foo} are equivalent. The use of the latter form is obligatory only when the variable name coincides with one of the reserved keywords (see section 13.2.3 Reserved Keywords). It also can be used to resolve ambiguity between using dash as a part of user name and as a subtraction operator: 

  long-name = 2
$long-name => 2
$long-name-1 error--> variable `long-name-1' used before definition
${long-name}-1 => 1
$long-name - 1 => 1

 


We recommend to always surround `-' with whitespace when it is used as arithmetic operator. 

The ${} notation also permits some operations similar to shell variable substitution. 


${variable:-text} 
Use default values. If variable is unset, return text, otherwise return the value of the variable. 
  $x error--> variable `x' used before definition
${x:-1} => 1
x = 2
${x:-1} => 2

 



${variable:=text} 
Assign default values. If variable is unset, text is assigned to it. The expression always returns the value of the variable. 
  $x error--> variable `x' used before definition
${x:=1} => 1
$x => 1

 



${variable:?text} 
Display error if unset. If variable is unset, text is written to the standard error (if text is empty, the default diagnostic message is used) and further execution of the program is aborted. Otherwise, the value of variable is returned. 
  $x error--> variable `x' used before definition
${x:?} error--> x: variable unset
${x:?foobar} error--> foobar

 



${variable::text} 
Prompt for the value if unset. If variable is unset, radtest prints text (or a default message, if it is empty), reads the standard input up to the newline character and returns the value read. Otherwise, the value of the variable is returned. This notation provides a convenient way for asking user to supply default values. 
  ${x::} -| (<teletype>:1)x?
${x::Enter value of x: } -| Enter value of x: 

 



${variable:&text} 
Prompt for the value with echo turned off if unset. This is similar to the ${variable::text}, with the exception that the input value will not be echoed on the screen. This notation provides a convenient way for asking user to supply default values for variables (such as passwords, shared secrets, etc.) while preventing them from being compromised. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.4.4 Accessing Elements of A/V Pair Lists 
Elements of an avlist are accessed as if it were an array, i.e.: 

  $variable [ attribute-name ]

 


If the attribute attribute-name is of string data type and variable may contain more than one pair with this attribute, adding an asterisk after attribute-name returns concatenated values of all such pairs: 

  $variable [ attribute-name * ]

 


Examples: 

  x = (NAS-Port-Id = 127.0.0.1 \
     Reply-Message = "a long"
     Reply-Message = " string"

$x[NAS-Port-Id] => 127.0.0.1
$x[Reply-Message] => "a long"
$x[Reply-Message*] => "a long string"

 


<FIXME> How to get nth instance of an attribute? What gets returned if there is no such attribute in the list? </> 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.4.5 Assignment Options 
You can set any radtest variable from the command line. There are two ways of doing so. 

First, you can use variable assignment option `--assign' (or `-a'). Its syntax is: 

  --assign variable=text
-a variable=text

 


For example: 

  radtest -a foobar=5

 


Another way is useful when you load a radtest program by `--file' or `-f'. This second way consists in including a variable assignment in the form 

  variable=text

 


in the command line after the script name. For example: 

  radtest -f myprog.rad foo=5 addr=127.0.0.1

 


This method is especially useful for executable scripts that are run using #! shell magic. Consider a simple script: 

  #! /usr/local/bin/radtest -f
print $addr

 


The value of addr can be given to the script from the command line as in the example below: 

  myprog.rad addr=127.0.0.1

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.4.6 Built-in Variables 
The following variables are predefined: 


_ 
(an underscore character) 
Contains the result of last evaluated expression. 


REPLY_CODE 
Contains the last reply code received from the RADIUS server (integer). 

REPLY 
Contains the A/V pairs lastly received from the RADIUS server (avlist). 

SOURCEIP 
Contains the source IP address of the RADIUS client (ipaddr). By default, it equals the IP address set via source_ip statement in your `client.conf' file (see section 13.1 Client Configuration). 

INPUT 
The value of the input read by input statement (see section input). 

OPTVAR 
The option obtained by the recent call to getopt (see section getopt). 

OPTARG 
Argument to the option obtained by the recent call to getopt. 

OPTIND 
Index of the next command line argument to be processed by getopt. If the last call to getopt returned false, OPTIND contains index of the first non-optional argument in the command line. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.5 Positional Parameters 
Normally radtest stops parsing its command line when it encounters either first non-optional argument (i.e. the one not starting with dash), or an argument consisting of two dashes. The rest of the command line starting from the first non-optional argument forms positional parameters. These parameters are said to form the top-level environment. 

Similarly, when invoking a user-defined function (see section 13.2.7 Function Definitions), arguments passed to it are said to form the current environment of the function. These arguments are positional parameters for this function. 

Positional parameters are assigned numbers starting from 1. To access (dereference) a positional parameter, the syntax $n is used, where n is the number of the parameter. Alternative forms, such as ${n} or ${n:-text}, can also be used. These work exactly as described in 13.2.4.3 Dereferencing Variables). 

The number of positional parameters can be accessed using a special notation $#. 

Several things need to be mentioned: 


All top-level positional parameters have string data type, whereas the types of positional parameters in a function current environment are determined before inoking the function. 

Special notion $0 returns the name of the function being evaluated. When used in the top-level environment, it returns the name of radtest program as given by `--file' (`-f') option. 

Dereferencing non-existing parameter returns empty string. This differs from dereferencing non-existing variable, which results in error. 

AWK programmers should note that assignments (see section 13.2.4.5 Assignment Options) are not included in the top level environment (see example below). 
For example, suppose you run: 

  radtest -f script.rad name foo=bar 5

 


Then, the top-level environment of program `script.rad' consists of the following variables: 

  $0 => script.rad
$1 => name
$2 => 5

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.6 Expressions 
An expression evaluates to a value, which can be printed, assigned to a variable, used in a conditional statement or passed to a function. As in other languages, expressions in radtest include literals, variable and positional parameter dereferences, function calls and combinations of these with various operators. 


13.2.6.1 Arithmetic Operations     
13.2.6.2 String Operations     
13.2.6.3 Operations on A/V Lists     
13.2.6.4 Comparison Operations     
13.2.6.5 Boolean Operations     
13.2.6.6 Conversion Between Data Types     
13.2.6.7 Function Calls     
13.2.6.8 Operator Precedence (How Operators Nest)     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.6.1 Arithmetic Operations 
Radtest provides the common arithmetic operators, which follow normal precedence rules (see section 13.2.6.8 Operator Precedence (How Operators Nest)), and work as you would expect them to. The only notable exception is subtraction operator (minus) which can be used as part of a variable or attribute name, and therefore expressions like $x-3 are ambiguous. This expression can be thought of either as a dereference of the variable x-3 (see section 13.2.4.3 Dereferencing Variables), or as subtraction of the value 3 from the value of the variable x. Radtest always resolves this ambiguityin the favor of variable dereference. Therefore we advise you to always surround minus sign by whitespace, if it is used as a subtraction operator. So, instead of $x-3, write $x - 3. For other methods of solving this ambiguity, See minus-ambiguity. 

This table lists the arithmetic operators in order from highest precedence to lowest: 


- x 
Negation. 

+ x 
Unary plus. This is equivalent to x. <FIXME> Should this imply converting x to integer? </> 

x * y 
Multiplication. 

x / y 
Division. 

x % y 
Remainder. 

x + y 
Addition. 

x - y 
Subtraction. 

Unary plus and minus have the same precedence, the multiplication, division and remainder all have the same precedence, and addition and subtraction have the same precedence. 

If x and y are of different data types, their values are first coerced to a common data type, selected using a set of rules (see section 13.2.6.6 Conversion Between Data Types). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.6.2 String Operations 
There is only one string operation: concatenation. It is represented by plus sign, e.g.: 

  "string" + "ent" => "stringent"

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.6.3 Operations on A/V Lists 
(This message will disappear, once this node revised.) 
The following operations are defined on A/V lists: 


x + y 
Addition. The A/V pairs from y are added to x, honoring the respective pairs additivity (see section additivity). For example: 
  ( User-Name = "foo" ) + ( Password = "bar" )
=> ( User-Name = "foo" Password = "bar" )

( User-Name = "foo" Service-Type = Login-User ) + \
 ( Service-Type = Framed-User Password = "bar" )
=> ( User-Name = "foo" \
          Service-Type = Framed-User \
          Password = "bar" )

 



x - y 
Subtraction. The result of this operation is an A/V list consisting of pairs from x, which are not found in y. 
  ( User-Name = "foo" Service-Type = Login-User ) - \
( Service-Type = Framed-User )
=> ( User-Name = "foo" )

 


Notice, that only attribute name matters, its value is ignored. <FIXME> Is this correct? Is there a better way to put it? should we provide an operation that whould compare both attribute number and its value? </> 


x % y 
Intersection. The result of this operation is an A/V pair list consisting of pairs from x which are also present in y. 
  ( User-Name = "foo" Service-Type = Login-User ) - \
( Service-Type = Framed-User )
=> ( Service-Type = Login-User )

 


<FIXME> Same as above. </> 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.6.4 Comparison Operations 
Comparison expressions compare operands for relationships such as equality. They return boolean values, i.e. true or false. The comparison operations are nonassociative, i.e. they cannot be used together as in: 

  # Wrong!
1 < $x < 2

 


Use boolean operations (see section 13.2.6.5 Boolean Operations) to group comparisons together. 

Comparison operations can only be used in conditional expressions. 

This table lists all comparison operators in order from highest precedence to lowest (notice, however, the comment after it): 


x = y 
True if x is equal to y. C and AWK programmers, please note single equal sign! 

x != y 
True if x is not equal to y. 

x < y 
True if x is less than y. 

x <= y 
True if x is less than or equal to y. 

x > y 
True if x is greater than y. 

x >= y 
True if x is greater than or equal to y. 
Operators = and != have equal precedence. Operators <, <=, >, >= have equal precedence. 

Most operators are defined for all radtest data types. However, only = and != are defined for avlists. Using any other comparison operator with avlists produces error. 

If x and y are of different data types, their values are first coerced to a common data type, selected using a set of rules (see section 13.2.6.6 Conversion Between Data Types). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.6.5 Boolean Operations 
A boolean operation is a combination of comparison expressions. Boolean operations can only be used in conditional expressions. 

This table lists all comparison operators in order from highest precedence to lowest. 


not x 
! x 
True if x is false. 

x and y 
True if both x and y are true. The subexpression y is evaluated only if x is true. 

x or y 
True if at least one of x or y is true. The subexpression y is evaluated only if x is false. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.6.6 Conversion Between Data Types 
(This message will disappear, once this node revised.) 
The unary negation operand is always converted to integer type: 

  -(1 + 1)  => -2
-(127.0.0.1 + 2) => -2130706435
- ("1" + "1") => -11
- "text" error--> cannot convert string to integer

 


The unary not operand is converted using the following rules: 


If the operand is integer, no conversion is performed. 
If the operand is STRNUM (see STRNUM) or ipaddr, it is converted to integer. 
If the operand is string (but is not STRNUM), the result of not is true only if the operand is an empty string. 
If the operand is avl, the result of not is true if the list is empty. 
Examples: 

  not 0 => 1
not 10 => 0
not "23" => 0
not "0" => 1
not "text" => 0
not "" => 1
not 127.0.0.1 => 0
not 0.0.0.0 => 1

 


When operands of two different data types are used in a binary operation, one of the operands is converted (cast) to another operand's type according to the following rules: 


If one of the operands is literal, radtest attemtps to convert another operand to the literal data type. If this attempt fails, it goes on to rule 2. 
If one of operands is STRNUM (see STRNUM) and another is of numeric data type (i.e. either integer or ipaddr), the latter is converted to string representation. 
If one of the operands is ipaddr and another is integer, the latter is converted to ipaddr. 
Otherwise, if one of the operands is string, the second operand is also converted to string. 
Otherwise, the two operands are incompatible. Radtest prints appropriate diagnostics and aborts execution of the current statement. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.6.7 Function Calls 
A function is a name for a particular sequence of statements. It is defined using special definition syntax (see section 13.2.7 Function Definitions). Normally a function return some value. The way to use this value in an expression is with a function call expression, which consists of the function name followed by a comma-separated list of arguments in parentheses. The arguments are expressions which provide values for the function call environment (see section 13.2.5 Positional Parameters. When there is more than one argument, they are separated by commas. <FIXME> Actually, commas in the argument list are optional. At least, now... </> If there are no arguments, write just `()' after the function name. Here are some examples: 

  foo()             no arguments
bar(1)            one argument
bar(1, "string")  two arguments

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.6.8 Operator Precedence (How Operators Nest) 
Operator precedence determines the order of executing operators, when different operators appear close by in one expression. For example, * has higher precedence than +; thus, a + b * c means to multiply b and c, and then add a to the product. 

You can overrule the precedence of the operators by using parentheses. You can think of the precedence rules as saying where the parentheses are assumed to be if you do not write parentheses yourself. Thus the above example is equivalent to a + (b * c). 

When operators of equal precedence are used together, the leftmost operator groups first. Thus, a - b + c groups as (a - b) + c. 

This table lists radtest operators in order from highest precedence to the lowest: 


$ 
Dereference. 

(...) 
Grouping. 

+ - not ! 
Unary plus, minus. Unary boolean negation. 

* / % 
Multiplication, division, modulus. 

+ - 
Addition, subtraction. 

< <= = != > >= 
Relational operators. 

and 
Logical `and'. 

or 
Logical `or'. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.7 Function Definitions 
A function is a name for a particular sequence of statements. The syntax for the function definition is: 

  name
begin
  ...
end

 


where name is function name and `...' represent a non-empty list of valid radtest statements. 

Notice that newline characters are obligatory after name, begin and before the final end keyword. 

If the function accepts arguments, these can be referenced in the function body using $n notation (see section 13.2.5 Positional Parameters). To return the value from the function return statement is used. 

For example, here is a function that computes sum of the squares of its two arguments: 

  hypo
begin
        return $1*$1 + $2*$2
end

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.8 Interacting with Radius Servers 
Radtest provides two commands for interaction with remote RADIUS servers. 

Command send sends request to the server specified in `raddb/client.conf'. Its syntax is: 

  send [flags] port-type code [expr-or-pair-list]

 


Optional flags can be used for fine-tuning the internals of send. You will seldom need to use these, unless you are developing GNU Radius. See section send, for the detailed description of these. 

The first obligatory argument, port-type, specifies which RADIUS port to send the request to. Specifying `auth' will send the request to the authentication port (see section auth-port); specifying `acct' will send it to the accounting port (see section acct-port). 

Argument code gives the request code. It is either a number or a symbolic request code name (see section 13.2.2.1 Numeric Values). 

The last argument, expr-or-pair-list is either a radtest expression evaluating to avlist or a list of A/V pairs. These pairs will be included in the request. 

Here are several examples: 

  # Send a Status-Server request without attributes.
send auth Status-Server

# Send an Access-Request with two attributes
send auth Access-Request User-Name = "foo" User-Password = "bar"

# Send an Accounting-Request, taking attributes from the variable
# attr
send acct Accounting-Request $attr

 


Command send stores the reply code into the variable REPLY_CODE and reply pairs into the variable REPLY (see section 13.2.4.6 Built-in Variables). <FIXME> How do I know if send has failed? </> 

Another primitive is expect. Expect takes at most two arguments: a request code (either numeric or symbolic, (see section 13.2.2.1 Numeric Values)) and optional list of A/V pairs (similar to send expr-or-pair-list argument). Expect check if these match current REPLY_CODE and REPLY values and if so, prints the string `PASS'. Otherwise, it prints `FAIL'. This command is designed primarily for use in GNU Radius testsuite. 

Expect is usually used right after send, as shown in the example below: 

  send auth Access-Request User-Name = "foo" User-Password = "bar"
expect Access-Accept Reply-Message = "Access allowed"

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.9 Conditional Statements 
(This message will disappear, once this node revised.) 
Radtest provides two kinds of conditional statements: if and case. 


If statement 
An if statement in its simplest form is: 

  if cond stmt

 


where cond is a conditional expression and stmt is a valid radtest statement. Optional newline may be inserted between cond stmt. 

In this form, if evaluates the condition and if it yields true, executes the statement. For example: 

  if $REPLY[NAS-IP-Address] = 127.0.0.1
   print "Request from localhost"

 


More complex form of this statement allows to select between the two statements: 

  if cond stmt-1 else stmt-2 

 


Here, stmt-1 will be executed if cond evaluates to true, and stmt-2 will be executed if cond evaluates to false. 

Notice, that an optional newline is allowed between cond and stmt-1 and right after else keyword. However, a newline before else constitutes an error. 

If several statements should be executed in a branch of the if statement, use compound statement as in the example below: 

  if $REPLY_CODE != Accounting-Response
begin
  print "Accounting failed.\n"
  exit 1        
end else
  print "Accounting succeeded.\n"

 


If statements can be nested to any depth. 


Case statement 
Case statement allows select a statement based on whether a string expression matches given regular expression. The syntax of case statement is: 

  case expr in
expr-1 ) stmt-1
expr-2 ) stmt-2
...
expr-n ) stmt-n
end

 


where expr is a control expression, expr-1, expr-2 etc. are expressions evaluating to extended POSIX regular expressions (for the detailed description of these see section `Regular Expression Library' in Regular Expression Library). 

Case statement first evaluates expr and converts it to string data type. Then it evaluates each expr-n in turn and tests if the resulting regular expression matches expr. If so, the statement stmt-n is executed and the execution of case statement finishes. 

The following example illustrates the concept: 

  case $COMMAND in
"auth.*")       authenticate($LIST, no)
"acct")         authenticate($LIST, yes)
".*")           begin
                  print "Unknown command."
                  exit 1
                end
end

 


Bourne shell programmers should notice that: 


Case statement ends with end, not esac. 
There is no need to put ;; at the end of each branch, 
Boolean operations are not allowed in expr-n. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.10 Loops 
(This message will disappear, once this node revised.) 
Two looping constructs are provided: while and do...while. 


While loop 
The syntax of a while loop is: 

  while cond
  stmt

 


Newline after cond is obligatory. 


Do...while loop 
  do
  stmt
while cond

 


As usual do...while loop differs from its while counterpart in that its stmt is executed at least once. 

The looping constructs can be nested to any depth. 

Two special statements are provided for branching within loop constructs. These are break and continue. 

Break statement stops the execution of the current loop statement and passes control to the statement immediately following it 

  while $x < 10
begin
  if $x < $y
     break
  ...
  x = $x + 1
end
print "OK\n"

 


In the example above, execution of break statement passes control to print statement. 

Break may also take an argument: a literal number representing the number of nested loop statements to break from. For example, the break statement in the sample code below will exit from the outermost while: 

  while $y < 10
begin
  while $x < 10
  begin
    if $x < $y
       break 2
    ...
    x = $x + 1 
  end
  ...
  y = $y + 1 
end  
print "OK\n"

 


Continue statement passes control to the condition of the current looping construct. When used with a numeric argument, the latter specifies the number of the nesting looping construct to pass control to (as with break, the innermost loop is considered to have number 1, so continue is equivalent to continue 1). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.11 Built-in Primitives 

Radtest built-in: getopt optstring [opt [arg [ind]]] 
Getopt is used to break up command line options for subsequent parsing. 

The only mandatory argument, optstring is a list of short (one-character) options to be recognized. Each short option character in optstring may be followed by one colon to indicate it has a required argument, and by two colons to indicate it has an optional argument. <FIXME> Document starting `+' and `-' chars. </> 

Each subsequent invocation of getopt processes next command line argument. Getopt returns true if the argument is an option and returns false otherwise. It stores the retrieved option (always with a leading dash) in the variable opt (OPTVAR by default). If the option has an argument, the latter is stored in the variable arg (OPTARG by default). Index of the next command line argument to be processed is preserved in the variable ind (OPTIND by default). 

The usual way of processing command line options is by invoking getopt in a condition expression of while loop and analyzing its return values within the loop. For example: 

  while getopt "hf:"
case $OPTVAR in
"-h")  print "Got -h option\n"
"-f")  print "Got -f option. Argument is " $OPTARG "\n"
".*")  begin
          print "Unknown option: " $OPTVAR "\n"
          exit 1
       end
  end
end

 




Radtest statement: input [expr name] 
Evaluates expr and prints its result on standard output. Then reads a line from standard input and assigns it to the variable name. 

If expr is given, name must also be present. 

If name is not given, variable INPUT is used by default. 



Radtest statement: set options 
Sets radtest command line options. Options should be a valid radtest command line (see section 13.2.1 Invoking radtest). 


Radtest statement: shift [expr] 
Shift positional parameters left by one, so that $2 becomes $1, $3 becomes $2 etc. $# is decremented. $0 is not affected. 

If expr is given, it is evaluated, converted to integer and used as shift value. Thus shift 2 shifts all positional parameters left by 2. 



Radtest statement: return [expr] 
Returns from the current function (see section 13.2.7 Function Definitions). If expr is present, it is evaluated and the value thus obtained becomes the function return value. 

It is an error to use return outside of a function definition. 



Radtest statement: break [n] 
Exit from within a loop.If n is specified, break from number levels. n must be >= 1. If n is greater than the number of enclosing loops, an error message is issued. 

See section 13.2.10 Loops, for the detailed discussion of the subject. 



Radtest statement: continue [n] 
Resume the next iteration of the enclosing loop. If n is specified, resume at the nth enclosing loop. n must be >= 1. If n is greater than the number of enclosing loops, an error message is issued. 

See section 13.2.10 Loops, for the detailed discussion of the subject. 



Radtest statement: exit [expr] 
Exit to the shell. If expr is specified, it is evaluated and used as exit code. Otherwise, 0 is returned to the shell. 


Radtest statement: print expr-list 
Evaluate and print expressions. Expr-list is whitespace or comma-separated list of expressions. Each expression is evaluated in turn and printed to the standard output. 


Radtest statement: send [flags] port-type code expr-or-pair-list 
Send a request to the RADIUS server and wait for the reply. Stores reply code in the variable REPLY_CODE and reply A/V pairs in the variable REPLY (see section 13.2.8 Interacting with Radius Servers). 

flags are a whitespace-separated list of variable assignments. Following variables are understood: 


repeat=n 
Unconditionally resend the request n times. 

id=n 
Specify the request ID. 

keepauth=1 
Do not alter request authenticator when resending the request. 

port-type 
Specifies which port to use when sending the request. Use `auth' to send the request to the authentication port (see section auth-port), and `acct' to send it to the accounting port (see section acct-port). 

code 
RADIUS request code. Either numeric or symbolic (see section 13.2.2.1 Numeric Values). 

expr-or-pair-list 
Specifies the A/V pairs to include in the request. This argument is either an expression evaluating to avlist, or an immediate avlist (see section 13.2.2.3 Lists of A/V pairs). In the latter case, the parentheses around the list are optional. 



Radtest statement: expect code [expr-or-pair-list] 
Test if REPLY_CODE matches code and, optionally, if REPLY matches expr-or-pair-list. If so, print the string `PASS', otherwise print `FAIL'. 

See section 13.2.8 Interacting with Radius Servers, for the detailed discussion of this statement. 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.2.12 Sample Radtest Program 
As an example, let's consider radauth program (see section 12.6 radauth). Its main purpose is to send authentication request to the remote server, analyze its reply and if it is positive, send an appropriate accounting record, thereby initiating user's session. Optionally, the script should also be able to send a lone accounting record. 

In the discussion below, we will show and explain subsequent parts of the script text. For the ease of explanation, each line of program text will be prepended by its ordinal line number. 


Parsing command line options 
The script begins as follows: 

    1 #! /usr/bin/radtest -f
  2 
  3 while getopt "n:s:P:hv"
  4 begin
  5   case $OPTVAR in
  6   "-n") NASIP = $OPTARG 
  7   "-s") SID = $OPTARG 
  8   "-P") PID = $OPTARG 
  9   "-v") set -v

 



1 
It is a pragmatic comment informing shell that it should run radtest in order to interpret the program. 

3 
This line starts option processing loop. Getopt (see section getopt) in line 3 analyzes each subsequent command line argument and if it is an option checks whether it matches one of the option letters defined in its first argument. The option letter will be returned in OPTVAR variable, its argument (if any) -- in OPTARG variable. 

4 -- 8 
OPTARG value is analyzed using case statement. Lines 6 -- 8 preserve OPTARG values in appropriate variables for later use. NASIP will be used as the value of NAS-IP-Address attribute, SID is the session id (Acct-Session-Id attribute), and PID is the port number (for NAS-Port-Id attribute. 

9 
This line sets `-v' option to the radtest interpreter (see section 13.2.1 Invoking radtest). 
The next piece of code handles `-h' and erroneous options: 

   10   "-h") begin
 11           print <<-EOT
 12            usage: radauth [OPTIONS] [COMMAND] login [password]
 13            Options are:
 14            -v         Print verbose descriptions of what is being done
 15            -n IP      Set NAS IP address
 16            -s SID     Set session ID
 17            -P PORT    Set NAS port number
 18            COMMAND is one of:
 19            auth       Send only Access-Request (default)
 20            acct       Send Access-Request. If successfull, send
 21                       accounting start request
 22            start      Send accounting start request
 23            stop       Send accounting stop request
 24            EOT
 25           exit 0
 26         end
 27   ".*") begin
 28           print "Unknown option: " $OPTVAR "\n"
 29           exit 1
 30         end
 31   end
 32 end

 



10 -- 26 
Print short description and exit, if the program is given `-h'. Notice that `here document' syntax is used to print the text (See section 13.2.2.2 Character Strings, for its description). The leading whitespace in lines 12 to 24 is composed of tabulation characters (ASCII 9), not usual space characters (ASCII 32), as required by `<<-' construct. 

27 -- 30 
These lines handle unrecognized options. 

31 
Closes case statement started on line 5 
32 
Closes compound statement started on line 4 

Checking Command Line Consistency 
   33 
 34 shift ${OPTIND}-1
 35 
 36 if $# > 3
 37 begin
 38         print "Wrong number of arguments."
 39         print "Try radauth -h for more info"
 40         exit 1
 41 end

 



34 
OPTIND keeps the ordinal number of the first non-optional argument. This line shifts off all the options processed by getopt, so that the first non-optional argument may be addressed by $1 notation. Notice use of curly braces to solve minus ambiguity (see minus-ambiguity). 

36 -- 41 
At this point we may have at most three arguments: command, user name, and password. If there are more, display the diagnostic message and exit the program. 
Next piece of code: 

   42
 43 case $1 in
 44 "auth|acct|start|stop") begin
 45                           COMMAND=$1
 46                           shift 1
 47                         end
 48 ".*")   COMMAND="auth"
 49 end
 50 
 51 LOGIN=${1:?User name is not specified. Try radauth -h for more info.}
 52 
 53 if ${NASIP:-} = ""
 54         NASIP=$SOURCEIP
 55 
 56 LIST = ( User-Name = $LOGIN NAS-IP-Address = $NASIP )

 



43 -- 48 
Check if a command is given. If so, store command name in the variable COMMAND and shift arguments by one, so login becomes argument $1. Otherwise, assume `auth' command. 

51 
If the user login name is supplied, store it into LOGIN variable. Otherwise, print diagnostic message and exit. 

53 -- 54 
Provide a default value for NASIP variable from the built-in variable SOURCEIP (see section 13.2.4.6 Built-in Variables) 

56 
The variable LIST will hold the list of A/V pairs to be sent to the server. This line initializes it with a list of two A/V pairs: User-Name and NAS-IP-Address. 

Defining Accounting Function 
Accounting function will be used to send accounting requests to the server. It is supposed to take a single argument: an avlist of A/V pairs to be sent to the server. 

   57 
 58 'acct'
 59 begin
 60   if ${SID:-} = ""
 61     input "Enter session ID: " SID
 62   if ${PID:-} = ""
 63     input "Enter NAS port ID: " PID
 64   send auth Accounting-Request $1 + \
            (Acct-Session-Id = $SID NAS-Port-Id = $PID)

 



58 -- 59 
These lines start the function definition. Notice quoting of the function name (`acct'): it is necessary because it coincides with a reserved keyword (see section 13.2.3 Reserved Keywords). 

60 -- 61 
If the value of SID (session ID) is not supplied, prompt the user to input it. 

62 -- 63 
If the value of PID (port ID) is not supplied, prompt the user to input it. 

64 
Send accounting request. The list of A/V pairs to send is formed by concatenating Acct-Session-Id and NAS-Port-Id attributes to the function's first argument. 
The final part of acct function analyzes the reply from the server: 

   65   if $REPLY_CODE != Accounting-Response
 66   begin
 67     print "Accounting failed.\n"
 68     exit 1  
 69   end
 70   print "Accounting OK.\n"
 71   exit 0
 72 end
 73

 


Notice, that acct never returns. Instead it exits with an error code indicating success or failure. 


Defining Authentication Function 
The purpose of the authentication function auth is to send an Access-Request to the server and perform some actions based on its reply. 

The function will take three arguments: 


$1 
The list of A/V pairs to include in the request. 

$2 
User password. 

$3 
This argument indicates whether accounting request must be sent after successful authentication. String `yes' means to send the accounting request, `no' means not to send it. 
The function is not expected to return. Instead it should exit to the shell with an appropriate error code. 

   74 'auth'
 75 begin
 76   send auth Access-Request $1 + (User-Password = $2)

 



74 -- 75 
Begin the function definition. Notice quoting of the function name (`auth'): it is necessary because it coincides with a reserved keyword (see section 13.2.3 Reserved Keywords). 

76 
Send the initial authentication request. The list of A/V pairs is formed by appending User-Password pair to the list given by the first argument to the function. 
The rest of the function analyzes the reply from the server and takes appropriate actions. Notice that if the server replies with an Access-Challenge packet, we will have to send subsequent authentication requests, so this piece of code is enclosed within a while loop. 

First, the function handles Access-Accept and Access-Reject replies: 

   77   while 1
 78   begin
 79     if $REPLY_CODE = Access-Accept
 80     begin
 81       print "Authentication passed. " + $REPLY[Reply-Message*] + "\n"
 82       if ${3:-no} = no
 83         exit 0
 84       'acct'($1 + ( Acct-Status-Type = Start ))
 85     end else if $REPLY_CODE = Access-Reject
 86     begin
 87       print "Authentication failed. " + $REPLY[Reply-Message*] + "\n"
 88       break

 



77 
Begin an "endless" while loop. It will eventually be exited either using break, or using exit (see below). 

79 -- 84 
Hanlde Access-Accept replies: 

81 
Print the reply message. Notice the use of `*' to print all the instances of Reply-Message attribute from the reply packet (see section 13.2.4.4 Accessing Elements of A/V Pair Lists). 

82 -- 83 
If the third argument is missing or is a string `no', exit indicating success (see section 13.2.4.3 Dereferencing Variables). 

84 
Otherwise, call acct function to perform accounting. The A/V pairs included in the accounting request are formed by adding Acct-Status-Type attribute to the list given by the first argument to the function. 

85 -- 88 
Handle Access-Reject replies. Print the reply message and break from the loop. 
Next piece of code deals with Access-Challenge replies. For simplicity we assume that such replies always carry user menus (See section 5.13 Login Menus -- `raddb/menus', for the description of these). So, upon receiving an Access-Challenge we should print out the menu, read the users selection and send back an Access-Request to the server. This part is the only one that actually continues the loop at line 77. 

   89     end else if $REPLY_CODE = Access-Challenge
 90     begin
 91       print $REPLY[Reply-Message*]
 92       input 
 93       send auth Access-Request \
 94         (User-Name = $LOGIN User-Password = $INPUT \
             State = $REPLY[State])

 



91 
Print the menu contents carrieb by Reply-Message attributes. There may be several instances of the attribute, hence the use of `*' to concatenate their values together. 

92 
Read the input from the user. The input will be stored in INPUT variable. See section 13.2.11 Built-in Primitives, for the description of input statement. 

93 -- 94 
Send an Access-Request packet with three attributes. User-Password contains the user reply, State contains the menu state from the server reply packet. 
Final part of the function:    95     end else begin
 96       print "Authentication failed. Reply code " + $REPLY_CODE + "\n"
 97       break
 98     end
 99   end
100   exit 1
101 end
102

 



95 -- 98 
Handle unknown reply codes. 

99 
Closes the while loop started on line 77. 

100 
Exit to the shell indicating failure. This statement will be reached only if a break is executed either on line 88 or on line 97. 

101 
Closes function definition started on lines 74 -- 75 

Final Part of Radauth Program 
The final part selects an action based on the user command and executes it. It is equivalent to the main function in a C program: 

  103 case ${COMMAND} in
104 "auth")   'auth'($LIST, ${2:&Password: }, no)
105 "acct")   'auth'($LIST, ${2:&Password: }, yes)
106 "start")  'acct'($LIST+(Acct-Status-Type = Start))
107 "stop")   'acct'($LIST+(Acct-Status-Type = Stop))
108 ".*")       begin
109               print "Unknown command. Try radauth -h for more info"
110               exit 1
111             end
112 end
113 
114 # End of radauth

 



103 
Select an action based on the value of COMMAND variable. 

104 -- 105 
Call auth function. If the second argument is given in the command line, its value is taken as user's password. Otherwise, the user is prompted for the password with the string `Password: '. The input is read with echo turned off to prevent the password from being compromised (the `:&' construct, see section 13.2.4.3 Dereferencing Variables). 

106 -- 107 
Call acct function for `start' and stop commands. 

108 -- 111 
Handle an unknown command verb. 

112 
Closes case statement from line 103. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.3 radsession 
radsession is a Guile script that sends authentication and accounting requests to the Radius server. To invoke the script, run 

  radsession options action

 


Possible actions are: 


`--auth' 
Send authentication request. 
`--start' 
Send accounting start request. 
`--stop' 
Send accounting stop request. 
Options determine the contents of the request's pairlist. They are: 


`-l STRING' 
`--login STRING' 
Set login name. 
`-p STRING' 
`--passwd STRING' 
Set password. 
`-n IP' 
`--nas IP' 
Set the value of NAS-IP-Address attribute. 
`-s STRING' 
`--sid STRING' 
Set the session ID (Acct-Session-Id attribute). 
`-P NUMBER' 
`--port NUMBER' 
Set the port number (NAS-Port-Id attribute). 
`-h' 
`--help' 
Print a short usage message and exit. 
`-v' 
`--verbose' 
Verbosely list the contents of the received reply. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.4 nas.scm 
nas.scm is a Guile program that allows one to convert a GNU/Linux box into a NAS. It requires Guile version 1.4 or better and PPP version 2.3.7 or better. 

To use it, you will basically need to do the following: 


Create links: 
  ln -s libexec/nas.scm /etc/ppp/ip-up
ln -s libexec/nas.scm /etc/ppp/ip-down

 


Here, libexec denotes the location of your libexec directory, where nas.scm is installed. If not overridden at configure time, it defaults to `prefix/libexec'. These links assure that ppp will invoke nas.scm when the user's session starts and ends, thus giving it a possibility to send accounting requests. 

Configure the file `raddb/client.conf'. 
Edit the file `raddb/nas.rc'. The supplied `nas.rc' template is tailored to work in most environments. The only variables you may need to change are nas-log-facility, specifying the syslog facility to be used for logging, and pppd-args, keeping the arguments to be given to ppp. 
Configure your `/etc/inittab' and getty. 
For example, if you use mgetty, then the `inittab' entries for dial-up lines will look like: 

  d0:345:respawn:/sbin/mgetty ttyS0 vt100
d1:345:respawn:/sbin/mgetty ttyS1 vt100
...

 


mgetty's `login.config' will then contain the following line: 

  *       -       -       /usr/local/libexec/nas.scm @

 


If you use agetty, then the `inittab' will contain (with the long lines split for readability) 

  d0:345:respawn:/sbin/agetty -mt60 \
   -l /usr/local/libexec/nas.scm 38400,19200,9600 \
   ttyS0 vt100
d1:345:respawn:/sbin/agetty -mt60 \
   -l /usr/local/libexec/nas.scm 38400,19200,9600 \
   ttyS1 vt100
...

 





--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

13.5 pam_radius.so 
pam_radius.so is a PAM module for Radius authentication. The module understands the following command line options: 


`audit' 
Enable audit information. 

`debug[=level]' 
Enable debugging information. The higher level is, the more debugging info is output. When omitted, level defaults to 100. 
Debugging levels equal to or greater than 10 compromise users' passwords, so use them sparingly. 


`use_authtok' 
Use the authentication token passed from the previous module in the stack. 

`confdir=path' 
Look for configuration files in path. The default is `$sysconfdir/etc/raddb'. 

`attr:' 
This keyword marks the end of command line options. The part of the command line after it is parsed as a whitespace-separated list of A/V pairs to be sent with the request. 

`service_type=type' 
This option is retained for compatibility with the 0.96 series of GNU Radius. It is equivalent to 
          attr: Service-Type=type

 


The pam_radius.so module logs its messages under LOG_AUTH syslog facility. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14. Attribute List 
The following sections describe the most frequently used Radius attributes. Each attribute is described as follows: 

  ATTRIBUTE name value type

 
 Users:  user-flags 
 Hints:  hints-flags 
 Huntgroups:  huntgroup-flags 
 Additivity:  additivity 
 Proxy propagated:  prop 


These values have the following meaning: 


name 
The attribute name. 
value 
The attribute number. 
type 
The attribute type. 
user-flags 
Syntax flags defining in which part of a `raddb/users' entry this attribute may be used. The flags consist of two letters: `L' means the attribute can be used in the LHS, `R' means it can be used in the RHS. 
hints-flags 
Syntax flags defining in which part of a `raddb/hints' entry this attribute may be used. 
huntgroup-flags 
Syntax flags defining in which part of a `raddb/huntgroups' entry this attribute may be used. 
additivity 
The additivity of the attribute determines what happens if a rule attempts to add to the pair list an attribute that is already present in this list. Depending on its value, the actions of the server are: 
Append 
New attribute is appended to the end of the list. 
Replace 
New attribute replaces the old. 
Drop 
New attribute is dropped. The old one remains in the list. 
prop 
Is the attribute propagated back to the NAS if the server works in proxy mode? 
The entry N/A for any of this fields signifies "not applicable". 


14.1 Authentication Attributes     
14.2 Accounting Attributes     
14.3 Radius Internal Attributes     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1 Authentication Attributes 
These are the attributes the NAS uses in authentication packets and expects to get back in authentication replies. These can be used in matching rules. 


14.1.1 CHAP-Password     
14.1.2 Callback-Id     
14.1.3 Callback-Number     
14.1.4 Called-Station-Id     
14.1.5 Calling-Station-Id     
14.1.6 Class     
14.1.7 Framed-Compression     
14.1.8 Framed-IP-Address     
14.1.9 Framed-IP-Netmask     
14.1.10 Framed-MTU     
14.1.11 Framed-Protocol     
14.1.12 Framed-Route     
14.1.13 Framed-Routing     
14.1.14 Idle-Timeout     
14.1.15 NAS-IP-Address     
14.1.16 NAS-Identifier     
14.1.17 NAS-Port-Id     
14.1.18 NAS-Port-Type     
14.1.19 Reply-Message     
14.1.20 Service-Type     
14.1.21 Session-Timeout     
14.1.22 State     
14.1.23 Termination-Action     
14.1.24 User-Name     
14.1.25 User-Password     
14.1.26 Vendor-Specific     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.1 CHAP-Password 
  ATTRIBUTE CHAP-Password 3 string

 
 Users:  L- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  No 


This attribute indicates the response value provided by a PPP Challenge-Handshake Authentication Protocol (CHAP) user in response to the challenge. It is only used in Access-Request packets. 

The CHAP challenge value is found in the CHAP-Challenge attribute (60) if present in the packet, otherwise in the request authenticator field. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.2 Callback-Id 
  ATTRIBUTE Callback-Id 20 string

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  No 


This attribute indicates the name of a place to be called, to be interpreted by the NAS. It may be used in Access-Accept packets. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.3 Callback-Number 
  ATTRIBUTE Callback-Number 19 string

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  No 


This attribute indicates a dialing string to be used for callback. It may be used in Access-Accept packets. It may be used in an Access-Request packet as a hint to the server that a Callback service is desired, but the server is not required to honor the hint. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.4 Called-Station-Id 
  ATTRIBUTE Called-Station-Id 30 string

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


This attribute allows the NAS to send in the Access-Request packet the phone number that the user called, using Dialed Number Identification (DNIS) or similar technology. Note that this may be different from the phone number the call comes in on. It is only used in Access-Request packets. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.5 Calling-Station-Id 
  ATTRIBUTE Calling-Station-Id 31 string

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


This attribute allows the NAS to send in the Access-Request packet the phone number that the call came from, using automatic number identification (ANI) or similar technology. It is only used in Access-Request packets. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.6 Class 
  ATTRIBUTE Class 25 string

 
 Users:  LR 
 Hints:  LR 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


This attribute is available to be sent by the server to the client in an Access-Accept and should be sent unmodified by the client to the accounting server as part of the Accounting-Request packet if accounting is supported. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.7 Framed-Compression 
  ATTRIBUTE Framed-Compression 13 integer

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Replace 
 Proxy propagated:  Yes 


  VALUE      Framed-Compression  None                 0       
VALUE      Framed-Compression  Van-Jacobson-TCP-IP  1       

 


This attribute indicates a compression protocol to be used for the link. It may be used in Access-Accept packets. It may be used in an Access-Request packet as a hint to the server that the NAS would prefer to use that compression, but the server is not required to honor the hint. 

More than one compression protocol attribute may be sent. It is the responsibility of the NAS to apply the proper compression protocol to appropriate link traffic. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.8 Framed-IP-Address 
  ATTRIBUTE Framed-IP-Address 8 ipaddr

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Replace 
 Proxy propagated:  No 


This attribute indicates the address to be configured for the user. It may be used in Access-Accept packets. It may be used in an Access-Request packet as a hint by the NAS to the server that it would prefer that address, but the server is not required to honor the hint. 

The value 0xFFFFFFFF (255.255.255.255) indicates that the NAS should allow the user to select an address. The value 0xFFFFFFFE (255.255.255.254) indicates that the NAS should select an address for the user (e.g. assigned from a pool of addresses kept by the NAS). Other valid values indicate that the NAS should use that value as the user's IP. 

When used in a RHS, the value of this attribute can optionally be followed by a plus sign. This usage means that the value of NAS-Port-Id must be added to this IP before replying. For example, 

          Framed-IP-Address = 10.10.0.1+

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.9 Framed-IP-Netmask 
  ATTRIBUTE Framed-IP-Netmask 9 ipaddr

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Replace 
 Proxy propagated:  No 


This attribute indicates the IP netmask to be configured for the user when the user is a router to a network. It may be used in Access-Accept packets. It may be used in an Access-Request packet as a hint by the NAS to the server that it would prefer that netmask, but the server is not required to honor the hint. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.10 Framed-MTU 
  ATTRIBUTE Framed-MTU 12 integer

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Replace 
 Proxy propagated:  Yes 


This attribute indicates the maximum transmission unit to be configured for the user, when it is not negotiated by some other means (such as PPP). It is only used in Access-Accept packets. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.11 Framed-Protocol 
  ATTRIBUTE Framed-Protocol 7 integer

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Replace 
 Proxy propagated:  Yes 


  VALUE      Framed-Protocol   PPP                  1       
VALUE      Framed-Protocol   SLIP                 2       

 


This attribute indicates the framing to be used for framed access. It may be used in both Access-Request and Access-Accept packets. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.12 Framed-Route 
  ATTRIBUTE Framed-Route 22 string

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  No 


This attribute provides routing information to be configured for the user on the NAS. It is used in the Access-Accept packet and can appear multiple times. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.13 Framed-Routing 
  ATTRIBUTE Framed-Routing 10 integer

 
 Users:  -R 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Replace 
 Proxy propagated:  No 


  VALUE      Framed-Routing    None                 0       
VALUE      Framed-Routing    Broadcast            1       
VALUE      Framed-Routing    Listen               2       
VALUE      Framed-Routing    Broadcast-Listen     3       

 


This attribute indicates the routing method for the user when the user is a router to a network. It is only used in Access-Accept packets. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.14 Idle-Timeout 
  ATTRIBUTE Idle-Timeout 28 integer

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  Yes 


This attribute sets the maximum number of consecutive seconds of idle connection allowed to the user before termination of the session or prompt. The server may send this attribute to the client in an Access-Accept or Access-Challenge. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.15 NAS-IP-Address 
  ATTRIBUTE NAS-IP-Address 4 ipaddr

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


This attribute indicates the identifying IP of the NAS which is requesting authentication of the user. It is only used in Access-Request packets. Each Access-Request packet should contain either a NAS-IP-Address or a NAS-Identifier attribute (14.1.16 NAS-Identifier). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.16 NAS-Identifier 
  ATTRIBUTE NAS-Identifier 32 string

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


This attribute contains a string identifying the NAS originating the access request. It is only used in Access-Request packets. Either NAS-IP-Address or NAS-Identifier should be present in an Access-Request packet. 

See section 14.1.15 NAS-IP-Address. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.17 NAS-Port-Id 
  ATTRIBUTE NAS-Port-Id 5 integer

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


This attribute indicates the physical port number of the NAS that is authenticating the user. It is only used in Access-Request packets. Note that here we are using "port" in its sense of a physical connection on the NAS, not in the sense of a TCP or UDP port number. 

Some NASes try to encode various information in the NAS-Port-Id attribute value. For example, the MAX Ascend terminal server constructs NAS-Port-Id by concatenating the line type (one digit), the line number (two digits), and the channel number (two digits), thus producing a five-digit port number. In order to normalize such encoded port numbers we recommend using a rewrite function (see section 5.12 Rewrite functions -- `raddb/rewrite'). A rewrite function for MAX Ascend servers is provided in the distribution. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.18 NAS-Port-Type 
  ATTRIBUTE NAS-Port-Type 61 integer

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Append 
 Proxy propagated:  No 


  VALUE      NAS-Port-Type     Async                0       
VALUE      NAS-Port-Type     Sync                 1       
VALUE      NAS-Port-Type     ISDN                 2       
VALUE      NAS-Port-Type     ISDN-V120            3       
VALUE      NAS-Port-Type     ISDN-V110            4       

 


This attribute indicates the type of the physical port of the NAS that is authenticating the user. It can be used instead of or in addition to the NAS-Port-Id (14.1.17 NAS-Port-Id) attribute. It is only used in Access-Request packets. Either NAS-Port or NAS-Port-Type or both should be present in an Access-Request packet, if the NAS differentiates among its ports. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.19 Reply-Message 
  ATTRIBUTE Reply-Message 18 string

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Append 
 Proxy propagated:  Yes 


This attribute indicates text that may be displayed to the user. 

When used in an Access-Accept, it is the success message. 

When used in an Access-Reject, it is the failure message. It may indicate a dialog message to prompt the user before another Access-Request attempt. 

When used in an Access-Challenge, it may indicate a dialog message to prompt the user for a response. 

Multiple Reply-Message attributes may be included, and if any are displayed, they must be displayed in the same order as they appear in in the packet. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.20 Service-Type 
  ATTRIBUTE Service-Type 6 integer

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Replace 
 Proxy propagated:  Yes 


  VALUE      Service-Type      Login-User           1       
VALUE      Service-Type      Framed-User          2       
VALUE      Service-Type      Callback-Login-User  3       
VALUE      Service-Type      Callback-Framed-User 4       
VALUE      Service-Type      Outbound-User        5       
VALUE      Service-Type      Administrative-User  6       
VALUE      Service-Type      NAS-Prompt-User      7       
VALUE      Service-Type      Authenticate-Only    8       
VALUE      Service-Type      Call-Check           10      

 


This attribute indicates the type of service the user has requested, or the type of service to be provided. It may be used in both Access-Request and Access-Accept packets. 

When used in an Access-Request the service type represents a hint to the Radius server that the NAS has reason to believe the user would prefer the kind of service indicated. 

When used in an Access-Accept, the service type is an indication to the NAS that the user must be provided this type of service. 

The meaning of various service types is as follows: 


Login-User 
The user should be connected to a host. 

Framed-User 
A framed protocol, such as PPP or SLIP, should be started for the user. The Framed-IP-Address attribute (see section 14.1.8 Framed-IP-Address) will supply the IP to be used. 

Callback-Login-User 
The user should be disconnected and called back, then connected to a host. 

Callback-Framed-User 
The user should be disconnected and called back; then a framed protocol, such as PPP or SLIP, should be started for the user. 

Outbound-User 
The user should be granted access to outgoing devices. 

Administrative-User 
The user should be granted access to the administrative interface to the NAS, from which privileged commands can be executed. 

NAS-Prompt 
The user should be provided a command prompt on the NAS, from which nonprivileged commands can be executed. 

Authenticate-Only 
Only authentication is requested, and no authorization information needs to be returned in the Access-Accept. 

Call-Check 
Callback-NAS-Prompt 
The user should be disconnected and called back, then provided a command prompt on the NAS, from which nonprivileged commands can be executed. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.21 Session-Timeout 
  ATTRIBUTE Session-Timeout 27 integer

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  Yes 


This attribute sets the maximum number of seconds of service to be provided to the user before termination of the session or prompt. The server may send this attribute to the client in an Access-Accept or Access-Challenge. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.22 State 
  ATTRIBUTE State 24 string

 
 Users:  LR 
 Hints:  LR 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


This attribute is available to be sent by the server to the client in an Access-Challenge and must be sent unmodified from the client to the server in the new Access-Request reply to that challenge, if any. 

This attribute is available to be sent by the server to the client in an Access-Accept that also includes a Termination-Action attribute with the value RADIUS-Request. If the NAS performs the termination action by sending a new Access-Request upon termination of the current session, it must include the State attribute unchanged in that Access-Request. 

In either usage, no interpretation by the client should be made. A packet may have only one State attribute. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.23 Termination-Action 
  ATTRIBUTE Termination-Action 29 integer

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Replace 
 Proxy propagated:  No 


  VALUE      Termination-Action  Default              0       
VALUE      Termination-Action  RADIUS-Request       1       

 


This attribute indicates what action the NAS should take when the specified service is completed. It is only used in Access-Accept packets. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.24 User-Name 
  ATTRIBUTE User-Name 1 string

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Replace 
 Proxy propagated:  Yes 


This attribute indicates the name of the user to be authenticated or accounted. It is used in Access-Request and Accounting attributes. The length of the user name is usually limited by some arbitrary value. By default, Radius supports user names up to 32 characters long. This value can be modified by redefining the RUT_USERNAME macro in the `include/radutmp.h' file in the distribution directory and recompiling the program. 

Some NASes have peculiarities about sending long user names. For example, the Specialix Jetstream 8500 24-port access server inserts a `/' character after the 10th character if the user name is longer than 10 characters. In such cases, we recommend applying rewrite functions in order to bring the user name to its normal form (see section 5.12 Rewrite functions -- `raddb/rewrite'). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.25 User-Password 
  ATTRIBUTE User-Password 2 string

 
 Users:  L- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  No 


This attribute indicates the password of the user to be authenticated, or the user's input following an Access-Challenge. It is only used in Access-Request packets. 

On transmission, the password is hidden. The password is first padded at the end with nulls to a multiple of 16 octets. A one-way MD5 hash is calculated over a stream of octets consisting of the shared secret followed by the request authenticator. This value is XORed with the first 16 octet segment of the password and placed in the first 16 octets of the String field of the User-Password attribute. 

If the password is longer than 16 characters, a second one-way MD5 hash is calculated over a stream of octets consisting of the shared secret followed by the result of the first xor. That hash is XORed with the second 16 octet segment of the password and placed in the second 16 octets of the string field of the User-Password attribute. 

If necessary, this operation is repeated, with each XOR result being used along with the shared secret to generate the next hash to XOR the next segment of the password, up to no more than 128 characters. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.1.26 Vendor-Specific 
(This message will disappear, once this node revised.) 
  ATTRIBUTE Vendor-Specific 26 string

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Append 
 Proxy propagated:  No 


This attribute is available to allow vendors to support their own extended attributes not suitable for general usage. <FIXME> some more detail over the VSAs? How does GNU Radius handle unknown VSAs? </> 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2 Accounting Attributes 
These are attributes the NAS sends along with accounting requests. These attributes can not be used in matching rules. 


14.2.1 Acct-Authentic     
14.2.2 Acct-Delay-Time     
14.2.3 Acct-Input-Octets     
14.2.4 Acct-Input-Packets     
14.2.5 Acct-Output-Octets     
14.2.6 Acct-Output-Packets     
14.2.7 Acct-Session-Id     
14.2.8 Acct-Session-Time     
14.2.9 Acct-Status-Type     
14.2.10 Acct-Terminate-Cause     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2.1 Acct-Authentic 
  ATTRIBUTE Acct-Authentic 45 integer

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  N/A 


  VALUE           Acct-Authentic          RADIUS          1
VALUE           Acct-Authentic          Local           2
VALUE           Acct-Authentic          Remote          3

 


This attribute may be included in an Accounting-Request to indicate how the user was authenticated, whether by Radius, the NAS itself, or another remote authentication protocol. Users who are delivered service without being authenticated should not generate accounting records. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2.2 Acct-Delay-Time 
  ATTRIBUTE Acct-Delay-Time 41 integer

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  N/A 


This attribute indicates how many seconds the client has been trying to send this record for, and can be subtracted from the time of arrival on the server to find the approximate time of the event generating this Accounting-Request. (Network transit time is ignored.) 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2.3 Acct-Input-Octets 
  ATTRIBUTE Acct-Input-Octets 42 integer

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  N/A 


This attribute indicates how many octets have been received from the port over the course of this service being provided, and can only be present in Accounting-Request records where Acct-Status-Type is set to Stop. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2.4 Acct-Input-Packets 
  ATTRIBUTE Acct-Input-Packets 47 integer

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  N/A 


This attribute indicates how many packets have been received from the port over the course of this service being provided to a framed user, and can only be present in Accounting-Request records where Acct-Status-Type is set to Stop. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2.5 Acct-Output-Octets 
  ATTRIBUTE Acct-Output-Octets 43 integer

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  N/A 


This attribute indicates how many octets have been sent to the port in the course of delivering this service, and can only be present in Accounting-Request records where Acct-Status-Type is set to Stop. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2.6 Acct-Output-Packets 
  ATTRIBUTE Acct-Output-Packets 48 integer

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  N/A 


This attribute indicates how many packets have been sent to the port in the course of delivering this service to a framed user, and can only be present in Accounting-Request records where Acct-Status-Type is set to Stop. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2.7 Acct-Session-Id 
  ATTRIBUTE Acct-Session-Id 44 string

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  N/A 


This attribute is a unique accounting ID to make it easy to match start and stop records in a log file. The start and stop records for a given session must have the same Acct-Session-Id. An Accounting-Request packet must have an Acct-Session-Id. An Access-Request packet may have an Acct-Session-Id; if it does, then the NAS must use the same Acct-Session-Id in the Accounting-Request packets for that session. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2.8 Acct-Session-Time 
  ATTRIBUTE Acct-Session-Time 46 integer

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  N/A 


This attribute indicates how many seconds the user has received service for, and can only be present in Accounting-Request records where Acct-Status-Type is set to Stop. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2.9 Acct-Status-Type 
  ATTRIBUTE Acct-Status-Type 40 integer

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  N/A 


  VALUE    Acct-Status-Type    Start              1
VALUE    Acct-Status-Type    Stop               2 
VALUE    Acct-Status-Type    Alive              3
VALUE    Acct-Status-Type    Accounting-On      7
VALUE    Acct-Status-Type    Accounting-Off     8

 


This attribute indicates whether this Accounting-Request marks the beginning of the user service (Start) or the end (Stop). 

It may also be used to mark the start of accounting (for example, upon booting) by specifying Accounting-On and to mark the end of accounting (for example, just before a scheduled reboot) by specifying Accounting-Off. 

A special value Alive or Interim-Update indicates the packet that contains some additional data to the initial Start record or to the last Alive record. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.2.10 Acct-Terminate-Cause 
  ATTRIBUTE Acct-Terminate-Cause 49 integer

 
 Users:  -- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  N/A 
 Proxy propagated:  N/A 


  VALUE    Acct-Terminate-Cause    User-Request            1
VALUE    Acct-Terminate-Cause    Lost-Carrier            2
VALUE    Acct-Terminate-Cause    Lost-Service            3
VALUE    Acct-Terminate-Cause    Idle-Timeout            4
VALUE    Acct-Terminate-Cause    Session-Timeout         5
VALUE    Acct-Terminate-Cause    Admin-Reset             6
VALUE    Acct-Terminate-Cause    Admin-Reboot            7
VALUE    Acct-Terminate-Cause    Port-Error              8
VALUE    Acct-Terminate-Cause    NAS-Error               9
VALUE    Acct-Terminate-Cause    NAS-Request             10
VALUE    Acct-Terminate-Cause    NAS-Reboot              11
VALUE    Acct-Terminate-Cause    Port-Unneeded           12
VALUE    Acct-Terminate-Cause    Port-Preempted          13
VALUE    Acct-Terminate-Cause    Port-Suspended          14
VALUE    Acct-Terminate-Cause    Service-Unavailable     15
VALUE    Acct-Terminate-Cause    Callback                16
VALUE    Acct-Terminate-Cause    User-Error              17
VALUE    Acct-Terminate-Cause    Host-Request            18

 


This attribute indicates how the session was terminated, and can only be present in Accounting-Request records where Acct-Status-Type is set to Stop. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3 Radius Internal Attributes 
These are attributes used by GNU Radius during the processing of a request. They are never returned to the NAS. Mostly, they are used in matching rules. 


14.3.1 Acct-Ext-Program     
14.3.2 Acct-Type     
14.3.4 Auth-Data     
14.3.3 Auth-Failure-Trigger     
14.3.5 Auth-Type     
14.3.6 Crypt-Password     
14.3.7 Exec-Program-Wait     
14.3.8 Exec-Program     
14.3.9 Fall-Through     
14.3.10 Group     
14.3.11 Hint     
14.3.12 Huntgroup-Name     
14.3.13 Log-Mode-Mask     
14.3.14 Login-Time     
14.3.15 Match-Profile     
14.3.16 Menu     
14.3.17 Pam-Auth     
14.3.18 Prefix     
14.3.19 Proxy-Replied     
14.3.20 Realm-Name     
14.3.21 Replace-User-Name     
14.3.22 Rewrite-Function     
14.3.23 Scheme-Acct-Procedure     
14.3.24 Scheme-Procedure     
14.3.25 Simultaneous-Use     
14.3.26 Strip-User-Name     
14.3.27 Suffix     
14.3.28 Termination-Menu     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.1 Acct-Ext-Program 
  ATTRIBUTE Acct-Ext-Program 2008 string

 
 Users:  -- 
 Hints:  -R 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  N/A 


The Acct-Ext-Program attribute can be used in RHS of an `raddb/hints' to require the execution of an external accounting program or filter. If the attribute value starts with a vertical bar (`|'), then the attribute specifies the filter program to be used. If it starts with a slash (`/'), then it is understood as the full pathname and arguments for the external program to be executed. Using any other character as the start of this string results in error. 

The command line can reference any attributes from both check and reply pairlists using attribute macros (see section 5.14 Macro Substitution). 

Before the execution of the program, radiusd switches to the uid and gid of the user daemon and the group daemon. You can override these defaults by setting variables exec-program-user and exec-program-group in configuration file to proper values (see section The option statement). 

The accounting program must exit with status 0 to indicate a successful accounting. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.2 Acct-Type 
  ATTRIBUTE Acct-Type 2003 integer

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Append 
 Proxy propagated:  N/A 


  VALUE           Acct-Type               None    0
VALUE           Acct-Type               System  1
VALUE           Acct-Type               Detail  2
VALUE           Acct-Type               SQL     3

 


The Acct-Type allows one to control which accounting methods must be used for a given user or group of users. In the absence of this attribute, all currently enabled accounting types are used. See section 8. Accounting, for more information about accounting types. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.3 Auth-Failure-Trigger 
This attribute specifies an external program or a Scheme expression to be run upon an authentication failure. The handling of this attribute depends upon its value: 

If the value of Auth-Failure-Trigger begins with `/', it is taken to contain a command line for invoking an external program. In this case radiusd invokes the program much the same way it does when handling Exec-Program attribute, i.e. the program is invoked with standard input closed, its standard output and standard error are captured and redirected to `radlog/radius.stderr' file, the return value of the program is ignored. 

If the value of Auth-Failure-Trigger begins with `(', it is executed it as a Scheme expression. The return value of the expression is ignored. 

This attribute is designed as a means to provide special handling for authentication failures. It can be used, for example, to increase failure counters and to block accounts after a specified number of authentication failures occurs. See section 7.10 Controlling Authentication Probes, for the detailed discussion of its usage. 

<FIXME> There is no corresponding Auth-Success-Trigger... Exec-Program or Scheme-Procedure may be used for the purpose, the latter, however, is not able to execute s-exps. At the time of this writing the release 1.3 is being prepared, so I do not want to introduce any possibly destabilizing changes. This will be fixed in future releases. </> 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.4 Auth-Data 
  ATTRIBUTE Auth-Data 2006 string

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Replace 
 Proxy propagated:  N/A 


The Auth-Data can be used to pass additional data to the authentication methods that need them. In version 1.3 of GNU Radius, this attribute may be used in conjunction with the SQL and Pam authentication types. When used with the Pam authentication type, this attribute holds the name of the PAM service to use. This attribute is temporarily appended to the authentication request, so its value can be referenced to as %C{Auth-Data}. See section 5.11.2 Authentication Server Parameters, for an example of of using the Auth-Data attribute in `raddb/sqlserver': 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.5 Auth-Type 
  ATTRIBUTE Auth-Type 1000 integer

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Append 
 Proxy propagated:  No 


  VALUE      Auth-Type         Local                0       
VALUE      Auth-Type         System               1       
VALUE      Auth-Type         Crypt-Local          3       
VALUE      Auth-Type         Reject               4       
VALUE      Auth-Type         SQL                  252     
VALUE      Auth-Type         Pam                  253     
VALUE      Auth-Type         Accept               254     

 


This attribute tells the server which type of authentication to apply to a particular user. It can be used in the LHS of the user's profile (see section 7. Authentication.) 

Radius interprets values of Auth-Type attribute as follows: 


Local 
The value of the User-Password attribute from the record is taken as a cleantext password and is compared against the User-Password value from the input packet. 

System 
This means that a user's password is stored in a system password type. Radius queries the operating system to determine if the user name and password supplied in the incoming packet are O.K. 

Crypt-Local 
The value of the User-Password attribute from the record is taken as an MD5 hash on the user's password. Radius generates MD5 hash on the supplied User-Password value and compares the two strings. 

Reject 
Authentication fails. 

Accept 
Authentication succeeds. 

SQL 
Mysql 
The MD5-encrypted user's password is queried from the SQL database (7.6 SQL Authentication Type). Mysql is an alias maintained for compatibility with other versions of Radius. 

Pam 
The user-name--password combination is checked using PAM. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.6 Crypt-Password 
  ATTRIBUTE Crypt-Password 1006 string

 
 Users:  L- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Append 
 Proxy propagated:  No 


This attribute is intended to be used in user's profile LHS. It specifies the MD5 hash of the user's password. When this attribute is present, Auth-Type = Crypt-Local is assumed. If both Auth-Type and Crypt-Password are present, the value of Auth-Type is ignored. 

See section 14.3.5 Auth-Type. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.7 Exec-Program-Wait 
  ATTRIBUTE Exec-Program-Wait 1039 string

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  No 


When present in the RHS, the Exec-Program-Wait attribute specifies the program to be executed when the entry matches. If the attribute value string starts with vertical bar (`|'), then the attribute specifies the filter program to be used. If it starts with slash (`/'), then it is understood as the full pathname and arguments for the external program to be executed. Using any other character as the start of this string results in error. 


14.3.7.1 Running an External Program     
14.3.7.2 Using an External Filter     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.7.1 Running an External Program 
The command line can reference any attributes from both check and reply pairlists using attribute macros see section 5.14 Macro Substitution. 

Before the execution of the program, radiusd switches to uid and gid of the user daemon and the group daemon. You can override these defaults by setting the variable exec-program-user in the configuration file to a proper value. See section The option statement. 

The daemon will wait until the program terminates. The return value of its execution determines whether the entry matches. If the program exits with a nonzero code, then the match fails. If it exits with a zero code, the match succeeds. In this case the standard output of the program is read and parsed as if it were a pairlist. The attributes thus obtained are added to the entry's reply attributes. 


Example. 
Suppose the `users' file contains the following entry: 

  DEFAULT Auth-Type = System,
                Simultaneous-Use = 1
        Exec-Program-Wait = "/usr/local/sbin/telauth \
                             %C{User-Name} \
                             %C{Calling-Station-Id}"

 


Then, upon successful matching, the program `/usr/local/sbin/telauth' will be executed. It will get as its arguments the values of the User-Name and Calling-Station-Id attributes from the request pairs. 

The `/usr/local/sbin/telauth' can, for example, contain the following: 

  #! /bin/sh

DB=/var/db/userlist

if grep "$1:$2" $DB; then
    echo "Service-Type = Login,"
    echo "Session-Timeout = 1200"
    exit 0
else
    echo "Reply-Message = \
          \"You are not authorized to log in\""
    exit 1
fi

 


It is assumed that `/var/db/userlist' contains a list of username:caller-id pairs for those users that are authorized to use login service. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.7.2 Using an External Filter 
If the value of Exec-Program-Wait attribute begins with `|', radiusd strips this character from the value and uses the resulting string as a name of the predefined external filter. Such filter must be declared in `raddb/config' (see section 5.1.10 filters statement). 


Example. 
Let the `users' file contain the following entry: 
  DEFAULT Auth-Type = System,
                Simultaneous-Use = 1
        Exec-Program-Wait = "|myfilter"

 


and let the `raddb/config' contain the following (6): 

  filters {
    filter myfilter {
        exec-path "/usr/libexec/myfilter";
        error-log "myfilter.log";
        auth {
            input-format "%C{User-Name}
                          %C{Calling-Station-Id}";
            wait-reply yes;
        };
    };        
};                        

 
Then, upon successful authentication, the program /usr/libexec/myfilter will be invoked, if it hasn't already been started for this thread. Any output it sends to its standard error will be redirected to the file `myfilter.log' in the current logging directory. A string consisting of the user's login name and his calling station ID followed by a newline will be sent to the program. 

The following is a sample /usr/libexec/myfilter written in the shell: 

  #! /bin/sh

DB=/var/db/userlist

while read NAME CLID
do
    if grep "$1:$2" $DB; then
        echo "0 Service-Type = Login, Session-Timeout = 1200"
    else
        echo "1 Reply-Message = \
              \"You are not authorized to log in\""
    fi
done

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.8 Exec-Program 
  ATTRIBUTE Exec-Program 1038 string

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  No 


When present in the RHS, the Exec-Program attribute specifies the full pathname and arguments for the program to be executed when the entry matches. 

The command line can reference any attributes from both check and reply pairlists, using attribute macros (see section 5.14 Macro Substitution). 

Before the execution of the program, radiusd switches to the uid and gid of the user daemon and the group daemon. You can override these defaults by setting variables exec-program-user and exec-program-group in configuration file to proper values The option statement. 

The daemon does not wait for the process to terminate. 


Example 
Suppose the `users' file contains the following entry: 

  DEFAULT Auth-Type = System,
                Simultaneous-Use = 1
        Exec-Program = "/usr/local/sbin/logauth \
                        %C{User-Name} \
                        %C{Calling-Station-Id}"

 


Then, upon successful matching, the program `/usr/local/sbin/logauth' will be executed. It will get as its arguments the values of the User-Name and Calling-Station-Id attributes from the request pairs. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.9 Fall-Through 
  ATTRIBUTE Fall-Through 1036 integer

 
 Users:  LR 
 Hints:  LR 
 Huntgroups:  -- 
 Additivity:  Append 
 Proxy propagated:  No 


  VALUE      Fall-Through      No                   0       
VALUE      Fall-Through      Yes                  1       

 


The Fall-Through attribute should be used in the reply list. If its value is set to Yes in a particular record, that tells Radius to continue looking up other records even when the record at hand matches the request. It can be used to provide default values for several profiles. 

Consider the following example. Let's suppose the `users' file contains the following: 

  johns   Auth-Type = SQL
                Framed-IP-Address = 11.10.10.251,
                Fall-Through = Yes

smith   Auth-Type = SQL
                Framed-IP-Address = 11.10.10.252,
                Fall-Through = Yes

DEFAULT NAS-IP-Address = 11.10.10.1
        Service-Type = Framed-User,
                Framed-Protocol = PPP


 


Then after successful matching of a particular user's record, the matching will continue until it finds the DEFAULT entry, which will add its RHS to the reply pairs for this request. The effect is that, if user `johns' authenticates successfully she gets the following reply pairs: 

          Service-Type = Framed-User,
        Framed-Protocol = PPP,  
        Framed-IP-Address = 11.10.10.251

 


whereas user smith gets 

          Service-Type = Framed-User,
        Framed-Protocol = PPP,  
        Framed-IP-Address = 11.10.10.252

 


Note that the attribute Fall-Through itself is never returned to the NAS. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.10 Group 
  ATTRIBUTE Group 1005 string

 
 Users:  L- 
 Hints:  L- 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.11 Hint 
  ATTRIBUTE Hint 1040 string

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Append 
 Proxy propagated:  No 


Use the Hint attribute to specify additional matching criteria depending on the hint (see section 5.6 Request Processing Hints -- `raddb/hints'). 

Let the `hints' file contain 

  DEFAULT         Prefix = "S", Strip-User-Name = No
                Hint = "SLIP"

 


and the `users' file contain 

  DEFAULT Hint = "SLIP",
                NAS-IP-Address = 11.10.10.12,
                Auth-Type = System
        Service-Type = Framed-User,
                Framed-Protocol = SLIP

 



Then any user having a valid system account and coming from NAS `11.10.10.12' will be provided SLIP service if his user name starts with `S'. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.12 Huntgroup-Name 
  ATTRIBUTE Huntgroup-Name 221 string

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


The Huntgroup-Name can be used either in the LHS of the `users' file record or in the RHS of the `huntgroups' file record. 

When encountered in a LHS of a particular `users' profile, this attribute indicates the huntgroup name to be matched. Radius looks up the corresponding record in the `huntgroups' file. If such a record is found, each A/V pair from its reply list is compared against the corresponding pair from the request being processed. The request matches only if it contains all the attributes from the specified huntgroup, and their values satisfy the conditions listed in the huntgroup pairs. 

For example, suppose that the authentication request contains the following attributes: 

  User-Name = "john",
User-Password = "guess",
NAS-IP-Address = 10.11.11.1,
NAS-Port-Id = 24

 


Let us further suppose that the `users' file contains the following entry: 

  john    Huntgroup-Name = "users_group",
                Auth-Type = System
        Service-Type = Login

 


and, finally, `huntgroups' contains the following entry: 

  users_group     NAS-IP-Address = 10.11.11.1
                NAS-Port-Id < 32

 


Then the authentication request will succeed, since it contains NAS-Port-Id attribute and its value is less than 32. 

See section 5.7 Huntgroups -- `raddb/huntgroups'. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.13 Log-Mode-Mask 
  ATTRIBUTE Log-Mode-Mask 2007 integer

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Append 
 Proxy propagated:  N/A 


  VALUE           Log-Mode-Mask           Log-Auth                1
VALUE           Log-Mode-Mask           Log-Auth-Pass           2
VALUE           Log-Mode-Mask           Log-Failed-Pass         4
VALUE           Log-Mode-Mask           Log-Pass                6
VALUE           Log-Mode-Mask           Log-All                 7

 


Log-Mode-Mask is used to control the verbosity of authentication log messages for given user or class of users. The meaning of its values is: 


Log-Auth 
Do not log successful authentications. 
Log-Auth-Pass 
Do not show the password with the log message from a successful authentication. 
Log-Failed-Pass 
Do not show a failed password. 
Log-Pass 
Do not show a plaintext password, either failed or succeeded. 
Log-All 
Do not log authentications at all. 
Technical details: After authentication, the server collects all Log-Mode-Mask attributes from the incoming request and LHS of the user's entry. The values of these attributes ORed together form a mask, which is applied via an XOR operation to the current log mode. The value thus obtained is used as effective log mode. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.14 Login-Time 
  ATTRIBUTE Login-Time 1042 string

 
 Users:  L- 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Append 
 Proxy propagated:  No 


The Login-Time attribute specifies the time range over which the user is allowed to log in. The attribute should be specified in the LHS. 

The format of the Login-Time string is the same as that of UUCP time ranges. The following description of the time range format is adopted from the documentation for the Taylor UUCP package: 

A time string may be a list of simple time strings separated with vertical bars `|' or commas `,'. 

Each simple time string must begin either with a day-of-week abbreviation (one of `Su', `Mo', `Tu', `We', `Th', `Fr', `Sa'), or `Wk' for any day from Monday to Friday inclusive, or `Any' or `Al' for any day. 

Following the day may be a range of hours separated with a hyphen, using 24-hour time. The range of hours may cross 0; for example `2300-0700' means any time except 7 AM to 11 PM. If no time is given, calls may be made at any time on the specified day(s). 

The time string may also be the single word `Never', which does not match any time. 

Here are a few sample time strings with an explanation of what they mean. 


`Wk2305-0855,Sa,Su2305-1655' 
This means weekdays before 8:55 AM or after 11:05 PM, any time Saturday, or Sunday before 4:55 PM or after 11:05 PM. These are approximately the times during which night rates apply to phone calls in the U.S.A. Note that this time string uses, for example, `2305' rather than `2300'; this will ensure a cheap rate even if the computer clock is running up to five minutes ahead of the real time. 


`Wk0905-2255,Su1705-2255' 
This means weekdays from 9:05 AM to 10:55 PM, or Sunday from 5:05 PM to 10:55 PM. This is approximately the opposite of the previous example. 


`Any' 
This means any day. Since no time is specified, it means any time on any day. 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.15 Match-Profile 
  ATTRIBUTE Match-Profile 2004 string

 
 Users:  LR 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Append 
 Proxy propagated:  No 


The Match-Profile attribute can be used in LHS and RHS lists of a user profile. Its value is the name of another user's profile (target profile). When Match-Profile is used in the LHS, the incoming packet will match this profile only if it matches the target profile. In this case the reply pairs will be formed by concatenating the RHS lists from both profiles. When used in the RHS, this attribute causes the reply pairs from the target profile to be appended to the reply from the current profile if the target profile matches the incoming request. 

For example: 

  IPPOOL  NAS-IP-Address = 10.10.10.1
                Framed-Protocol = PPP,
                Framed-IP-Address = "10.10.10.2"

IPPOOL  NAS-IP-Address = 10.10.11.1
                Framed-Protocol = PPP,
                Framed-IP-Address = "10.10.11.2"

guest   Auth-Type = SQL
                Service-Type = Framed-User,
        Match-Profile = IPPOOL

 


In this example, when user guest comes from NAS 10.10.10.1, he is assigned IP 10.10.10.2, otherwise if he is coming from NAS 10.10.11.1 he is assigned IP 10.10.11.2. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.16 Menu 
  ATTRIBUTE Menu 1001 string

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  No 


This attribute should be used in the RHS. If it is used, it should be the only reply item. 

The Menu attribute specifies the name of the menu to be presented to the user. The corresponding menu code is looked up in the `RADIUS_DIR/menus/' directory (see section 5.13 Login Menus -- `raddb/menus'). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.17 Pam-Auth 
  ATTRIBUTE Pam-Auth 1041 string

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Append 
 Proxy propagated:  No 


The Pam-Auth attribute can be used in conjunction with 

  Auth-Type = Pam

 


to supply the PAM service name instead of the default `radius'. It is ignored if Auth-Type attribute is not set to Pam. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.18 Prefix 
  ATTRIBUTE Prefix 1003 string

 
 Users:  L- 
 Hints:  L- 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


The Prefix attribute indicates the prefix that the user name should contain in order for a particular record in the profile to be matched. This attribute should be specified in the LHS of the `users' or `hints' file. 

For example, if the `users' file contained 

  DEFAULT Prefix = "U", Auth-Type = System
                Service-Type = Login-User

 


then the user names `Ugray' and `Uyoda' would match this record, whereas `gray' and `yoda' would not. 

Both Prefix and Suffix attributes may be specified in a profile. In this case the record is matched only if the user name contains both the prefix and the suffix specified. 

See section 14.3.27 Suffix, and 14.3.26 Strip-User-Name. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.19 Proxy-Replied 
  ATTRIBUTE Proxy-Replied 2012 integer

 
 Users:  L- 
 Hints:  L- 
 Huntgroups:  L- 
 Additivity:  Replace 
 Proxy propagated:  N/A 


  VALUE      Proxy-Replied     No                   0       
VALUE      Proxy-Replied     Yes                  1       

 


radiusd adds this attribute to the incoming request if it was already processed by a remote radius server. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.20 Realm-Name 
(This message will disappear, once this node revised.) 
  ATTRIBUTE Realm-Name 2013 string

 
 Users:  L- 
 Hints:  L- 
 Huntgroups:  L- 
 Additivity:  Append 
 Proxy propagated:  No 


<FIXME> This is an `internal attribute'. It keeps the realm name of the user. The Realm-Name attribute is added to the proxied request after receiving a reply from the realm server. See section 14.3.19 Proxy-Replied. </> 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.21 Replace-User-Name 
  ATTRIBUTE Replace-User-Name 2001 string

 
 Users:  LR 
 Hints:  LR 
 Huntgroups:  -- 
 Additivity:  Append 
 Proxy propagated:  No 


  VALUE      Replace-User-Name  No                   0       
VALUE      Replace-User-Name  Yes                  1       

 


Use this attribute to modify the user name from the incoming packet. The Replace-User-Name can reference any attributes from both LHS and RHS pairlists using attribute macros (5.14 Macro Substitution). 

For example, the `users' entry 

  guest   NAS-IP-Address = 11.10.10.11,
                Calling-Station-Id != ""
                Auth-Type = Accept
        Replace-User-Name = "guest#%C{Calling-Station-Id}",
                Service-Type = Framed-User,
                Framed-Protocol = PPP

 


allows the use of PPP service for user name guest, coming from NAS `11.10.10.11' with a nonempty Calling-Station-Id attribute. A string consisting of a `#' character followed by the Calling-Station-Id value is appended to the user name. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.22 Rewrite-Function 
  ATTRIBUTE Rewrite-Function 2004 string

 
 Users:  LR 
 Hints:  LR 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


The Rewrite-Function attribute specifies the name of the rewriting function to be applied to the request. The attribute may be specified in either pairlist in the entries of the `hints' or `huntgroups' configuration file. 

The corresponding function should be defined in `rewrite' as 

  integer name()

 


i.e., it should return an integer value and should not take any arguments. 

See section Packet rewriting rules, 5.6 Request Processing Hints -- `raddb/hints'; 5.7 Huntgroups -- `raddb/huntgroups'. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.23 Scheme-Acct-Procedure 
  ATTRIBUTE Scheme-Acct-Procedure 2010 string

 
 Users:  -- 
 Hints:  -R 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  N/A 


The Scheme-Acct-Procedure attribute is used to set the name of the Scheme accounting procedure. See section 11.3.3 Accounting with Scheme, for information about how to write Scheme accounting procedures. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.24 Scheme-Procedure 
  ATTRIBUTE Scheme-Procedure 2009 string

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Append 
 Proxy propagated:  N/A 


The Scheme-Procedure attribute is used to set the name of the Scheme authentication procedure. See section 11.3.2 Authentication with Scheme, for information about how to write Scheme authentication procedures. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.25 Simultaneous-Use 
  ATTRIBUTE Simultaneous-Use 1034 integer

 
 Users:  L- 
 Hints:  -R 
 Huntgroups:  -R 
 Additivity:  Append 
 Proxy propagated:  No 


This attribute specifies the maximum number of simultaneous logins a given user is permitted to have. When the user is logged in this number of times, any further attempts to log in are rejected. 

See section 7.9 Multiple Login Checking. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.26 Strip-User-Name 
  ATTRIBUTE Strip-User-Name 1035 integer

 
 Users:  LR 
 Hints:  LR 
 Huntgroups:  -R 
 Additivity:  Append 
 Proxy propagated:  No 


  VALUE      Strip-User-Name   No                   0       
VALUE      Strip-User-Name   Yes                  1       

 


The value of Strip-User-Name indicates whether Radius should strip any prefixes/suffixes specified in the user's profile from the user name. When it is set to Yes, the user names will be logged and accounted without any prefixes or suffixes. 

A user may have several user names for different kind of services. In this case differentiating the user names by their prefixes and stripping them off before accounting would help keep accounting records consistent. 

For example, let's suppose the `users' file contains 

  DEFAULT Suffix = ".ppp",
                Strip-User-Name = Yes,
                Auth-Type = SQL
        Service-Type = Framed-User,
                Framed-Protocol = PPP

DEFAULT Suffix = ".slip",
                Strip-User-Name = Yes,
                Auth-Type = SQL
        Service-Type = Framed-User,
                Framed-Protocol = SLIP

 


Now, user `johns', having a valid account in the SQL database, logs in as `johns.ppp'. She then is provided the PPP service, and her PPP session is accounted under user name `johns'. Later on, she logs in as `johns.slip'. In this case she is provided the SLIP service and again her session is accounted under her real user name `johns'. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.27 Suffix 
  ATTRIBUTE Suffix 1004 string

 
 Users:  L- 
 Hints:  L- 
 Huntgroups:  LR 
 Additivity:  Append 
 Proxy propagated:  No 


The Suffix attribute indicates the suffix that the user name should contain in order for a particular record in the profile to be matched. This attribute should be specified in LHS of the `users' or `hints' file. 

For example, if the `users' file contained 

  DEFAULT Suffix = ".ppp", Auth-Type = System,
                Strip-User-Name = Yes
        Service-Type = Framed-User,
                Framed-Protocol = PPP        

 


then the user names `gray.ppp' and `yoda.ppp' would match this record, whereas `gray' and `yoda' would not. 

Both Prefix and Suffix attributes may be specified in a profile. In this case the record is matched only if the user name contains both the prefix and the suffix specified. 

See section 14.3.18 Prefix, and 14.3.26 Strip-User-Name. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

14.3.28 Termination-Menu 
  ATTRIBUTE Termination-Menu 1002 string

 
 Users:  -R 
 Hints:  -- 
 Huntgroups:  -- 
 Additivity:  Replace 
 Proxy propagated:  No 


This attribute should be used in the RHS. If it is used, it should be the only reply item. 

The Termination-Menu specifies the name of the menu file to be presented to the user after finishing his session. The corresponding menu code is looked up in the `RADIUS_DIR/menus/' directory (see section 5.13 Login Menus -- `raddb/menus'). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

15. Reporting Bugs 
It is possible you will encounter a bug in one of the Radius programs. If this happens, we would like to hear about it. As the purpose of bug reporting is to improve software, please be sure to include maximum information when reporting a bug. The information needed is: 


Conditions under which the bug appears. 
Version of the package you are using. 
Compilation options used when configuring the package. 
If the bug is found in radiusd daemon, run `radiusd -v' and include the output it produces. 
Contents of Radius configuration directory (`/usr/local/etc/raddb' or whatever you have set it to while configuring). 
Log messages produced. 
Send your report to bug-gnu-radius@gnu.org. Allow us a couple of days to answer. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

16. Where to Get Information about GNU Radius 
The two places to look for news regarding GNU Radius are the Radius homepage at http://www.gnu.org/software/radius and the Radius project page at http://savannah.gnu.org/projects/radius. 

The following mailing lists are related to GNU Radius: 


info-gnu-radius@gnu.org 
This list distributes announcements and progress reports on GNU Radius. This is a moderated list. Please do not send bug reports or requests for help to this list; there exist special mailing lists for these purposes. To subscribe to the list, visit http://mail.gnu.org/mailman/listinfo/info-gnu-radius. 
help-gnu-radius@gnu.org 
This list is the place for users and installers of GNU Radius to ask for help. The list is not moderated, but postings are allowed for list members only. To subscribe to the list, visit http://mail.gnu.org/mailman/listinfo/help-gnu-radius. 
bug-gnu-radius@gnu.org 
This list distributes bug reports, bug fixes, and suggestions for improvements in Radius. User discussion of Radius bugs also occurs here. The list is not moderated; postings are allowed for anybody. To subscribe to the list, visit http://mail.gnu.org/mailman/listinfo/bug-gnu-radius. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

How to Obtain Radius 
GNU Radius is free software; this means that everyone is free to use it and free to redistribute it on certain conditions. GNU Radius is not in the public domain; it is copyrighted and there are restrictions on its distribution, but these restrictions are designed to permit everything that a good cooperating citizen would want to do. What is not allowed is to try to prevent others from further sharing any version of GNU Radius that they might get from you. The precise conditions are found in the GNU General Public License that comes with Radius and also appears following this section. 

One way to get a copy of GNU Radius is from someone else who has it. You need not ask for our permission to do so, or tell any one else; just copy it. If you have access to the Internet, you can get the latest distribution version of GNU Radius by anonymous FTP. It is available at ftp://ftp.gnu.org/pub/gnu/radius 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

Radius Glossary 
Throughout this document the following terms are used: 


RADIUS (small capitals) 
The Remote Authentication Dial In User Service protocol as described in RFC 2138, 2865, and 2866. 

NAS 
A network access server, that is, a computer or a special device designed to provide access to the network. For example, it can be a computer connected to the network and equipped with several modems. Such a NAS will allow a user connecting to one of its modems to access the network. 

Service 
A service, such as PPP, SLIP, or telnet, provided to a user by the NAS. 

Session 
Each instance of a service. Sessions start when the service is first provided and close when the service is ended. A user may be allowed to have multiple sessions active simultaneously. 

Session ID 
The session identifier: a string of characters uniquely identifying the session. 

A/V pair 
Attribute-value pair: see 3.1 Attributes. 

Dial-in or dial-up user 
A user connecting to a service through the modem line. 

User database 
A database where a RADIUS server keeps information about users, their authentication information, etc. 

User's profile 
A record in the user database describing a particular user for purposes of authentication and authorization, i.e., how the user should be authenticated as well as which services he is allowed to be provided and parameters of these services. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

Acknowledgements 
I would like to acknowledge Oswaldo Aguirre and Francisco Obispo, who invested a lot of time and effort to debug and test the program. They also wrote web-radact -- a web interface to the radius database. 

Alexandre Oliva provided a lot of good advice and offered valuable help in testing Radius on various platforms. 

The following people provided many helpful comments, bug reports and patches: Dustin Mitchell, Jody Owens, Andrey Y. Mosienko, Oleg Gawriloff, Adrian P. van Bloois, Michael Samuel, Michael Smirnov, Andrey Pavlenko, Michael Weiser, Eric Salome, Clement Gerouville, Dave Restall, Vlad Lungu, Robert Abbate, Jaime Tellez Sanchez, Cornel Cristea, Krzysztof Kopera, and David Friedman. 

Additional people need to be thanked for their assistance in producing this manual. Lisa M. Goldstein coordinated its preparation and Joseph C. Fineman and Daniel Barowy did a remarkable job of editing. 

And of course, thanks to Richard M. Stallman for founding the FSF and starting the GNU project. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17. New Configuration Approach (draft) 
(This message will disappear, once this node revised.) 
This document presents a draft describing new approach for processing RADIUS requests. It is intended as a request for comments, and, in the long run, as a guide for GNU Radius developers. In its current state it is far from being complete. Please check http://www.gnu.org/software/radius/manual for updated versions. Feel free to send your comments and suggestions to bug-gnu-radius@gnu.org. 


17.1 A brief description of Currently Used Approach     
17.2 Deficiencies of Current Operation Model and Configuration Suite     
17.3 Proposed Solution    A Proposed Solution 
17.4 Changes to Rewrite Language     
17.5 Support for Traditional Configuration Files.     
17.6 New Configuration Files     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17.1 A brief description of Currently Used Approach 
When I started to write GNU Radius, back in 1998, I had two major aims. The first and primary aim was to create a flexible and robust system that would follow the principle of Jon Postel: 


Be liberal in what you accept and conservative in what you send. 
This, I believe, is the main principle of any good software for Internet. 

The second aim was to be backward compatible with the implementations that already existed back then. This seemed to be important (and the time has proved it was), because it would allow users to easily switch from older radius daemon to GNU Radius. 

An important part of every complex program is its configuration file. Traditional implementations of RADIUS servers (beginning from Livingston Radius) used a configuration suite consisting of several files, usually located in `/etc/raddb' subdirectory. Its main components were: 


`dictionary' 
A file containing translations of symbolic names of radius attributes and attribute values to their integer numbers as specified by RADIUS protocol. 

`hints' 
This file was intended to separate incoming requests in groups, based on the form of their login name. Traditionally such separation was performed basing on common prefixes and/or suffixes of login names. 

`huntgroups' 
The purpose of this file was to separate incoming requests depending on their source, i.e. on the NAS that sent them and the port number on that NAS. It also served as a sort of simplified access control list. 

`users' 
This file contained a users database. It described criteria for authentication and reply pairs to be sent back to requesting NASes. 
Among these files, the first two were used for requests of any kind, whereas `users' was used only for Access-Request packets. 

Though this configuration system suffered from many inconsistencies, the second aim required GNU Radius to use this approach. 

To compensate for its deficiencies and to fulfill the first aim, this configuration system was extended, while preserving its main functionality. A number of additional internal attributes were added, that control radiusd behavior. A new language was created whose main purpose was to modify incoming requests (see section 11.2 Rewrite). The support for GNU's Ubiquitous Intelligent Language for Extensions (see section 11.3 Guile) was added, that allowed to further extend GNU Radius functionality. 

The present operation model(7) of GNU Radius and its configuration file system(8) emerged as a result of the two development aims described above. Since 1998 up to present, GNU Radius users contributed a lot of ideas and code to the further development of the system. 

However, it became obvious that this system presents strong obstacles to the further development. The next section addresses its deficiencies. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17.2 Deficiencies of Current Operation Model and Configuration Suite 
The main deficiencies are inherited with the traditional configuration file suite. The rules for processing each request are split among three files, each of which is processed differently, despite of their external similarity. The administrator has to keep in mind a set of exotic rules when configuring the system(9). When matching incoming requests with configuration file entries (LHS, see section 3.3 Matching Rule), some attributes are taken verbatim, whereas others are used to control radiusd behavior and to pass additional data to other rules (see section 14.3 Radius Internal Attributes). The things become even more complicated when RADIUS realms come into play (see section 3.4.2.1 Proxy Service). Some attributes are meaningful only if used in a certain part of a certain configuration file rule. 

So, while being a lot more flexible than the approach used by other RADIUS implementations, the current system is quite difficult to maintain. 

Another deficiency is little control over actions executed on different events. For example, it is often asked how can one block a user account after a predefined number of authentication failures? Currently this can only be done by writing an external authentication procedure (either in Scheme, using Guile, or as a standalone executable, using Exec-Program-Wait). The proper solution would be to have a set of user-defined triggers for every RADIUS event (in this case, for authentication failure). 

Another commonly asked question is how to make radiusd execute several SQL queries when processing a request. While GNU Radius is not supposed to compensate for deficiencies of some SQL implementations that do not allow for nested queries, such a feature could come quite handy. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17.3 Proposed Solution 
(This message will disappear, once this node revised.) 
Processing of incoming requests is controlled by request-processing program. Request-processing program is a list-like structure, consisting of instructions. 


17.3.1 Request-processing Instruction     
17.3.2 grad_instr_conditional     
17.3.3 grad_instr_call     
17.3.4 grad_instr_return     
17.3.5 grad_instr_action     
17.3.6 grad_instr_reply     
17.3.7 grad_instr_proxy     
17.3.8 grad_instr_forward     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17.3.1 Request-processing Instruction 
Request-processing program consists of instructions. There are seven basic instruction types: 


grad_instr_conditional_t 
This instruction marks a branch point within the program. 

grad_instr_call_t 
Represents a call of a subprogram 

grad_instr_action_t 
Invokes a Rewrite <FIXME> or Scheme? </> function 

grad_instr_proxy_t 
Proxies a request to the remote server 

grad_instr_forward_t 
Forwards a request to the remote server 

grad_instr_reply_t 
Replies back to the requesting NAS. 
Consequently, an instruction is defined as a union of the above node types: 


Instruction: grad_instr_t 
  enum grad_instr_type
{
  grad_instr_conditional,
  grad_instr_call,
  grad_instr_return,
  grad_instr_action,
  grad_instr_reply,
  grad_instr_proxy,
  grad_instr_forward
};

typedef struct grad_instr grad_instr_t;

struct grad_instr
{
  enum grad_instr_type type;
  grad_instr_t *next;
  union
    {
      grad_instr_conditional_t cond;
      grad_instr_call_t call;
      grad_instr_action_t action;
      grad_instr_reply_t reply;
      grad_instr_proxy_t proxy;
      grad_instr_forward_t forward;
    } v;                                                             
};

 


Type member contains type of the instruction. The evaluator uses type to determine which part of union v, holds instruction-specific data. 

Next points to the next instruction. The evaluator will go to this instruction unless the present one changes the control flow. 

Finally, v contains instruction-specific data. These will be discussed in the following subsections. 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17.3.2 grad_instr_conditional 
(This message will disappear, once this node revised.) 

Instruction: grad_instr_conditional_t cond iftrue iffalse 
  struct grad_instr_conditional
{
  grad_entry_point_t cond;  /* Entry point to the compiled
                               Rewrite condition */
  grad_instr_t *iftrue;     /* Points to the ``true'' branch  */
  grad_instr_t *iffalse;    /* Points to the ``false'' branch  */
};
typedef struct grad_instr_conditional grad_instr_conditional_t;

 


Instructions of type grad_instr_conditional_t indicate branching. Upon encountering an grad_instr_conditional_t, the engine executes a Rewrite expression pointed to by cond. If the expression evaluates to true, execution branches to instruction iftrue. Otherwise, if iffalse is not NULL, execution branches to that instruction. Otherwise, the control flow passes to grad_instr_t.next, as described in the previous section. 



RPL representation 

RPL defun: COND expr if-true [if-false] 


expr 
Textual representation of Rewrite conditional expression or its entry point. 
if-true 
RPL expression executed if expr evaluates to t. 
if-true 
Optional RPL expression that is executed if expr evaluates to nil. 

Example 
COND with two arguments: 

  (COND "%[User-Name] ~= \"test-.*\""
      (REPLY Access-Reject ("Reply-Message" . "Test accounts disabled")))

 


COND with three arguments: 

  (COND "%[Hint] == "PPP" && authorize(PAM)"
      (REPLY Access-Accept
             ("Service-Type" . "Framed-User")
             ("Framed-Protocol" . "PPP"))
      (REPLY Access-Reject
             ("Reply-Message" . "Access Denied")))

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17.3.3 grad_instr_call 
(This message will disappear, once this node revised.) 

Instruction: grad_instr_call_t entry 
  struct grad_instr_call {
       grad_instr_t *entry;    
};
typedef struct grad_instr_call grad_instr_call_t;

 


Instructions of type grad_instr_call instruct the engine to call the given subprogram. The engine pushes the current instruction <FIXME> definition of current instruction or pc? </> to the return point stack <FIXME> definition of this? </> and branches to instruction entry. Execution of the subprogram ends when the engine encounters an instruction of one of the following types: grad_instr_return, grad_instr_reply or grad_instr_proxy. 

If grad_instr_return is encountered, the engine pops the instruction from the top of the return point stack and makes it current instruction, then it branches to the next node. 

If grad_instr_reply or grad_instr_proxy is encountered, the engine, after executing corresponding actions, finishes executing the program. 


RPL representation 

RPL defun: CALL list 
RPL defun: CALL defun-name 
In the first form, the argument list is the RPL subprogram to be executed. 

In the second form defun-name is a name of the RPL subprogram defined by defun. 



Examples 
First form: 

  (CALL (ACTION "myfun(%[User-Name])")
      (REPLY Access-Reject
             ("Reply-Message" . "Access Denied")))

 


Second form: 

  (CALL process_users)

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17.3.4 grad_instr_return 
(This message will disappear, once this node revised.) 
An instruction of type grad_instr_return indicates a return point from the subprogram. If encountered in a subprogram (i.e. a program entered by grad_instr_call node), it indicates return to the calling subprogram (see the previous subsection). Otherwise, if grad_instr_return is encountered within the main trunk, it ends evaluating of the program. 

Instructions of this type have no data associated with them in union v. 


RPL representation 
RPL defun: RETURN 

Examples 
  (RETURN)

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17.3.5 grad_instr_action 
(This message will disappear, once this node revised.) 

Instruction: grad_instr_reply_t expr 
  struct grad_instr_action {
       grad_entry_point_t expr;    /* Entry point to the compiled
                                      Rewrite expression */
};
typedef struct grad_instr_action grad_instr_reply_t;

 


The machine executes a Rewrite expression with entry point expr. Any return value from the expression is ignored. <FIXME> Should the expression receive any arguments? If so, what arguments? I'd say it should take at least the request being processed and the reply pairs collected so far. </> 


RPL representation 
RPL defun: ACTION expr 
RPL defun: ACTION entry-point 
<FIXME> Description </> 



Examples 
  (ACTION "%[NAS-IP-Address] = request_source_ip()")

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17.3.6 grad_instr_reply 
(This message will disappear, once this node revised.) 

Instruction: grad_instr_reply_t return_code 
  struct grad_instr_reply {
       u_char reply_code;         /* Radius request code */
};
typedef struct grad_instr_reply grad_instr_reply_t;

 


grad_instr_reply instructs radiusd to send to the requesting NAS a reply with code reply_code. Any reply pairs collected while executing the program are attached to the reply. 

After executing grad_instr_reply instruction, the engine stops executing of the program. 

Any execution path will usually end with this instruction. 


RPL representation 

RPL defun: REPLY reply-code [attr-list] 

Arguments: 

reply-code 
Radius reply code. 
attr-list 
List of A/V pairs to be added to the reply. Each A/V pair is represented as a cons: (name-or-number . value). 

Example 
  (REPLY Access-Accept
       ("Service-Type" . "Framed-User")
       ("Framed-Protocol" . "PPP"))

 




--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

17.3.7 grad_instr_proxy 
(This message will disappear, once this node revised.) 

Instruction: grad_instr_proxy_t realm 
  struct grad_instr_proxy
{
  grad_realm_t realm;
};
typedef struct grad_instr_proxy grad_instr_proxy_t;

 


This instruction tells radius to proxy the request to the server defined in realm. In other words, the engine executes proxy_send. Further processing of the program is stopped. 


RPL representation 

RPL defun: PROXY realm-name 
Realm-name is name of the realm as defined in `raddb/realms'. <FIXME> No, no. That's senseless. I must get rid of `raddb/*'! </> 




Examples 
<FIXME> Fix the above definition, then provide an example </> . 

5.1 Run-Time Configuration Options -- `raddb/config'    Run-time configuration options. 
5.2 Dictionary of Attributes -- `raddb/dictionary'    Radius dictionary. 
5.3 Clients List -- `raddb/clients'    Clients lists the NASes that are allowed to communicate with radius. 
5.4 NAS List -- `raddb/naslist'    The naslist file keeps general information about the NASes. 
5.5 NAS Types -- `raddb/nastypes'    Information about how to query the NASes about active user sessions. 
5.6 Request Processing Hints -- `raddb/hints'    Important user information that is common for the users whose names match some pattern. 
5.7 Huntgroups -- `raddb/huntgroups'    Group users by the NAS (and, possibly, a port number) they come from. 
5.8 List of Proxy Realms -- `raddb/realms'    Communication with remote radius servers 
5.9 User Profiles -- `raddb/users'    User profile. 
5.10 List of Blocked Users -- `raddb/access.deny'    List of users which are denied access. 
5.11 SQL Configuration -- `raddb/sqlserver'    SQL server configuration. 
5.12 Rewrite functions -- `raddb/rewrite'    Rewrite functions allow to change the input packets. 
5.13 Login Menus -- `raddb/menus'    Menus allow user to select the type of service. 

Client configuration 

13.1 Client Configuration    Main client configuration file. 
                         



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

Introduction to Radius 
GNU Radius is a software package that provides authentication and accounting services. The acronym RADIUS stands for Remote Authentication Dial In User Service and (in that form) usually denotes the underlying protocol name. 

Historically, RADIUS servers were used as a means to authenticate the user coming from a dial-in connection, but GNU Radius is much more than an authentication system: it is an advanced, customizable, and extensible system for controlling access to the network. 

GNU Radius has several built-in authentication and accounting methods. When these methods are not enough, it allows the administrator to implement any new method she deems convenient. 

The GNU Radius package includes the server program, radiusd, which responds to authentication and accounting requests, and a set of accompanying programs designed to monitor the activity of the server and analyze the information it provides. 


1.0 Overview     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

1.0 Overview 
To illustrate what GNU Radius does, let's consider an imaginary internet service provider. Our provider has two network access servers (NASes for short)---i.e., two pieces of equipment which directly accept users' connections--and a core router that connects the ISP's internal network with the Internet backbone. 

When a user connects to a NAS, the server must verify that the user is actually registered and that the credentials she has supplied are correct. This first step is called authentication. 

Upon authenticating the user, the NAS must determine which services the user is permitted to use and to what extent the user may use them. This second step is called authorization. 

When the first two stages have been successfully completed, the NAS takes the third step and establishes the connection between the user and the main server. This connection is called a user session. For the purposes of accounting, the NAS remembers the exact time of the start of the session. When the session is terminated, the duration of the session and the number of bytes transferred are recorded as well. 

All three tasks can be accomplished by the use of user and accounting databases on each terminal server. However, this is not convenient, and it is error-prone in that the maintenance of separate databases for the same users is not a trivial task. What is worse, as the number of terminal servers grows, this maintenance problem becomes more difficult. 


How Does RADIUS Perform These Tasks? 
RADIUS allows an administrator to keep authentication and accounting data in a single place, no matter how many network access servers are actually present. Using RADIUS, NASes instead communicate with this central server to perform authentication and accounting, thus easing the burden on the system administrator. 

Let's return to our imaginary ISP. Suppose it runs a RADIUS daemon on its central server. Each NAS runs client software to communicate with the RADIUS server by sending radius packets. 

An average user session life cycle looks as follows. 

A user connects to the nearest NAS and supplies his login and password. The NAS forms an authentication request and sends it to the RADIUS server. 

The RADIUS server verifies the user's credentials and finds them sufficient. It then retrieves the user's authorization information from its database, packages it into an acknowledgement packet, and then sends it back to the NAS 

The NAS receives the acknowledgement packet and starts the user session. The information brought with the packet tells the NAS to establish a connection between the core router and the user, and to assign the user a certain IP address. Having established the session, the NAS informs the RADIUS server by sending it an accounting start packet. The server acknowledges the receipt of the accounting packet. 

Now suppose that after some time the user decides to break the connection. The NAS notices this and terminates the user's session. The NAS then sends an accounting stop packet to the RADIUS server to mark this event. Again, the server acknowledges the receipt of the packet. 


RADIUS Attributes 
Attributes are means of passing the information between the NAS and the server. Basically, an attribute is an integer number that identifies some piece of information. A set of properties are associated with each attribute, specifying the way to interpret the attribute. The most important property is the data type, which declares the type of data that the attribute identifies (character string, integer number, IP address, or raw binary data). 

The information to be transmitted with the request is packaged in a set of attribute-value pairs (or A/V pairs for short). Such pairs consist of attribute numbers and the associated data. 


RADIUS Packets 
There exist two basic kinds of RADIUS packets: authentication and accounting packets. Each of them is subdivided into requests and replies. 

Authentication requests are sent from the NAS to the RADIUS server and contain the information necessary to check the identity of the user. The minimum set of data in such packets consists of the user login name, user password, and NAS IP or identifier. 

Authentication replies are sent by the RADIUS server and contain the reply code and a set of additional attributes. According to their reply code the authentication replies are subdivided into authentication acknowledgements, authentication rejections, and authentication challenges. 

An authentication acknowledgement packet is sent to the NAS if the credentials supplied with the authentication request were correct. This kind of packet tells the NAS to establish a normal user session. The additional attributes in such packets carry the authorization data, i.e., they determine which kind of service the user is to be provided. 

An authentication rejection is sent to the NAS if the authentication has failed. This packet forbids the NAS to provide any service to the user. The additional attributes may carry descriptive text to be displayed as an explanation to the user for the failure of his request. 

Finally, an authentication challenge packet is sent to the NAS if the supplied credentials did not suffice to establish the authenticity of the user. This means that the dialog between the NAS and the RADIUS server continues. As the RADIUS server asks for additional authentication credentials, the NAS acts as a liaison, passing server requests to the user and sending user replies back to the server. Such a dialog ends when the RADIUS server sends either an acknowledgement packet or a rejection packet. 

An accounting request is sent to the server when the NAS wishes to report some event in the user session: the start of the session, session termination, etc. The attributes carry the actual information about the event. 

For each accounting request that has been received and successfully processed, the RADIUS server sends back an accounting acknowledgement. This packet carries no attributes, but simply informs the NAS that the information it had sent was received. 

Occasionally, a RADIUS server may fail to receive incoming requests or may fail to process them due to high server load. In order to prevent such requests from being lost, the NAS retransmits the request if no response from the server is received within a predefined interval of time (a timeout interval). Usually the NAS is configured in such a way that it continues retransmitting failed requests until either it receives a reply from the server or a predefined number of retries are exhausted, whichever occurs first. Furthermore, a NAS may be configured to communicate with a set of backup RADIUS servers. In this case it applies the described process to each server from the set, until one of them responds or the set is exhausted. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

2. Naming Conventions 
This chapter describes file naming conventions used throughout this document. 

Programs from the GNU Radius package use the following directories to store various configuration and log files: 


Configuration or database directory 
A directory where all configuration files are stored. 

Log directory 
A directory where radiusd stores its log files. 

Accounting directory 
A directory where radiusd stores accounting detail files (see section 8.2 Detailed Request Accounting). 

Data directory 
A directory where shared data files are stored, such as Rewrite or Scheme source files. 

The default locations of these directories are determined at compile time. Usually these are: 

Directory  Short name  Default location 
Configuration directory  `raddb'  /usr/local/etc/raddb  
Log directory  `radlog'  /var/log  
Accounting directory  `radacct'  /var/log/radacct  
Data directory  `datadir'  /usr/local/share/radius/1.3  


These locations may differ depending on your local site configuration. 

Throughout this document we will refer to these directories by their short names. For example, when we say: 

  ... this information is contained in file `raddb/sqlserver' 

 


we actually mean `/usr/local/etc/raddb/sqlserver'. 

To get the default directory names that your version of Radius was compiled with, run radiusd --version. 

Locations of these directories may be overridden by specifying the appropriate command line options. For example, any program from the GNU Radius package accepts the command line option `-d' or `--directory', which introduces the configuration directory path. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3. How Radius Operates 
The main purpose of GNU Radius is to centralize authentication of users coming from various network stations, pursuant to the RADIUS specification. Its primary usage is for dial-in users, though it can be used for any kind of network connection. 


3.1 Attributes     
3.2 RADIUS Requests    RADIUS requests. 
3.3 Matching Rule    Rules for request processing. 
3.4 Processing Requests    How GNU Radius processes incoming requests. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.1 Attributes 
Information carried by RADIUS requests is stored as a list of attribute-value pairs. Each pair consists of an attribute number and an attribute value. The attribute number identifies the type of information the pair carries, and the attribute value keeps the actual data. 

The value part of an attribute can contain data of one of the following types: 


Integer 
A 32-bit unsigned integer value. 
IP-number 
An IPv4 IP-number. 
String 
A character string up to 253 characters long. 
For convenience, the attributes and the values of some frequently used integer attributes are given symbolic names. These names are assigned to attributes and values in the dictionary file (see section 5.2 Dictionary of Attributes -- `raddb/dictionary'). 

Attribute numbers range from 1 to 255. Attributes with numbers greater than 255 are used internally by the server and cannot be sent to the NAS. 

The vendor-specific attribute number 26 is special, allowing vendors of the NAS hardware or software to support their own extended attributes. vendor-specific attribute. 

Each attribute has a set of properties associated with it. The properties are: 


Usage flags 
These flags determine the usage of the attribute in the configuration files `huntgroups', `hints', and `users'. 
Propagation 
When a RADIUS server functions in proxy mode, it uses the propagation flag to determine which attributes from the reply packet should be passed back to the requesting NAS (see section 3.4.2.1 Proxy Service). 
additivity 
Some configuration rules may cause the addition of new A/V pairs to the incoming request. Before the addition of a new pair, radiusd scans the request to see if it already contains a pair with the same attribute. If it does, the value of the additivity determines the following additional actions: 
None 
The old pair is retained in the request; the new pair is not added to it. 
Replace 
The old pair is retained in the request, but its value is replaced with that of the new pair. 
Append 
The new pair is appended to the end of the pair list. 
Attributes are declared in the `raddb/dictionary' file. For a detailed description, see 5.2.4 ATTRIBUTE statement. For information about particular attributes, see 14. Attribute List. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.2 RADIUS Requests 
The term request refers to both the authentication/accounting request packet from a NAS to a RADIUS server and the response packet that the server sends back to the NAS. 

Each request contains the following fields: 


`Code' 
The code field identifies the type of the request. 

`Identifier' 
The number in the range 0--255 used to match the request with the reply. 

`Length' 
The length of the request packet. 

`Authenticator' 
The 16-byte hash value used to authenticate the packet. 

`Attributes' 
The list of attribute-value pairs carrying actual information about the request. 



3.2.1 Authentication Requests     
3.2.2 Accounting Requests     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.2.1 Authentication Requests 
A NAS sends authentication requests (packets with code field set to Access-Request) to a RADIUS server when a user is trying to connect to that NAS. Such requests convey information used to determine whether a user is allowed access to the NAS, and whether any special services are requested for that user. 

An Access-Request must contain a User-Name attribute 14.1.24 User-Name. This packet should contain a NAS-IP-Address attribute, a NAS-Identifier attribute, or both. It also must contain either a User-Password attribute or a CHAP-Password attribute. These attributes are passed after being encoded using a method based on the RSA Message Digest Algorithm MD5. 

The Access-Request should contain a NAS-Port or NAS-Port-Type attribute or both, unless the type of access being requested does not involve a port or the NAS does not distinguish among its ports. 

Upon receiving an Access-Request packet for a particular user and authenticating that user, the RADIUS server replies to the NAS that has sent the packet with any one of the following packets: 


Access-Accept 
Access-Reject 
Access-Challenge 
GNU Radius replies with an Access-Accept packet when it has successfully authenticated the user. Such a reply packet provides the configuration information necessary to begin delivery of service to the user. 

GNU Radius replies with an Access-Reject packet when it is unable to authenticate the user. Such a packet may contain a descriptive text encapsulated in one or more Reply-Message attributes. The NAS may display this text along with its response to the user. 

GNU Radius replies with an Access-Challenge packet when it needs to obtain more information from the user in order to determine the user's authenticity or to determine the kind of service to be provided to the user. 

An Access-Challenge packet may include one or more Reply-Message attributes, and it may or may not include a single State attribute. No other attributes are permitted in an Access-Challenge packet. 

Upon receipt of an Access-Challenge, the Identifier field is matched with a pending Access-Request. Additionally, the Response Authenticator field must contain the correct response for the pending Access-Request. In the event of an invalid packet, GNU Radius discards the offending packet and issues the appropriate log message. 

If the NAS does not support challenge/response, it treats an Access-Challenge as though it had received an Access-Reject instead. Otherwise, upon receipt of a valid Access-Challenge the NAS prompts the user for a response, possibly displaying the text message provided in the Reply-Message attributes of the request. It then sends its original Access-Request with a new request ID and request authenticator, along with the User-Password attribute replaced by the encrypted user's response, and including the State attribute from the Access-Challenge, if any. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.2.2 Accounting Requests 
Accounting-Request packets are sent from a NAS to a RADIUS server to allow for accounting of a service provided to a user. 

Upon receipt of an Accounting-Request packet, the server attempts to record it (see section 8. Accounting), and if it succeeds in doing so, it replies with an Accounting-Response packet. Otherwise, it sends no reply, which then causes the NAS to retransmit its request within a preconfigured interval of time. Such retransmits will continue until either the server responds with an Accounting-Response packet or a preconfigured number of retransmits is reached, whichever occurs first. 

Any attribute valid in an Access-Request or Access-Accept packet is also valid in an Accounting-Request packet, except the following attributes, which are never present in any Accounting-Request packet: 


User-Password 
CHAP-Password 
Reply-Message 
State 
Either a NAS-IP-Address or a NAS-Identifier must be present in an Accounting-Request packet. It should contain either a NAS-Port or a NAS-Port-Type attribute (or both), unless the service does not involve a port or the NAS does not distinguish among its ports. 

If the Accounting-Request packet includes a Framed-IP-Address, that attribute must contain the actual IP of the user. 

There are five types of accounting packets, differentiated by the value of the Acct-Status-Type attribute. These are: 


Session Start Packet 
The session start packet is sent after the user has successfully passed the authentication and has started to receive the requested service. It must contain at least following attributes: 

Acct-Status-Type = Start 
User-Name 
Acct-Session-Id 
NAS-IP-Address 
NAS-Port-Id 

Session Stop Packet 
The session stop packet is sent after the user has disconnected. It conveys the information about the duration of the session, number of octets transferred, etc. It must contain at least the following attributes: 

Acct-Status-Type = Stop 
User-Name 
NAS-IP-Address 
Acct-Session-Id 
The last three of them are used to find the corresponding session start packet. 


Keepalive Packet 
The keepalive packet is sent by the NAS when it obtains some new information about the user's session, e.g. it has determined its IP or has changed the connection speed. The packet must contain at least the following attributes: 

Acct-Status-Type = Alive 
User-Name 
NAS-IP-Address 
Acct-Session-Id 

Accounting-Off Packet 
By sending this packet, the NAS requests that radiusd mark all sessions registered from this particular NAS as finished. Receiving this packet usually means that the NAS is to be shut down, or is about to change its configuration in a way that requires all currently opened sessions to be closed. The packet must contain at least the following attributes: 

Acct-Status-Type = Accounting-Off 
NAS-IP-Address 

Accounting-On Packet 
By sending this packet, the NAS informs radiusd that it is ready to accept the incoming connections. Usually this packet is sent after startup, or after a major reconfiguration of the NAS. It must contain at least the following attributes: 

Acct-Status-Type = Accounting-On 
NAS-IP-Address 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.3 Matching Rule 
A record in the GNU Radius database describing a particular rule for matching an incoming request is called a matching rule. Each such rule defines an action to be taken when the match occurs. 

The matching rule consists of three distinct parts: 


Label 
This is used to identify the rule. The special usernames DEFAULT and BEGIN are reserved. These will be described in detail below. 

Left-Hand Side (LHS) 
The list of attribute-value pairs used for matching the profile against an incoming request. 

Right-Hand Side (RHS) 
The list of attribute-value pairs that define the action to be taken if the request matches LHS. 
The following GNU Radius configuration files keep data in a matching rule format: `hints', `huntgroups', and `users'. Although they keep data in a similar format, the rules that are used to match incoming requests against the contents of these files differ from file to file. The following section describes these rules in detail. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.4 Processing Requests 
Upon receiving a request, radiusd applies to it a number of checks to determine whether the request comes from an authorized source. If these checks succeed, the request is processed and answered. Otherwise, the request is dropped and corresponding error message is issued (see section 9. Logging). 

The following checks are performed: 


Check if the username is supplied. 
If the packet lacks the User-Name attribute, it is not processed. 
Check if the NAS is allowed to speak. 
The source IP of the machine that sent the packet is looked up in the `clients' file (see section 5.3 Clients List -- `raddb/clients'). If no match is found, the request is rejected. 
Compute the encryption key. 
Using the data from the packet and the shared key value from the `clients' file, Radius computes the MD5 encryption key that will be used to decrypt the value of the User-Password attribute. 
Process user-name hints. 
User-name hints are special rules that modify the request depending on the user's name and her credentials. These rules allow an administrator to divide users into distinct groups, each group having its own authentication and/or accounting methods. The user-name hints are stored in `raddb/hints' (see section 5.6 Request Processing Hints -- `raddb/hints'). 
Process huntgroup rules. 
Huntgroup rules allow an administrator to segregate incoming requests depending on the NAS and/or port number they came from. These rules are stored in `raddb/huntgroups' (see section 5.7 Huntgroups -- `raddb/huntgroups'). 
Determine whether the request must be proxied to another RADIUS server. 
The requests pertaining to another realm are immediately forwarded to the remote RADIUS server for further processing. See section 3.4.2 Proxying, for the description of this process. 
Process individual user profiles 
This step applies only to authentication requests. 

3.4.1 Checking for Duplicate Requests     
3.4.2 Proxying     
3.4.3 Hints     
3.4.4 Huntgroups     
3.4.5 User Profiles     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.4.1 Checking for Duplicate Requests 
As described above (see section 3. How Radius Operates), a NAS may decide to retransmit the request under certain circumstances. This behavior ensures that no requests are lost. For example, consider the following scenario: 


The NAS sends a request to the server. 
The server processes it and sends back the reply. 
The reply is lost due to a network outage, or the load average of the NAS is too high and it drops the response. 
The NAS retransmits the request. 
Thus the RADIUS server will receive and process the same request twice. This probably won't do any harm if the request in question is an authentication one, but for accounting requests it will lead to duplicate accounting. To avoid such an undesirable effect, radiusd keeps a queue of received requests. When an incoming request arrives, radiusd first scans the request queue to see if the request is a duplicate. If so, it drops the request; otherwise, it inserts the request into the queue for processing. After the request is completed, it will still reside in the queue for a preconfigured interval of time (see section 5.1.3 auth statement, parameter request-cleanup-delay). 

By default, radiusd considers two requests to be equal if the following conditions are met: 


Both requests come from the same NAS. 
They are of the same type. 
The request identifier is the same for both requests. 
The request authenticator is the same for both requests. 
Additionally, radiusd may be configured to take into account the contents of both requests. This may be necessary, since some NASes modify the request authenticator or request identifier before retransmitting the request, so the method described above fails to recognize the request as a duplicate. This extended comparison is described in detail in 6.1 Extended Comparison. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.4.2 Proxying 
Proxying is a mode of operation where a RADIUS server forwards incoming requests from a NAS to another RADIUS server, waits for the latter to reply, and then forwards the reply back to the requesting NAS. A common use for such operation mode is to provide roaming between several internet service providers (ISPs). Roaming permits ISPs to share their resources, allowing each party's users to connect to other party's equipment. Thus, users traveling outside the area of one ISP's coverage are still able to access their services through another ISP. 


3.4.2.1 Proxy Service     
3.4.2.2 Realms     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.4.2.1 Proxy Service 
Suppose the ISP `Local' has a roaming arrangement with the ISP `Remote'. When the user of `Remote' dials in to the NAS of `Local', the NAS sends the authentication request to the `Local' RADIUS server. The server then determines that this is a roaming user, stores a copy of the request in its internal queue, and forwards the request to the `Remote' RADIUS server for processing. Thus, the `Local' RADIUS server acts as a client for the `Remote' RADIUS server. 

When the `Remote' RADIUS server responds, the `Local' RADIUS server receives the response, and passes it back to the NAS. The copy of the request from the server's queue determines which NAS originated the request. Before passing the request back to the NAS, the server removes information specific to the `Remote' site, such as Framed-IP-Address, Framed-Netmask, etc. Only the attributes marked with a `propagation' flag (see section 3.1 Attributes) are passed back to the NAS. After removing site-specific attributes, the `Local' RADIUS server passes the request through its user profiles (see section 3.4.5 User Profiles) to insert any local, site-specific information that might be needed. Finally, it passes the reply back to the NAS. 

Proxied accounting requests are processed in a similar manner, except that no attribute filtering takes place, as accounting responses do not carry any A/V pairs. 

This example illustrates only the simplest proxy chain, consisting of two servers; real-life proxy chains may consist of several servers. For example, our `Remote' RADIUS server might also act as a proxy, forwarding the request to yet another RADIUS server, and so on. 

Note that when the accounting request passes through a chain of forwarding servers, the accounting records are stored on all servers in the chain. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.4.2.2 Realms 
GNU Radius determines which server a request must be forwarded to by the request's authentication realm. There are three kinds of realms: 


A named realm is the part of a user name following the at sign (`@'). For example, if the user name is `jsmith@this.net', then `this.net' is the realm. The named realms can be cascaded; e.g., a request with user name `jsmith@this.net@remote.net' will first be forwarded to the RADIUS server of the realm `remote.net', which in turn will forward it to `this.net'. 
A default realm defines the server to which the requests for realms not mentioned explicitly in the configuration are forwarded. 
An empty realm defines the server to which the requests without explicitly named realms are forwarded. If the configuration does not define an empty realm, such requests are processed by the server itself. 
GNU Radius keeps the information about the realms it serves in the `raddb/realms' configuration file (see section 5.8 List of Proxy Realms -- `raddb/realms'). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.4.3 Hints 
User-name hints are special rules that modify the incoming request depending on the user name and its credentials. Hints are stored as a list of matching rules (see section 3.3 Matching Rule). Upon receiving a request, radiusd scans the hint entries sequentially, comparing each rule's label with the value of the User-Name attribute from the request. If they coincide, then radiusd appends the contents of the rule's RHS to the request's pair list. 

The two user names must match exactly in order for a hint to take effect, unless the hint's checklist contains either the Prefix or the Suffix attribute. The special name `DEFAULT' or `DEFAULT%d' (where %d denotes any decimal number), used as a hint's label, matches any user name. 

Two special attributes, Prefix and Suffix, may be used in LHS to restrict the match to a specified part of a user name. Both are string attributes. The Prefix instructs radiusd to accept the hint only if the user name begins with the given prefix. Similarly, Suffix instructs radiusd to accept the hint only if the user name ends with the given suffix. A hint may contain both Prefix and Suffix attributes. 

In addition to these two attributes, a hint's LHS may contain User-ID and Group attributes. 

The following attributes, when used in a hint's RHS have special meaning. They are not appended to the request pair list. Instead, they are removed after completing their function: 

Fall-Through 
If this attribute is present and is set to Yes, radiusd continues scanning the hints after processing the current entry. This allows radiusd to apply several hints to a single packet. 
Rewrite-Function 
If this attribute is present, the specified rewrite function is invoked. 
Replace-User-Name 
The value of this attribute is expanded (see section 5.14 Macro Substitution) and replaces the value of the User-Name attribute from the request. 
Hint rules are defined in the `raddb/hints' file (see section 5.6 Request Processing Hints -- `raddb/hints'). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.4.4 Huntgroups 
Huntgroups are special rules that allow an administrator to provide alternate processing of certain incoming requests depending on the NAS IP and port number they come from. These rules are stored as a list of matching rules (see section 3.3 Matching Rule). 

Upon receiving a request, radiusd scans this list sequentially until it finds an entry such that the conditions set forth in its LHS are matched by the request. If such an entry is found, radiusd verifies that the request meets the conditions described by RHS. If it does not, the request is rejected. In short, a huntgroup requires that any request matching its LHS must match also its RHS. 

The label part of the rule is not used in comparisons; instead it is used to label huntgroups. All entries with the same label form a single huntgroup. The special attribute Huntgroup-Name can be used to request a match against a particular huntgroup (see section 14.3.12 Huntgroup-Name). 

Huntgroup rules are defined in the `raddb/huntgroups' file (see section 5.7 Huntgroups -- `raddb/huntgroups'). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

3.4.5 User Profiles 
User profiles are per-user matching rules (see section 3.3 Matching Rule). All incoming authentication requests are compared with the user profiles after they have passed both hints and huntgroups. radiusd selects the user profiles whose label matches the value of the User-Name attribute from the incoming request. 

The selected profiles form the list of authentication rules for the request. In order for a profile to be selected, its label must either coincide literally with the User-Name value, or be one of the special labels, DEFAULT or BEGIN. 

Rules in an authentication list are ordered as follows: first go all the profiles with the BEGIN label, followed by the profiles whose labels match the User-Name literally, followed finally by the rules labeled with the DEFAULT. (1) 

Within each of the three sublists, the rules preserve the order in which they appear in the `raddb/users' file. Once the list is constructed, it is scanned sequentially until the rule is found whose LHS matches the incoming request. If no such rule is found, the authentication fails. Otherwise, the contents of its RHS are appended to the reply list being constructed. If the RHS of the matched rule contains the attribute Fall-Through with the value Yes, the matching continues. When the list is exhausted, the authentication result is sent back to the NAS along with the A/V pairs collected in the reply list. 

User profiles are defined in the `raddb/users' file (see section 5.9 User Profiles -- `raddb/users'). 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

4. How to Start the Daemon. 
When started radiusd uses the configuration values from the following sources (in order of increasing precedence): 


Compiled-in defaults 
`raddb/config' file. 
Command line arguments 
Whenever a command line options has its equivalent in config file the use of this equivalent should be preferred (see section 5.1 Run-Time Configuration Options -- `raddb/config'). 

The following command line options are accepted: 


`-A' 
`--log-auth-detail' 
Enable detailed authentication logging. When this option is specified each authentication request is logged to the file `radacct/NASNAME/detail.auth', where NASNAME is replaced by the short name of the NAS from `raddb/naslist' 2. Naming Conventions. 
Config file equivalent: auth { detail yes; };. 


`-a DIR' 
`--acct-directory DIR' 
Specify accounting directory. 
Config file equivalent: option { acct-dir DIR; };. 


`-b' 
`--dbm' 
Enable DBM support. 
Config file equivalent: usedbm yes;. 


`-d DIR' 
`--config-directory DIR' 
`--directory D' 
Specify alternate configuration directory. Default is `/usr/local/etc/raddb'. 

`-f' 
`--foreground' 
Stay in foreground. We recommend to use it for debugging purposes only. 

`-i IP' 
`--ip-address' 
Specifies the IP address radiusd will listen on. If this option is not specified, the program will listen on all IP addresses, assigned to the machine it runs on. 
Config file equivalent: option { source-ip IP; };. 

Note that listen statement in `raddb/config' provides a better control over IP addresses to listen on (see section 5.1.3 auth statement, and see section 5.1.4 acct statement). 


`-L' 
`--license' 
Display GNU General Public License and exit. 

`-l DIR' 
`--logging-directory DIR' 
Specify alternate logging directory. 
Config file equivalent: option { log-dir DIR; };. 


`-mb' 
`--mode b' 
"Builddbm" mode. Builds a DBM version of a plaintext users database. 12.8 builddbm. 

`-mc' 
`--mode c' 
Check configuration files and exit. All errors are reported via usual log channels. 

`-mt' 
`--mode t' 
Test mode. In this mode radiusd starts an interactive interpreter which allows to test various aspects of its configuration. 

`-N' 
`--auth-only' 
Process only authentication requests. 

`-n' 
`--do-not-resolve' 
Do not resolve IP addresses for diagnostic output. This can reduce the amount of network traffic and speed up the server. 

Config file equivalent: option { resolve no };. 


`-p PORTNO' 
`--port PORTNO' 
Listen the UDP port PORTNO. The accounting port is computed as PORTNO + 1. 

`-P DIR' 
`--pid-file-dir DIR' 
Specifies the alternate path for the pidfile. 

`-S' 
`--log-stripped-names' 
Log usernames stripped off any prefixes/suffixes. 
Config file equivalent: auth { strip-names yes };. 


`-s' 
`--single-process' 
Run in single process mode. This is for debugging purposes only. We strongly recommend against using this option. Use it only when absolutely necessary. 

`-v' 
`--version' 
Display program version and compilation options. 

`-x DEBUG_LEVEL' 
`--debug DEBUG_LEVEL' 
Set debugging level. DEBUG_LEVEL is a comma-separated list of assignments in the forms 
  MODULE
MODULE = LEVEL

 


where MODULE is the module name or any non-ambiguous assignment thereof, LEVEL is the debugging level in the range 0-100. 10.2 Debugging 

Config file equivalent:   logging {
        category debug {
                level DEBUG_LEVEL;
        };
};

 



`-y' 
`--log-auth' 
Log authentications. With this option enabled, Radius will log any authentication attempt into its log file 9. Logging. 
Config file equivalent: logging { category auth { detail yes; }; }; . 


`-z' 
`--log-auth-pass' 
Log passwords along with authentication information. Do not use this option. It is very insecure, since all users' passwords will be echoed in the logfile. This option is provided only for debugging purposes. 
Config file equivalent:   logging {
        category auth {
                print-pass yes;
        };
};

 



See section 5.1 Run-Time Configuration Options -- `raddb/config'. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5. Radius Configuration Files 
At startup, GNU Radius obtains the information vital for its functioning from a number of configuration files. These are normally found in /usr/local/etc/raddb directory, which is defined at configuration time, although their location can be specified at runtime. In the discussion below we will refer to this directory by `raddb'. See section 2. Naming Conventions. 

Each configuration file is responsible for a certain part of the GNU Radius functionality. The following table lists all configuration files along with a brief description of their purposes. 


`config' 
Determines the runtime defaults for radiusd, such as the IP address and ports to listen on, the sizes of the request queues, configuration of the SNMP subsystem, fine-tuning of the extension languages, etc. 

`clients' 
Lists the shared secret belonging to each NAS. It is crucial for the normal request processing that each NAS have an entry in this file. The requests from NASes that are not listed in `clients' will be ignored, as well as those from the NASes that have a wrong value for the shared secret configured in this file. 

`naslist' 
Defines the types for the known NASes. Its information is used mainly when performing multiple login checking (see section 7.9 Multiple Login Checking). 

`nastypes' 
Declares the known NAS types. The symbolic type names, declared in this file can be used in `naslist'. 

`dictionary' 
Defines the symbolic names for radius attributes and attribute values. Only the names declared in this file may be used in the files `users', `hints' and `huntgroups'. 

`huntgroups' 
Contains special rules that process the incoming requests basing on the NAS IP and port number they come from. These can also be used as a kind of access control list. 

`hints' 
Defines the matching rules that modify the incoming request depending on the user name and its credentials. 

`users' 
Contains the individual users' profiles. 

`realms' 
Defines the Radius realms and the servers that are responsible for them. 

`access.deny' 
A list of usernames that should not be allowed access via Radius. 

`sqlserver' 
Contains the configuration for the SQL system. This includes the type of SQL interface used, the IP and port number of the server and the definition of the SQL requests used by radiusd. 

`rewrite' 
Contains the source code of functions in Rewrite extension language. 

`menus' 
A subdirectory containing the authentication menus. 
The rest of this chapter describes each of these files in detail. 


5.1 Run-Time Configuration Options -- `raddb/config'    Run-time configuration options. 
5.2 Dictionary of Attributes -- `raddb/dictionary'    Radius dictionary. 
5.3 Clients List -- `raddb/clients'    Clients lists the NASes that are allowed to communicate with radius. 
5.4 NAS List -- `raddb/naslist'    The naslist file keeps general information about the NASes. 
5.5 NAS Types -- `raddb/nastypes'    Information about how to query the NASes about active user sessions. 
5.6 Request Processing Hints -- `raddb/hints'    Important user information that is common for the users whose names match some pattern. 
5.7 Huntgroups -- `raddb/huntgroups'    Group users by the NAS (and, possibly, a port number) they come from. 
5.8 List of Proxy Realms -- `raddb/realms'    Communication with remote radius servers 
5.9 User Profiles -- `raddb/users'    User profile. 
5.10 List of Blocked Users -- `raddb/access.deny'    List of users which are denied access. 
5.11 SQL Configuration -- `raddb/sqlserver'    SQL server configuration. 
5.12 Rewrite functions -- `raddb/rewrite'    Rewrite functions allow to change the input packets. 
5.13 Login Menus -- `raddb/menus'    Menus allow user to select the type of service. 
5.14 Macro Substitution    Macros which are expanded by the actual attribute values. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1 Run-Time Configuration Options -- `raddb/config' 
At startup radiusd obtains its configuration values from three places. The basic configuration is kept in the executable module itself. These values are overridden by those obtained from `raddb/config' file. Finally, the options obtained from the command line override the first two sets of options. 

When re-reading of the configuration is initiated either by SIGHUP signal or by SNMP channel any changes in the config file take precedence over command line arguments, since `raddb/config' is the only way to change configuration of the running program. 

This chapter discusses the `raddb/config' file in detail. 

The `raddb/config' consists of statements and comments. Statements end with a semicolon. Many statements contain a block of sub-statements which also terminate with a semicolon. 

Comments can be written in shell, C, or C++ constructs, i.e. any of the following represent a valid comment: 

  # A shell comment
/* A C-style
 * multi-line comment
 */
// A C++-style comment

 


These are the basic statements: 

5.1.1 option block    Option block: set the global program options. 
5.1.2 logging block    Fine-tune the logging. 
5.1.3 auth statement    Configure authentication service. 
5.1.4 acct statement    Configure accounting service. 
5.1.5 usedbm statement    Enable the DBM feature. 
5.1.6 snmp statement    Configure SNMP service. 
5.1.7 rewrite statement.    Configure Rewrite interface. 
5.1.8 guile statement    Configure Guile interface. 
5.1.9 message statement    Configure server reply messages. 
5.1.10 filters statement    Configure authentication and accounting filters. 
5.1.11 mlc statement    Configure multiple login checking. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.1 option block 

Syntax: 
  option {
        source-ip number ; 
        max-requests number ; 
        radiusd-user string ; 
        exec-program-user string ; 
        username-chars string ; 
        log-dir string ; 
        acct-dir string ; 
        resolve bool ; 
        max-processes number ; 
        process-idle-timeout number ; 
        master-read-timeout number ; 
        master-write-timeout number ; 
} ;

 



Usage 
The option block defines the global options to be used by radiusd. 


Boolean statements 

resolve 
Determines whether radius should resolve the IP addresses for diagnostic output. Specifying resolve no speeds up the server and reduces the network traffic. 


Numeric statements 

source-ip 
Sets the source IP address. When this statement is not present, the IP address of the first available network interface on the machine will be used as source. 

max-requests 
Sets the maximum number of the requests in queue. 

max-processes 
Sets the maximum number of child processes. The default value is 16. If you plan to raise this value, make sure you have enough file descriptors available, as each child occupies four descriptors for its input/output channels. 

process-idle-timeout 
Sets the maximum idle time for child processes. A child terminates if it does not receive any requests from the main process within this number of seconds. By default, this parameter is 3600 seconds (one hour). 

master-read-timeout 
master-write-timeout 
These two values set the timeout values for the interprocess input/output operations in the main server process. More specifically, master-read-timeout sets the maximum number of seconds the main process will wait for the answer from the subprocess, and master-write-timeout sets the maximum number of seconds the main process will wait for the subprocess's comunication channel to become ready for input. By default, no timeouts are imposed. 


String statements 
radiusd-user 
Instructs radiusd to drop root privileges and to switch to the real user and group IDs of the given user after becoming daemon. Notice the following implications of this statement: 

All configuration files must be readable for this user. 
Authentication type System (see section 7.5 System Authentication Type) requires root privileges, so it cannot be used with radiusd-user. Any `raddb/users' profiles using this authentication type will be discarded. 
Authentication type PAM (see section 7.7 PAM Authentication Type) may require root provileges. It is reported to always require root privileges on some systems (notably on Solaris). 
exec-program-user statement (see below) is ignored when used with radiusd-user. 

exec-program-user 
Sets the privileges for the programs executed as a result of Exec-Program and Exec-Program-Wait. The real user and group ids will be retrieved from the `/etc/passwd' entry for the given user. 

username-chars 
Determines characters that are valid within a username. The alphanumeric characters are always allowed in a username, it is not necessary to specify them in this statement. By default the following characters are allowed in a username: `.-_!@#$%^&\/"'. The username-chars statement overrides this default, thus setting: 
  username-chars ":"

 


will restrict the set of allowed characters to the alphanumeric characters and colon. If you wish to expand the default character set, you will have to explicitly specify it in the username-chars argument, as shown in the example below: 

  username-chars ".-_!@#$%^&\\/\":"

 


(Notice the use of escape character `\'). 


log-dir 
Specifies the logging directory. 

acct-dir 
Specifies the accounting directory. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.2 logging block 

Syntax: 
  logging {
        prefix-hook string ; 
        suffix-hook string ; 
        category category_spec {
                channel channel_name ; 
                print-auth bool ; 
                print-pass bool ; 
                print-failed-pass bool ; 
                level debug_level ; 
        } ; 
        channel channel_name {
                file string ;
                syslog facility . priority ; 
                print-pid bool ; 
                print-category bool ; 
                print-cons bool ; 
                print-level bool ; 
                print-priority bool ; 
                print-tid bool; 
                print-milliseconds bool; 
                prefix-hook string ; 
                suffix-hook string ; 
        }; 
} ;


 



Usage 
The logging statement describes the course followed by radiusd's logging information. 

The parts of this statement are discussed below. 


5.1.2.1 Logging hooks     
5.1.2.2 category statement     
5.1.2.3 channel statement     
5.1.2.4 Example of the logging statement     



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.2.1 Logging hooks 
Most diagnostic messages displayed by radiusd describe some events that occured while processig a certain incoming request. By default they contain only a short summary of the event. Logging hooks are means of controlling actual amount of information displayed in such messages. They allow you to add to the message being displayed any relevant information from the incoming request that caused the message to appear. 

A hook is a special Rewrite function that takes three arguments and returns a string. There are two kinds of logging hooks: prefix and suffix. Return value from the prefix hook function will be displayed before the actual log message, that of the suffix hook function will be displayed after the message. 

Furthermore, there may be global and channel-specific hooks. Global hooks apply to all categories, unless overridden by category-specific hooks. Global prefix hook is enabled by prefix-hook statement appearing in the logging block. Global suffix hook is enabled by suffix-hook statement. Both statements take as their argument the name of corresponding Rewrite function. 

For detailed information about writing logging hooks, See section 11.2.7 Logging Hook Functions. 



--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.2.2 category statement 
Each line of logging information generated by radiusd has an associated category. The logging statement allows each category of output to be controlled independently of the others. The logging category is defined by category name and a severity. category name determines what part of radiusd daemon is allowed to send its logging information to this channel. It can be any of main, auth, acct, proxy, snmp. priority determines the minimum priority of the messages displayed by this channel. The priorities in ascending order are: debug, info, notice, warn, err, crit, alert, emerg. 

The full category specification, denoted by the category_spec in the above section, can take any of the following three forms: 


category_name 
Print the messages of given category. 
priority 
Print messages of all categories, abridged by given priority. If the priority is prefixed with `=', only messages with given priority will be displayed. If it is prefixed with `!', the messages with priority other than the specified will be displayed. Otherwise, the messages with priorities equal to or greater than the specified will be displayed. 
category_name . priority 
Print the messages of given category, abridged by given priority. The priority may be prefixed with either `=' or `!' as described above. The dot (`.') separates the priority from the category name, it may be surrounded by any amount of whitespace. 
Additional category options valid for auth category are: 

print-auth 
Log individual authentications. 
print-pass 
Include passwords for successful authentications. It is very insecure, since all users' passwords will be echoed in the logfile. This option is provided only for debugging purposes. 
print-failed-pass 
Include passwords for failed authentications. 


--------------------------------------------------------------------------------
[ < ] [ > ]    [ << ] [ Up ] [ >> ]             [Top] [Contents] [Index] [ ? ] 

5.1.2.3 channel statement 
Channels represent methods for recording logging information. Each channel has a unique name, and any categories which specify that name in a channel statement will use that channel. 

radiusd can write logging information to files or send it to syslog. The file statement sends the channel's output to the named file (see section 2. Naming Conventions). The syslog statement sends the channel's output to syslog with the specified facility and severity. 

Channel options modify the data flowing through the channel: 


print-pid 
Add the process ID of the process generating the logging information. 
print-cons 
Also send the logging information to the system console. 
print-category 
Add the category name to the logging information. 
print-priority 
print-level 
Add the priority name to the logging information. 
print-milliseconds 
Print timestamp with milliseconds. 
prefix-hook 
Declares the name of Rewrite function used as logging prefix hook for that channel (see section 5.1.2.1 Logging hooks). This overrides any global prefix hook. 
suffix-hook 
Declares the name of Rewrite function used as logging suffix hook for that channel (see section 5.1.2.1 Logging

 hooks). This overrides any global suffix hook. 


This manual documents version 1.10 of GNU Wget, the freely available utility for network downloads. 

Copyright   1996 2005 Free Software Foundation, Inc. 

Overview: Features of Wget. 
Invoking: Wget command-line arguments. 
Recursive Download: Downloading interlinked pages. 
Following Links: The available methods of chasing links. 
Time-Stamping: Mirroring according to time-stamps. 
Startup File: Wget's initialization file. 
Examples: Examples of usage. 
Various: The stuff that doesn't fit anywhere else. 
Appendices: Some useful references. 
Copying: You may give out copies of Wget and of this manual. 
Concept Index: Topics covered by this manual. 


--------------------------------------------------------------------------------
Next: Invoking, Previous: Top, Up: Top 

1 Overview
GNU Wget is a free utility for non-interactive download of files from the Web. It supports http, https, and ftp protocols, as well as retrieval through http proxies. 

This chapter is a partial overview of Wget's features. 

Wget is non-interactive, meaning that it can work in the background, while the user is not logged on. This allows you to start a retrieval and disconnect from the system, letting Wget finish the work. By contrast, most of the Web browsers require constant user's presence, which can be a great hindrance when transferring a lot of data. 
     
     
Wget can follow links in html and xhtml pages and create local versions of remote web sites, fully recreating the directory structure of the original site. This is sometimes referred to as  recursive downloading.  While doing that, Wget respects the Robot Exclusion Standard (/robots.txt). Wget can be instructed to convert the links in downloaded html files to the local files for offline viewing. 
     
     
File name wildcard matching and recursive mirroring of directories are available when retrieving via ftp. Wget can read the time-stamp information given by both http and ftp servers, and store it locally. Thus Wget can see if the remote file has changed since last retrieval, and automatically retrieve the new version if it has. This makes Wget suitable for mirroring of ftp sites, as well as home pages. 
     
     
Wget has been designed for robustness over slow or unstable network connections; if a download fails due to a network problem, it will keep retrying until the whole file has been retrieved. If the server supports regetting, it will instruct the server to continue the download from where it left off. 
     
     
Wget supports proxy servers, which can lighten the network load, speed up retrieval and provide access behind firewalls. However, if you are behind a firewall that requires that you use a socks style gateway, you can get the socks library and build Wget with support for socks. Wget uses the passive ftp downloading by default, active ftp being an option. 
     
     
Wget supports IP version 6, the next generation of IP. IPv6 is autodetected at compile-time, and can be disabled at either build or run time. Binaries built with IPv6 support work well in both IPv4-only and dual family environments. 
     
     
Built-in features offer mechanisms to tune which links you wish to follow (see Following Links). 
     
     
The progress of individual downloads is traced using a progress gauge. Interactive downloads are tracked using a  thermometer -style gauge, whereas non-interactive ones are traced with dots, each dot representing a fixed amount of data received (1KB by default). Either gauge can be customized to your preferences. 
     
     
Most of the features are fully configurable, either through command line options, or via the initialization file .wgetrc (see Startup File). Wget allows you to define global startup files (/usr/local/etc/wgetrc by default) for site settings. 
     
     
Finally, GNU Wget is free software. This means that everyone may use it, redistribute it and/or modify it under the terms of the GNU General Public License, as published by the Free Software Foundation (see Copying). 


--------------------------------------------------------------------------------
Next: Recursive Download, Previous: Overview, Up: Top 

2 Invoking
By default, Wget is very simple to invoke. The basic syntax is: 

     
     wget [option]... [URL]...
     

Wget will simply download all the urls specified on the command line. URL is a Uniform Resource Locator, as defined below. 

However, you may wish to change some of the default parameters of Wget. You can do it two ways: permanently, adding the appropriate command to .wgetrc (see Startup File), or specifying it on the command line. 

URL Format 
Option Syntax 
Basic Startup Options 
Logging and Input File Options 
Download Options 
Directory Options 
HTTP Options 
HTTPS (SSL/TLS) Options 
FTP Options 
Recursive Retrieval Options 
Recursive Accept/Reject Options 


--------------------------------------------------------------------------------
Next: Option Syntax, Up: Invoking 

2.1 URL Format
URL is an acronym for Uniform Resource Locator. A uniform resource locator is a compact string representation for a resource available via the Internet. Wget recognizes the url syntax as per rfc1738. This is the most widely used form (square brackets denote optional parts): 

     http://host[:port]/directory/file
     ftp://host[:port]/directory/file

You can also encode your username and password within a url: 

     ftp://user:password@host/path
     http://user:password@host/path

Either user or password, or both, may be left out. If you leave out either the http username or password, no authentication will be sent. If you leave out the ftp username, anonymous will be used. If you leave out the ftp password, your email address will be supplied as a default password.1 

Important Note: if you specify a password-containing url on the command line, the username and password will be plainly visible to all users on the system, by way of ps. On multi-user systems, this is a big security risk. To work around it, use wget -i - and feed the urls to Wget's standard input, each on a separate line, terminated by C-d. 

You can encode unsafe characters in a url as %xy, xy being the hexadecimal representation of the character's ascii value. Some common unsafe characters include % (quoted as %25), : (quoted as %3A), and @ (quoted as %40). Refer to rfc1738 for a comprehensive list of unsafe characters. 

Wget also supports the type feature for ftp urls. By default, ftp documents are retrieved in the binary mode (type i), which means that they are downloaded unchanged. Another useful mode is the a (ASCII) mode, which converts the line delimiters between the different operating systems, and is thus useful for text files. Here is an example: 

     ftp://host/directory/file;type=a

Two alternative variants of url specification are also supported, because of historical (hysterical?) reasons and their widespreaded use. 

ftp-only syntax (supported by NcFTP): 

     host:/dir/file

http-only syntax (introduced by Netscape): 

     host[:port]/dir/file

These two alternative forms are deprecated, and may cease being supported in the future. 

If you do not understand the difference between these notations, or do not know which one to use, just use the plain ordinary format you use with your favorite browser, like Lynx or Netscape. 



--------------------------------------------------------------------------------
Next: Basic Startup Options, Previous: URL Format, Up: Invoking 

2.2 Option Syntax
Since Wget uses GNU getopt to process command-line arguments, every option has a long form along with the short one. Long options are more convenient to remember, but take time to type. You may freely mix different option styles, or specify options after the command-line arguments. Thus you may write: 

     wget -r --tries=10 http://fly.srk.fer.hr/ -o log

The space between the option accepting an argument and the argument may be omitted. Instead -o log you can write -olog. 

You may put several options that do not require arguments together, like: 

     wget -drc URL

This is a complete equivalent of: 

     wget -d -r -c URL

Since the options can be specified after the arguments, you may terminate them with --. So the following will try to download url -x, reporting failure to log: 

     wget -o log -- -x

The options that accept comma-separated lists all respect the convention that specifying an empty list clears its value. This can be useful to clear the .wgetrc settings. For instance, if your .wgetrc sets exclude_directories to /cgi-bin, the following example will first reset it, and then set it to exclude /~nobody and /~somebody. You can also clear the lists in .wgetrc (see Wgetrc Syntax). 

     wget -X '' -X /~nobody,/~somebody

Most options that do not accept arguments are boolean options, so named because their state can be captured with a yes-or-no ( boolean ) variable. For example, --follow-ftp tells Wget to follow FTP links from HTML files and, on the other hand, --no-glob tells it not to perform file globbing on FTP URLs. A boolean option is either affirmative or negative (beginning with --no). All such options share several properties. 

Unless stated otherwise, it is assumed that the default behavior is the opposite of what the option accomplishes. For example, the documented existence of --follow-ftp assumes that the default is to not follow FTP links from HTML pages. 

Affirmative options can be negated by prepending the --no- to the option name; negative options can be negated by omitting the --no- prefix. This might seem superfluous if the default for an affirmative option is to not do something, then why provide a way to explicitly turn it off? But the startup file may in fact change the default. For instance, using follow_ftp = off in .wgetrc makes Wget not follow FTP links by default, and using --no-follow-ftp is the only way to restore the factory default from the command line. 



--------------------------------------------------------------------------------
Next: Logging and Input File Options, Previous: Option Syntax, Up: Invoking 

2.3 Basic Startup Options
-V
--version
Display the version of Wget. 

-h
--help
Print a help message describing all of Wget's command-line options. 

-b
--background
Go to background immediately after startup. If no output file is specified via the -o, output is redirected to wget-log. 



-e command
--execute command
Execute command as if it were a part of .wgetrc (see Startup File). A command thus invoked will be executed after the commands in .wgetrc, thus taking precedence over them. If you need to specify more than one wgetrc command, use multiple instances of -e. 


--------------------------------------------------------------------------------
Next: Download Options, Previous: Basic Startup Options, Up: Invoking 

2.4 Logging and Input File Options
-o logfile
--output-file=logfile
Log all messages to logfile. The messages are normally reported to standard error. 



-a logfile
--append-output=logfile
Append to logfile. This is the same as -o, only it appends to logfile instead of overwriting the old log file. If logfile does not exist, a new file is created. 



-d
--debug
Turn on debug output, meaning various information important to the developers of Wget if it does not work properly. Your system administrator may have chosen to compile Wget without debug support, in which case -d will not work. Please note that compiling with debug support is always safe Wget compiled with the debug support will not print any debug info unless requested with -d. See Reporting Bugs, for more information on how to use -d for sending bug reports. 



-q
--quiet
Turn off Wget's output. 



-v
--verbose
Turn on verbose output, with all the available data. The default output is verbose. 

-nv
--non-verbose
Non-verbose output turn off verbose without being completely quiet (use -q for that), which means that error messages and basic information still get printed. 



-i file
--input-file=file
Read urls from file. If - is specified as file, urls are read from the standard input. (Use ./- to read from a file literally named -.) 
If this function is used, no urls need be present on the command line. If there are urls both on the command line and in an input file, those on the command lines will be the first ones to be retrieved. The file need not be an html document (but no harm if it is) it is enough if the urls are just listed sequentially. 

However, if you specify --force-html, the document will be regarded as html. In that case you may have problems with relative links, which you can solve either by adding <base href="url"> to the documents or by specifying --base=url on the command line. 




-F
--force-html
When input is read from a file, force it to be treated as an html file. This enables you to retrieve relative links from existing html files on your local disk, by adding <base href="url"> to html, or using the --base command-line option. 



-B URL
--base=URL
When used in conjunction with -F, prepends URL to relative links in the file specified by -i. 


--------------------------------------------------------------------------------
Next: Directory Options, Previous: Logging and Input File Options, Up: Invoking 

2.5 Download Options
--bind-address=ADDRESS
When making client TCP/IP connections, bind to ADDRESS on the local machine. ADDRESS may be specified as a hostname or IP address. This option can be useful if your machine is bound to multiple IPs. 



-t number
--tries=number
Set number of retries to number. Specify 0 or inf for infinite retrying. The default is to retry 20 times, with the exception of fatal errors like  connection refused  or  not found  (404), which are not retried. 

-O file
--output-document=file
The documents will not be written to the appropriate files, but all will be concatenated together and written to file. If - is used as file, documents will be printed to standard output, disabling link conversion. (Use ./- to print to a file literally named -.) 
Note that a combination with -k is only well-defined for downloading a single document. 




-nc
--no-clobber
If a file is downloaded more than once in the same directory, Wget's behavior depends on a few options, including -nc. In certain cases, the local file will be clobbered, or overwritten, upon repeated download. In other cases it will be preserved. 
When running Wget without -N, -nc, or -r, downloading the same file in the same directory will result in the original copy of file being preserved and the second copy being named file.1. If that file is downloaded yet again, the third copy will be named file.2, and so on. When -nc is specified, this behavior is suppressed, and Wget will refuse to download newer copies of file. Therefore,  no-clobber  is actually a misnomer in this mode it's not clobbering that's prevented (as the numeric suffixes were already preventing clobbering), but rather the multiple version saving that's prevented. 

When running Wget with -r, but without -N or -nc, re-downloading a file will result in the new copy simply overwriting the old. Adding -nc will prevent this behavior, instead causing the original version to be preserved and any newer copies on the server to be ignored. 

When running Wget with -N, with or without -r, the decision as to whether or not to download a newer copy of a file depends on the local and remote timestamp and size of the file (see Time-Stamping). -nc may not be specified at the same time as -N. 

Note that when -nc is specified, files with the suffixes .html or .htm will be loaded from the local disk and parsed as if they had been retrieved from the Web. 




-c
--continue
Continue getting a partially-downloaded file. This is useful when you want to finish up a download started by a previous instance of Wget, or by another program. For instance: 
          wget -c ftp://sunsite.doc.ic.ac.uk/ls-lR.Z
     
If there is a file named ls-lR.Z in the current directory, Wget will assume that it is the first portion of the remote file, and will ask the server to continue the retrieval from an offset equal to the length of the local file. 

Note that you don't need to specify this option if you just want the current invocation of Wget to retry downloading a file should the connection be lost midway through. This is the default behavior. -c only affects resumption of downloads started prior to this invocation of Wget, and whose local files are still sitting around. 

Without -c, the previous example would just download the remote file to ls-lR.Z.1, leaving the truncated ls-lR.Z file alone. 

Beginning with Wget 1.7, if you use -c on a non-empty file, and it turns out that the server does not support continued downloading, Wget will refuse to start the download from scratch, which would effectively ruin existing contents. If you really want the download to start from scratch, remove the file. 

Also beginning with Wget 1.7, if you use -c on a file which is of equal size as the one on the server, Wget will refuse to download the file and print an explanatory message. The same happens when the file is smaller on the server than locally (presumably because it was changed on the server since your last download attempt) because  continuing  is not meaningful, no download occurs. 

On the other side of the coin, while using -c, any file that's bigger on the server than locally will be considered an incomplete download and only (length(remote) - length(local)) bytes will be downloaded and tacked onto the end of the local file. This behavior can be desirable in certain cases for instance, you can use wget -c to download just the new portion that's been appended to a data collection or log file. 

However, if the file is bigger on the server because it's been changed, as opposed to just appended to, you'll end up with a garbled file. Wget has no way of verifying that the local file is really a valid prefix of the remote file. You need to be especially careful of this when using -c in conjunction with -r, since every file will be considered as an "incomplete download" candidate. 

Another instance where you'll get a garbled file if you try to use -c is if you have a lame http proxy that inserts a  transfer interrupted  string into the local file. In the future a  rollback  option may be added to deal with this case. 

Note that -c only works with ftp servers and with http servers that support the Range header. 




--progress=type
Select the type of the progress indicator you wish to use. Legal indicators are  dot  and  bar . 
The  bar  indicator is used by default. It draws an ascii progress bar graphics (a.k.a  thermometer  display) indicating the status of retrieval. If the output is not a TTY, the  dot  bar will be used by default. 

Use --progress=dot to switch to the  dot  display. It traces the retrieval by printing dots on the screen, each dot representing a fixed amount of downloaded data. 

When using the dotted retrieval, you may also set the style by specifying the type as dot:style. Different styles assign different meaning to one dot. With the default style each dot represents 1K, there are ten dots in a cluster and 50 dots in a line. The binary style has a more  computer -like orientation 8K dots, 16-dots clusters and 48 dots per line (which makes for 384K lines). The mega style is suitable for downloading very large files each dot represents 64K retrieved, there are eight dots in a cluster, and 48 dots on each line (so each line contains 3M). 

Note that you can set the default style using the progress command in .wgetrc. That setting may be overridden from the command line. The exception is that, when the output is not a TTY, the  dot  progress will be favored over  bar . To force the bar output, use --progress=bar:force. 


-N
--timestamping
Turn on time-stamping. See Time-Stamping, for details. 



-S
--server-response
Print the headers sent by http servers and responses sent by ftp servers. 



--spider
When invoked with this option, Wget will behave as a Web spider, which means that it will not download the pages, just check that they are there. For example, you can use Wget to check your bookmarks: 
          wget --spider --force-html -i bookmarks.html
     
This feature needs much more work for Wget to get close to the functionality of real web spiders. 




-T seconds
--timeout=seconds
Set the network timeout to seconds seconds. This is equivalent to specifying --dns-timeout, --connect-timeout, and --read-timeout, all at the same time. 
When interacting with the network, Wget can check for timeout and abort the operation if it takes too long. This prevents anomalies like hanging reads and infinite connects. The only timeout enabled by default is a 900-second read timeout. Setting a timeout to 0 disables it altogether. Unless you know what you are doing, it is best not to change the default timeout settings. 

All timeout-related options accept decimal values, as well as subsecond values. For example, 0.1 seconds is a legal (though unwise) choice of timeout. Subsecond timeouts are useful for checking server response times or for testing network latency. 




--dns-timeout=seconds
Set the DNS lookup timeout to seconds seconds. DNS lookups that don't complete within the specified time will fail. By default, there is no timeout on DNS lookups, other than that implemented by system libraries. 



--connect-timeout=seconds
Set the connect timeout to seconds seconds. TCP connections that take longer to establish will be aborted. By default, there is no connect timeout, other than that implemented by system libraries. 



--read-timeout=seconds
Set the read (and write) timeout to seconds seconds. The  time  of this timeout refers idle time: if, at any point in the download, no data is received for more than the specified number of seconds, reading fails and the download is restarted. This option does not directly affect the duration of the entire download. 
Of course, the remote server may choose to terminate the connection sooner than this option requires. The default read timeout is 900 seconds. 




--limit-rate=amount
Limit the download speed to amount bytes per second. Amount may be expressed in bytes, kilobytes with the k suffix, or megabytes with the m suffix. For example, --limit-rate=20k will limit the retrieval rate to 20KB/s. This is useful when, for whatever reason, you don't want Wget to consume the entire available bandwidth. 
This option allows the use of decimal numbers, usually in conjunction with power suffixes; for example, --limit-rate=2.5k is a legal value. 

Note that Wget implements the limiting by sleeping the appropriate amount of time after a network read that took less time than specified by the rate. Eventually this strategy causes the TCP transfer to slow down to approximately the specified rate. However, it may take some time for this balance to be achieved, so don't be surprised if limiting the rate doesn't work well with very small files. 




-w seconds
--wait=seconds
Wait the specified number of seconds between the retrievals. Use of this option is recommended, as it lightens the server load by making the requests less frequent. Instead of in seconds, the time can be specified in minutes using the m suffix, in hours using h suffix, or in days using d suffix. 
Specifying a large value for this option is useful if the network or the destination host is down, so that Wget can wait long enough to reasonably expect the network error to be fixed before the retry. 




--waitretry=seconds
If you don't want Wget to wait between every retrieval, but only between retries of failed downloads, you can use this option. Wget will use linear backoff, waiting 1 second after the first failure on a given file, then waiting 2 seconds after the second failure on that file, up to the maximum number of seconds you specify. Therefore, a value of 10 will actually make Wget wait up to (1 + 2 + ... + 10) = 55 seconds per file. 
Note that this option is turned on by default in the global wgetrc file. 




--random-wait
Some web sites may perform log analysis to identify retrieval programs such as Wget by looking for statistically significant similarities in the time between requests. This option causes the time between requests to vary between 0 and 2 * wait seconds, where wait was specified using the --wait option, in order to mask Wget's presence from such analysis. 
A recent article in a publication devoted to development on a popular consumer platform provided code to perform this analysis on the fly. Its author suggested blocking at the class C address level to ensure automated retrieval programs were blocked despite changing DHCP-supplied addresses. 

The --random-wait option was inspired by this ill-advised recommendation to block many unrelated users from a web site due to the actions of one. 


--no-proxy
Don't use proxies, even if the appropriate *_proxy environment variable is defined. 
For more information about the use of proxies with Wget, See Proxies. 




-Q quota
--quota=quota
Specify download quota for automatic retrievals. The value can be specified in bytes (default), kilobytes (with k suffix), or megabytes (with m suffix). 
Note that quota will never affect downloading a single file. So if you specify wget -Q10k ftp://wuarchive.wustl.edu/ls-lR.gz, all of the ls-lR.gz will be downloaded. The same goes even when several urls are specified on the command-line. However, quota is respected when retrieving either recursively, or from an input file. Thus you may safely type wget -Q2m -i sites download will be aborted when the quota is exceeded. 

Setting quota to 0 or to inf unlimits the download quota. 




--no-dns-cache
Turn off caching of DNS lookups. Normally, Wget remembers the IP addresses it looked up from DNS so it doesn't have to repeatedly contact the DNS server for the same (typically small) set of hosts it retrieves from. This cache exists in memory only; a new Wget run will contact DNS again. 
However, it has been reported that in some situations it is not desirable to cache host names, even for the duration of a short-running application like Wget. With this option Wget issues a new DNS lookup (more precisely, a new call to gethostbyname or getaddrinfo) each time it makes a new connection. Please note that this option will not affect caching that might be performed by the resolving library or by an external caching layer, such as NSCD. 

If you don't understand exactly what this option does, you probably won't need it. 




--restrict-file-names=mode
Change which characters found in remote URLs may show up in local file names generated from those URLs. Characters that are restricted by this option are escaped, i.e. replaced with %HH, where HH is the hexadecimal number that corresponds to the restricted character. 
By default, Wget escapes the characters that are not valid as part of file names on your operating system, as well as control characters that are typically unprintable. This option is useful for changing these defaults, either because you are downloading to a non-native partition, or because you want to disable escaping of the control characters. 

When mode is set to  unix , Wget escapes the character / and the control characters in the ranges 0 31 and 128 159. This is the default on Unix-like OS'es. 

When mode is set to  windows , Wget escapes the characters \, |, /, :, ?, ", *, <, >, and the control characters in the ranges 0 31 and 128 159. In addition to this, Wget in Windows mode uses + instead of : to separate host and port in local file names, and uses @ instead of ? to separate the query portion of the file name from the rest. Therefore, a URL that would be saved as www.xemacs.org:4300/search.pl?input=blah in Unix mode would be saved as www.xemacs.org+4300/search.pl@input=blah in Windows mode. This mode is the default on Windows. 

If you append ,nocontrol to the mode, as in unix,nocontrol, escaping of the control characters is also switched off. You can use --restrict-file-names=nocontrol to turn off escaping of control characters without affecting the choice of the OS to use as file name restriction mode. 


-4
--inet4-only
-6
--inet6-only
Force connecting to IPv4 or IPv6 addresses. With --inet4-only or -4, Wget will only connect to IPv4 hosts, ignoring AAAA records in DNS, and refusing to connect to IPv6 addresses specified in URLs. Conversely, with --inet6-only or -6, Wget will only connect to IPv6 hosts and ignore A records and IPv4 addresses. 
Neither options should be needed normally. By default, an IPv6-aware Wget will use the address family specified by the host's DNS record. If the DNS specifies both an A record and an AAAA record, Wget will try them in sequence until it finds one it can connect to. 

These options can be used to deliberately force the use of IPv4 or IPv6 address families on dual family systems, usually to aid debugging or to deal with broken network configuration. Only one of --inet6-only and --inet4-only may be specified in the same command. Neither option is available in Wget compiled without IPv6 support. 


--prefer-family=IPv4/IPv6/none
When given a choice of several addresses, connect to the addresses with specified address family first. IPv4 addresses are preferred by default. 
This avoids spurious errors and connect attempts when accessing hosts that resolve to both IPv6 and IPv4 addresses from IPv4 networks. For example, www.kame.net resolves to 2001:200:0:8002:203:47ff:fea5:3085 and to 203.178.141.194. When the preferred family is IPv4, the IPv4 address is used first; when the preferred family is IPv6, the IPv6 address is used first; if the specified value is none, the address order returned by DNS is used without change. 

Unlike -4 and -6, this option doesn't inhibit access to any address family, it only changes the order in which the addresses are accessed. Also note that the reordering performed by this option is stable it doesn't affect order of addresses of the same family. That is, the relative order of all IPv4 addresses and of all IPv6 addresses remains intact in all cases. 


--retry-connrefused
Consider  connection refused  a transient error and try again. Normally Wget gives up on a URL when it is unable to connect to the site because failure to connect is taken as a sign that the server is not running at all and that retries would not help. This option is for mirroring unreliable sites whose servers tend to disappear for short periods of time. 



--user=user
--password=password
Specify the username user and password password for both ftp and http file retrieval. These parameters can be overridden using the --ftp-user and --ftp-password options for ftp connections and the --http-user and --http-password options for http connections. 


--------------------------------------------------------------------------------
Next: HTTP Options, Previous: Download Options, Up: Invoking 

2.6 Directory Options
-nd
--no-directories
Do not create a hierarchy of directories when retrieving recursively. With this option turned on, all files will get saved to the current directory, without clobbering (if a name shows up more than once, the filenames will get extensions .n). 

-x
--force-directories
The opposite of -nd create a hierarchy of directories, even if one would not have been created otherwise. E.g. wget -x http://fly.srk.fer.hr/robots.txt will save the downloaded file to fly.srk.fer.hr/robots.txt. 

-nH
--no-host-directories
Disable generation of host-prefixed directories. By default, invoking Wget with -r http://fly.srk.fer.hr/ will create a structure of directories beginning with fly.srk.fer.hr/. This option disables such behavior. 

--protocol-directories
Use the protocol name as a directory component of local file names. For example, with this option, wget -r http://host will save to http/host/... rather than just to host/.... 



--cut-dirs=number
Ignore number directory components. This is useful for getting a fine-grained control over the directory where recursive retrieval will be saved. 
Take, for example, the directory at ftp://ftp.xemacs.org/pub/xemacs/. If you retrieve it with -r, it will be saved locally under ftp.xemacs.org/pub/xemacs/. While the -nH option can remove the ftp.xemacs.org/ part, you are still stuck with pub/xemacs. This is where --cut-dirs comes in handy; it makes Wget not  see  number remote directory components. Here are several examples of how --cut-dirs option works. 

          No options        -> ftp.xemacs.org/pub/xemacs/
          -nH               -> pub/xemacs/
          -nH --cut-dirs=1  -> xemacs/
          -nH --cut-dirs=2  -> .
          
          --cut-dirs=1      -> ftp.xemacs.org/xemacs/
          ...
     
If you just want to get rid of the directory structure, this option is similar to a combination of -nd and -P. However, unlike -nd, --cut-dirs does not lose with subdirectories for instance, with -nH --cut-dirs=1, a beta/ subdirectory will be placed to xemacs/beta, as one would expect. 




-P prefix
--directory-prefix=prefix
Set directory prefix to prefix. The directory prefix is the directory where all other files and subdirectories will be saved to, i.e. the top of the retrieval tree. The default is . (the current directory). 


--------------------------------------------------------------------------------
Next: HTTPS (SSL/TLS) Options, Previous: Directory Options, Up: Invoking 

2.7 HTTP Options
-E
--html-extension
If a file of type application/xhtml+xml or text/html is downloaded and the URL does not end with the regexp \.[Hh][Tt][Mm][Ll]?, this option will cause the suffix .html to be appended to the local filename. This is useful, for instance, when you're mirroring a remote site that uses .asp pages, but you want the mirrored pages to be viewable on your stock Apache server. Another good use for this is when you're downloading CGI-generated materials. A URL like http://site.com/article.cgi?25 will be saved as article.cgi?25.html. 
Note that filenames changed in this way will be re-downloaded every time you re-mirror a site, because Wget can't tell that the local X.html file corresponds to remote URL X (since it doesn't yet know that the URL produces output of type text/html or application/xhtml+xml. To prevent this re-downloading, you must use -k and -K so that the original version of the file will be saved as X.orig (see Recursive Retrieval Options). 




--http-user=user
--http-password=password
Specify the username user and password password on an http server. According to the type of the challenge, Wget will encode them using either the basic (insecure) or the digest authentication scheme. 
Another way to specify username and password is in the url itself (see URL Format). Either method reveals your password to anyone who bothers to run ps. To prevent the passwords from being seen, store them in .wgetrc or .netrc, and make sure to protect those files from other users with chmod. If the passwords are really important, do not leave them lying in those files either edit the files and delete them after Wget has started the download. 




--no-cache
Disable server-side cache. In this case, Wget will send the remote server an appropriate directive (Pragma: no-cache) to get the file from the remote service, rather than returning the cached version. This is especially useful for retrieving and flushing out-of-date documents on proxy servers. 
Caching is allowed by default. 




--no-cookies
Disable the use of cookies. Cookies are a mechanism for maintaining server-side state. The server sends the client a cookie using the Set-Cookie header, and the client responds with the same cookie upon further requests. Since cookies allow the server owners to keep track of visitors and for sites to exchange this information, some consider them a breach of privacy. The default is to use cookies; however, storing cookies is not on by default. 



--load-cookies file
Load cookies from file before the first HTTP retrieval. file is a textual file in the format originally used by Netscape's cookies.txt file. 
You will typically use this option when mirroring sites that require that you be logged in to access some or all of their content. The login process typically works by the web server issuing an http cookie upon receiving and verifying your credentials. The cookie is then resent by the browser when accessing that part of the site, and so proves your identity. 

Mirroring such a site requires Wget to send the same cookies your browser sends when communicating with the site. This is achieved by --load-cookies simply point Wget to the location of the cookies.txt file, and it will send the same cookies your browser would send in the same situation. Different browsers keep textual cookie files in different locations: 

Netscape 4.x.
The cookies are in ~/.netscape/cookies.txt. 

Mozilla and Netscape 6.x.
Mozilla's cookie file is also named cookies.txt, located somewhere under ~/.mozilla, in the directory of your profile. The full path usually ends up looking somewhat like ~/.mozilla/default/some-weird-string/cookies.txt. 

Internet Explorer.
You can produce a cookie file Wget can use by using the File menu, Import and Export, Export Cookies. This has been tested with Internet Explorer 5; it is not guaranteed to work with earlier versions. 

Other browsers.
If you are using a different browser to create your cookies, --load-cookies will only work if you can locate or produce a cookie file in the Netscape format that Wget expects. 
If you cannot use --load-cookies, there might still be an alternative. If your browser supports a  cookie manager , you can use it to view the cookies used when accessing the site you're mirroring. Write down the name and value of the cookie, and manually instruct Wget to send those cookies, bypassing the  official  cookie support: 

          wget --no-cookies --header "Cookie: name=value"
     



--save-cookies file
Save cookies to file before exiting. This will not save cookies that have expired or that have no expiry time (so-called  session cookies ), but also see --keep-session-cookies. 



--keep-session-cookies
When specified, causes --save-cookies to also save session cookies. Session cookies are normally not saved because they are meant to be kept in memory and forgotten when you exit the browser. Saving them is useful on sites that require you to log in or to visit the home page before you can access some pages. With this option, multiple Wget runs are considered a single browser session as far as the site is concerned. 
Since the cookie file format does not normally carry session cookies, Wget marks them with an expiry timestamp of 0. Wget's --load-cookies recognizes those as session cookies, but it might confuse other browsers. Also note that cookies so loaded will be treated as other session cookies, which means that if you want --save-cookies to preserve them again, you must use --keep-session-cookies again. 




--ignore-length
Unfortunately, some http servers (cgi programs, to be more precise) send out bogus Content-Length headers, which makes Wget go wild, as it thinks not all the document was retrieved. You can spot this syndrome if Wget retries getting the same document again and again, each time claiming that the (otherwise normal) connection has closed on the very same byte. 
With this option, Wget will ignore the Content-Length header as if it never existed. 




--header=header-line
Send header-line along with the rest of the headers in each http request. The supplied header is sent as-is, which means it must contain name and value separated by colon, and must not contain newlines. 
You may define more than one additional header by specifying --header more than once. 

          wget --header='Accept-Charset: iso-8859-2' \
               --header='Accept-Language: hr'        \
                 http://fly.srk.fer.hr/
     
Specification of an empty string as the header value will clear all previous user-defined headers. 

As of Wget 1.10, this option can be used to override headers otherwise generated automatically. This example instructs Wget to connect to localhost, but to specify foo.bar in the Host header: 

          wget --header="Host: foo.bar" http://localhost/
     
In versions of Wget prior to 1.10 such use of --header caused sending of duplicate headers. 




--proxy-user=user
--proxy-password=password
Specify the username user and password password for authentication on a proxy server. Wget will encode them using the basic authentication scheme. 
Security considerations similar to those with --http-password pertain here as well. 




--referer=url
Include `Referer: url' header in HTTP request. Useful for retrieving documents with server-side processing that assume they are always being retrieved by interactive web browsers and only come out properly when Referer is set to one of the pages that point to them. 



--save-headers
Save the headers sent by the http server to the file, preceding the actual contents, with an empty line as the separator. 



-U agent-string
--user-agent=agent-string
Identify as agent-string to the http server. 
The http protocol allows the clients to identify themselves using a User-Agent header field. This enables distinguishing the www software, usually for statistical purposes or for tracing of protocol violations. Wget normally identifies as Wget/version, version being the current version number of Wget. 

However, some sites have been known to impose the policy of tailoring the output according to the User-Agent-supplied information. While this is not such a bad idea in theory, it has been abused by servers denying information to clients other than (historically) Netscape or, more frequently, Microsoft Internet Explorer. This option allows you to change the User-Agent line issued by Wget. Use of this option is discouraged, unless you really know what you are doing. 

Specifying empty user agent with --user-agent="" instructs Wget not to send the User-Agent header in http requests. 




--post-data=string
--post-file=file
Use POST as the method for all HTTP requests and send the specified data in the request body. --post-data sends string as data, whereas --post-file sends the contents of file. Other than that, they work in exactly the same way. 
Please be aware that Wget needs to know the size of the POST data in advance. Therefore the argument to --post-file must be a regular file; specifying a FIFO or something like /dev/stdin won't work. It's not quite clear how to work around this limitation inherent in HTTP/1.0. Although HTTP/1.1 introduces chunked transfer that doesn't require knowing the request length in advance, a client can't use chunked unless it knows it's talking to an HTTP/1.1 server. And it can't know that until it receives a response, which in turn requires the request to have been completed   a chicken-and-egg problem. 

Note: if Wget is redirected after the POST request is completed, it will not send the POST data to the redirected URL. This is because URLs that process POST often respond with a redirection to a regular page, which does not desire or accept POST. It is not completely clear that this behavior is optimal; if it doesn't work out, it might be changed in the future. 

This example shows how to log to a server using POST and then proceed to download the desired pages, presumably only accessible to authorized users: 

          # Log in to the server.  This can be done only once.
          wget --save-cookies cookies.txt \
               --post-data 'user=foo&password=bar' \
               http://server.com/auth.php
          
          # Now grab the page or pages we care about.
          wget --load-cookies cookies.txt \
               -p http://server.com/interesting/article.php
     
If the server is using session cookies to track user authentication, the above will not work because --save-cookies will not save them (and neither will browsers) and the cookies.txt file will be empty. In that case use --keep-session-cookies along with --save-cookies to force saving of session cookies. 



--------------------------------------------------------------------------------
Next: FTP Options, Previous: HTTP Options, Up: Invoking 

2.8 HTTPS (SSL/TLS) Options
To support encrypted HTTP (HTTPS) downloads, Wget must be compiled with an external SSL library, currently OpenSSL. If Wget is compiled without SSL support, none of these options are available. 

--secure-protocol=protocol
Choose the secure protocol to be used. Legal values are auto, SSLv2, SSLv3, and TLSv1. If auto is used, the SSL library is given the liberty of choosing the appropriate protocol automatically, which is achieved by sending an SSLv2 greeting and announcing support for SSLv3 and TLSv1. This is the default. 
Specifying SSLv2, SSLv3, or TLSv1 forces the use of the corresponding protocol. This is useful when talking to old and buggy SSL server implementations that make it hard for OpenSSL to choose the correct protocol version. Fortunately, such servers are quite rare. 




--no-check-certificate
Don't check the server certificate against the available certificate authorities. Also don't require the URL host name to match the common name presented by the certificate. 
As of Wget 1.10, the default is to verify the server's certificate against the recognized certificate authorities, breaking the SSL handshake and aborting the download if the verification fails. Although this provides more secure downloads, it does break interoperability with some sites that worked with previous Wget versions, particularly those using self-signed, expired, or otherwise invalid certificates. This option forces an  insecure  mode of operation that turns the certificate verification errors into warnings and allows you to proceed. 

If you encounter  certificate verification  errors or ones saying that  common name doesn't match requested host name , you can use this option to bypass the verification and proceed with the download. Only use this option if you are otherwise convinced of the site's authenticity, or if you really don't care about the validity of its certificate. It is almost always a bad idea not to check the certificates when transmitting confidential or important data. 




--certificate=file
Use the client certificate stored in file. This is needed for servers that are configured to require certificates from the clients that connect to them. Normally a certificate is not required and this switch is optional. 



--certificate-type=type
Specify the type of the client certificate. Legal values are PEM (assumed by default) and DER, also known as ASN1. 

--private-key=file
Read the private key from file. This allows you to provide the private key in a file separate from the certificate. 

--private-key-type=type
Specify the type of the private key. Accepted values are PEM (the default) and DER. 

--ca-certificate=file
Use file as the file with the bundle of certificate authorities ( CA ) to verify the peers. The certificates must be in PEM format. 
Without this option Wget looks for CA certificates at the system-specified locations, chosen at OpenSSL installation time. 




--ca-directory=directory
Specifies directory containing CA certificates in PEM format. Each file contains one CA certificate, and the file name is based on a hash value derived from the certificate. This is achieved by processing a certificate directory with the c_rehash utility supplied with OpenSSL. Using --ca-directory is more efficient than --ca-certificate when many certificates are installed because it allows Wget to fetch certificates on demand. 
Without this option Wget looks for CA certificates at the system-specified locations, chosen at OpenSSL installation time. 




--random-file=file
Use file as the source of random data for seeding the pseudo-random number generator on systems without /dev/random. 
On such systems the SSL library needs an external source of randomness to initialize. Randomness may be provided by EGD (see --egd-file below) or read from an external source specified by the user. If this option is not specified, Wget looks for random data in $RANDFILE or, if that is unset, in $HOME/.rnd. If none of those are available, it is likely that SSL encryption will not be usable. 

If you're getting the  Could not seed OpenSSL PRNG; disabling SSL.  error, you should provide random data using some of the methods described above. 




--egd-file=file
Use file as the EGD socket. EGD stands for Entropy Gathering Daemon, a user-space program that collects data from various unpredictable system sources and makes it available to other programs that might need it. Encryption software, such as the SSL library, needs sources of non-repeating randomness to seed the random number generator used to produce cryptographically strong keys. 
OpenSSL allows the user to specify his own source of entropy using the RAND_FILE environment variable. If this variable is unset, or if the specified file does not produce enough randomness, OpenSSL will read random data from EGD socket specified using this option. 

If this option is not specified (and the equivalent startup command is not used), EGD is never contacted. EGD is not needed on modern Unix systems that support /dev/random. 



--------------------------------------------------------------------------------
Next: Recursive Retrieval Options, Previous: HTTPS (SSL/TLS) Options, Up: Invoking 

2.9 FTP Options
--ftp-user=user
--ftp-password=password
Specify the username user and password password on an ftp server. Without this, or the corresponding startup option, the password defaults to -wget@, normally used for anonymous FTP. 
Another way to specify username and password is in the url itself (see URL Format). Either method reveals your password to anyone who bothers to run ps. To prevent the passwords from being seen, store them in .wgetrc or .netrc, and make sure to protect those files from other users with chmod. If the passwords are really important, do not leave them lying in those files either edit the files and delete them after Wget has started the download. 




--no-remove-listing
Don't remove the temporary .listing files generated by ftp retrievals. Normally, these files contain the raw directory listings received from ftp servers. Not removing them can be useful for debugging purposes, or when you want to be able to easily check on the contents of remote server directories (e.g. to verify that a mirror you're running is complete). 
Note that even though Wget writes to a known filename for this file, this is not a security hole in the scenario of a user making .listing a symbolic link to /etc/passwd or something and asking root to run Wget in his or her directory. Depending on the options used, either Wget will refuse to write to .listing, making the globbing/recursion/time-stamping operation fail, or the symbolic link will be deleted and replaced with the actual .listing file, or the listing will be written to a .listing.number file. 

Even though this situation isn't a problem, though, root should never run Wget in a non-trusted user's directory. A user could do something as simple as linking index.html to /etc/passwd and asking root to run Wget with -N or -r so the file will be overwritten. 




--no-glob
Turn off ftp globbing. Globbing refers to the use of shell-like special characters (wildcards), like *, ?, [ and ] to retrieve more than one file from the same directory at once, like: 
          wget ftp://gnjilux.srk.fer.hr/*.msg
     
By default, globbing will be turned on if the url contains a globbing character. This option may be used to turn globbing on or off permanently. 

You may have to quote the url to protect it from being expanded by your shell. Globbing makes Wget look for a directory listing, which is system-specific. This is why it currently works only with Unix ftp servers (and the ones emulating Unix ls output). 




--no-passive-ftp
Disable the use of the passive FTP transfer mode. Passive FTP mandates that the client connect to the server to establish the data connection rather than the other way around. 
If the machine is connected to the Internet directly, both passive and active FTP should work equally well. Behind most firewall and NAT configurations passive FTP has a better chance of working. However, in some rare firewall configurations, active FTP actually works when passive FTP doesn't. If you suspect this to be the case, use this option, or set passive_ftp=off in your init file. 




--retr-symlinks
Usually, when retrieving ftp directories recursively and a symbolic link is encountered, the linked-to file is not downloaded. Instead, a matching symbolic link is created on the local filesystem. The pointed-to file will not be downloaded unless this recursive retrieval would have encountered it separately and downloaded it anyway. 
When --retr-symlinks is specified, however, symbolic links are traversed and the pointed-to files are retrieved. At this time, this option does not cause Wget to traverse symlinks to directories and recurse through them, but in the future it should be enhanced to do this. 

Note that when retrieving a file (not a directory) because it was specified on the command-line, rather than because it was recursed to, this option has no effect. Symbolic links are always traversed in this case. 




--no-http-keep-alive
Turn off the  keep-alive  feature for HTTP downloads. Normally, Wget asks the server to keep the connection open so that, when you download more than one document from the same server, they get transferred over the same TCP connection. This saves time and at the same time reduces the load on the server. 
This option is useful when, for some reason, persistent (keep-alive) connections don't work for you, for example due to a server bug or due to the inability of server-side scripts to cope with the connections. 



--------------------------------------------------------------------------------
Next: Recursive Accept/Reject Options, Previous: FTP Options, Up: Invoking 

2.10 Recursive Retrieval Options
-r
--recursive
Turn on recursive retrieving. See Recursive Download, for more details. 

-l depth
--level=depth
Specify recursion maximum depth level depth (see Recursive Download). The default maximum depth is 5. 



--delete-after
This option tells Wget to delete every single file it downloads, after having done so. It is useful for pre-fetching popular pages through a proxy, e.g.: 
          wget -r -nd --delete-after http://whatever.com/~popular/page/
     
The -r option is to retrieve recursively, and -nd to not create directories. 

Note that --delete-after deletes files on the local machine. It does not issue the DELE command to remote FTP sites, for instance. Also note that when --delete-after is specified, --convert-links is ignored, so .orig files are simply not created in the first place. 




-k
--convert-links
After the download is complete, convert the links in the document to make them suitable for local viewing. This affects not only the visible hyperlinks, but any part of the document that links to external content, such as embedded images, links to style sheets, hyperlinks to non-html content, etc. 
Each link will be changed in one of the two ways: 

The links to files that have been downloaded by Wget will be changed to refer to the file they point to as a relative link. 
Example: if the downloaded file /foo/doc.html links to /bar/img.gif, also downloaded, then the link in doc.html will be modified to point to ../bar/img.gif. This kind of transformation works reliably for arbitrary combinations of directories. 

The links to files that have not been downloaded by Wget will be changed to include host name and absolute path of the location they point to. 
Example: if the downloaded file /foo/doc.html links to /bar/img.gif (or to ../bar/img.gif), then the link in doc.html will be modified to point to http://hostname/bar/img.gif. 

Because of this, local browsing works reliably: if a linked file was downloaded, the link will refer to its local name; if it was not downloaded, the link will refer to its full Internet address rather than presenting a broken link. The fact that the former links are converted to relative links ensures that you can move the downloaded hierarchy to another directory. 

Note that only at the end of the download can Wget know which links have been downloaded. Because of that, the work done by -k will be performed at the end of all the downloads. 




-K
--backup-converted
When converting a file, back up the original version with a .orig suffix. Affects the behavior of -N (see HTTP Time-Stamping Internals). 

-m
--mirror
Turn on options suitable for mirroring. This option turns on recursion and time-stamping, sets infinite recursion depth and keeps ftp directory listings. It is currently equivalent to -r -N -l inf --no-remove-listing. 



-p
--page-requisites
This option causes Wget to download all the files that are necessary to properly display a given html page. This includes such things as inlined images, sounds, and referenced stylesheets. 
Ordinarily, when downloading a single html page, any requisite documents that may be needed to display it properly are not downloaded. Using -r together with -l can help, but since Wget does not ordinarily distinguish between external and inlined documents, one is generally left with  leaf documents  that are missing their requisites. 

For instance, say document 1.html contains an <IMG> tag referencing 1.gif and an <A> tag pointing to external document 2.html. Say that 2.html is similar but that its image is 2.gif and it links to 3.html. Say this continues up to some arbitrarily high number. 

If one executes the command: 

          wget -r -l 2 http://site/1.html
     
then 1.html, 1.gif, 2.html, 2.gif, and 3.html will be downloaded. As you can see, 3.html is without its requisite 3.gif because Wget is simply counting the number of hops (up to 2) away from 1.html in order to determine where to stop the recursion. However, with this command: 

          wget -r -l 2 -p http://site/1.html
     
all the above files and 3.html's requisite 3.gif will be downloaded. Similarly, 

          wget -r -l 1 -p http://site/1.html
     
will cause 1.html, 1.gif, 2.html, and 2.gif to be downloaded. One might think that: 

          wget -r -l 0 -p http://site/1.html
     
would download just 1.html and 1.gif, but unfortunately this is not the case, because -l 0 is equivalent to -l inf that is, infinite recursion. To download a single html page (or a handful of them, all specified on the command-line or in a -i url input file) and its (or their) requisites, simply leave off -r and -l: 

          wget -p http://site/1.html
     
Note that Wget will behave as if -r had been specified, but only that single page and its requisites will be downloaded. Links from that page to external documents will not be followed. Actually, to download a single page and all its requisites (even if they exist on separate websites), and make sure the lot displays properly locally, this author likes to use a few options in addition to -p: 

          wget -E -H -k -K -p http://site/document
     
To finish off this topic, it's worth knowing that Wget's idea of an external document link is any URL specified in an <A> tag, an <AREA> tag, or a <LINK> tag other than <LINK REL="stylesheet">. 




--strict-comments
Turn on strict parsing of html comments. The default is to terminate comments at the first occurrence of -->. 
According to specifications, html comments are expressed as sgml declarations. Declaration is special markup that begins with <! and ends with >, such as <!DOCTYPE ...>, that may contain comments between a pair of -- delimiters. html comments are  empty declarations , sgml declarations without any non-comment text. Therefore, <!--foo--> is a valid comment, and so is <!--one-- --two-->, but <!--1--2--> is not. 

On the other hand, most html writers don't perceive comments as anything other than text delimited with <!-- and -->, which is not quite the same. For example, something like <!------------> works as a valid comment as long as the number of dashes is a multiple of four (!). If not, the comment technically lasts until the next --, which may be at the other end of the document. Because of this, many popular browsers completely ignore the specification and implement what users have come to expect: comments delimited with <!-- and -->. 

Until version 1.9, Wget interpreted comments strictly, which resulted in missing links in many web pages that displayed fine in browsers, but had the misfortune of containing non-compliant comments. Beginning with version 1.9, Wget has joined the ranks of clients that implements  naive  comments, terminating each comment at the first occurrence of -->. 

If, for whatever reason, you want strict comment parsing, use this option to turn it on. 



--------------------------------------------------------------------------------
Previous: Recursive Retrieval Options, Up: Invoking 

2.11 Recursive Accept/Reject Options
-A acclist --accept acclist
-R rejlist --reject rejlist
Specify comma-separated lists of file name suffixes or patterns to accept or reject (see Types of Files for more details). 

-D domain-list
--domains=domain-list
Set domains to be followed. domain-list is a comma-separated list of domains. Note that it does not turn on -H. 

--exclude-domains domain-list
Specify the domains that are not to be followed. (see Spanning Hosts). 



--follow-ftp
Follow ftp links from html documents. Without this option, Wget will ignore all the ftp links. 



--follow-tags=list
Wget has an internal table of html tag / attribute pairs that it considers when looking for linked documents during a recursive retrieval. If a user wants only a subset of those tags to be considered, however, he or she should be specify such tags in a comma-separated list with this option. 

--ignore-tags=list
This is the opposite of the --follow-tags option. To skip certain html tags when recursively looking for documents to download, specify them in a comma-separated list. 
In the past, this option was the best bet for downloading a single page and its requisites, using a command-line like: 

          wget --ignore-tags=a,area -H -k -K -r http://site/document
     
However, the author of this option came across a page with tags like <LINK REL="home" HREF="/"> and came to the realization that specifying tags to ignore was not enough. One can't just tell Wget to ignore <LINK>, because then stylesheets will not be downloaded. Now the best bet for downloading a single page and its requisites is the dedicated --page-requisites option. 


-H
--span-hosts
Enable spanning across hosts when doing recursive retrieving (see Spanning Hosts). 

-L
--relative
Follow relative links only. Useful for retrieving a specific home page without any distractions, not even those from the same hosts (see Relative Links). 

-I list
--include-directories=list
Specify a comma-separated list of directories you wish to follow when downloading (see Directory-Based Limits for more details.) Elements of list may contain wildcards. 

-X list
--exclude-directories=list
Specify a comma-separated list of directories you wish to exclude from download (see Directory-Based Limits for more details.) Elements of list may contain wildcards. 

-np

--no-parent
Do not ever ascend to the parent directory when retrieving recursively. This is a useful option, since it guarantees that only the files below a certain hierarchy will be downloaded. See Directory-Based Limits, for more details. 


--------------------------------------------------------------------------------
Next: Following Links, Previous: Invoking, Up: Top 

3 Recursive Download
GNU Wget is capable of traversing parts of the Web (or a single http or ftp server), following links and directory structure. We refer to this as to recursive retrieval, or recursion. 

With http urls, Wget retrieves and parses the html from the given url, documents, retrieving the files the html document was referring to, through markup like href, or src. If the freshly downloaded file is also of type text/html or application/xhtml+xml, it will be parsed and followed further. 

Recursive retrieval of http and html content is breadth-first. This means that Wget first downloads the requested html document, then the documents linked from that document, then the documents linked by them, and so on. In other words, Wget first downloads the documents at depth 1, then those at depth 2, and so on until the specified maximum depth. 

The maximum depth to which the retrieval may descend is specified with the -l option. The default maximum depth is five layers. 

When retrieving an ftp url recursively, Wget will retrieve all the data from the given directory tree (including the subdirectories up to the specified depth) on the remote server, creating its mirror image locally. ftp retrieval is also limited by the depth parameter. Unlike http recursion, ftp recursion is performed depth-first. 

By default, Wget will create a local directory tree, corresponding to the one found on the remote server. 

Recursive retrieving can find a number of applications, the most important of which is mirroring. It is also useful for www presentations, and any other opportunities where slow network connections should be bypassed by storing the files locally. 

You should be warned that recursive downloads can overload the remote servers. Because of that, many administrators frown upon them and may ban access from your site if they detect very fast downloads of big amounts of content. When downloading from Internet servers, consider using the -w option to introduce a delay between accesses to the server. The download will take a while longer, but the server administrator will not be alarmed by your rudeness. 

Of course, recursive download may cause problems on your machine. If left to run unchecked, it can easily fill up the disk. If downloading from local network, it can also take bandwidth on the system, as well as consume memory and CPU. 

Try to specify the criteria that match the kind of download you are trying to achieve. If you want to download only one page, use --page-requisites without any additional recursion. If you want to download things under one directory, use -np to avoid downloading things from other directories. If you want to download all the files from one directory, use -l 1 to make sure the recursion depth never exceeds one. See Following Links, for more information about this. 

Recursive retrieval should be used with care. Don't say you were not warned. 



--------------------------------------------------------------------------------
Next: Time-Stamping, Previous: Recursive Download, Up: Top 

4 Following Links
When retrieving recursively, one does not wish to retrieve loads of unnecessary data. Most of the time the users bear in mind exactly what they want to download, and want Wget to follow only specific links. 

For example, if you wish to download the music archive from fly.srk.fer.hr, you will not want to download all the home pages that happen to be referenced by an obscure part of the archive. 

Wget possesses several mechanisms that allows you to fine-tune which links it will follow. 

Spanning Hosts: (Un)limiting retrieval based on host name. 
Types of Files: Getting only certain files. 
Directory-Based Limits: Getting only certain directories. 
Relative Links: Follow relative links only. 
FTP Links: Following FTP links. 


--------------------------------------------------------------------------------
Next: Types of Files, Up: Following Links 

4.1 Spanning Hosts
Wget's recursive retrieval normally refuses to visit hosts different than the one you specified on the command line. This is a reasonable default; without it, every retrieval would have the potential to turn your Wget into a small version of google. 

However, visiting different hosts, or host spanning, is sometimes a useful option. Maybe the images are served from a different server. Maybe you're mirroring a site that consists of pages interlinked between three servers. Maybe the server has two equivalent names, and the html pages refer to both interchangeably. 

Span to any host -H
The -H option turns on host spanning, thus allowing Wget's recursive run to visit any host referenced by a link. Unless sufficient recursion-limiting criteria are applied depth, these foreign hosts will typically link to yet more hosts, and so on until Wget ends up sucking up much more data than you have intended. 

Limit spanning to certain domains -D
The -D option allows you to specify the domains that will be followed, thus limiting the recursion only to the hosts that belong to these domains. Obviously, this makes sense only in conjunction with -H. A typical example would be downloading the contents of www.server.com, but allowing downloads from images.server.com, etc.: 
          wget -rH -Dserver.com http://www.server.com/
     
You can specify more than one address by separating them with a comma, e.g. -Ddomain1.com,domain2.com. 


Keep download off certain domains --exclude-domains
If there are domains you want to exclude specifically, you can do it with --exclude-domains, which accepts the same type of arguments of -D, but will exclude all the listed domains. For example, if you want to download all the hosts from foo.edu domain, with the exception of sunsite.foo.edu, you can do it like this: 
          wget -rH -Dfoo.edu --exclude-domains sunsite.foo.edu \
              http://www.foo.edu/
     


--------------------------------------------------------------------------------
Next: Directory-Based Limits, Previous: Spanning Hosts, Up: Following Links 

4.2 Types of Files
When downloading material from the web, you will often want to restrict the retrieval to only certain file types. For example, if you are interested in downloading gifs, you will not be overjoyed to get loads of PostScript documents, and vice versa. 

Wget offers two options to deal with this problem. Each option description lists a short name, a long name, and the equivalent command in .wgetrc. 


-A acclist
--accept acclist
accept = acclist
The argument to --accept option is a list of file suffixes or patterns that Wget will download during recursive retrieval. A suffix is the ending part of a file, and consists of  normal  letters, e.g. gif or .jpg. A matching pattern contains shell-like wildcards, e.g. books* or zelazny*196[0-9]*. 
So, specifying wget -A gif,jpg will make Wget download only the files ending with gif or jpg, i.e. gifs and jpegs. On the other hand, wget -A "zelazny*196[0-9]*" will download only files beginning with zelazny and containing numbers from 1960 to 1969 anywhere within. Look up the manual of your shell for a description of how pattern matching works. 

Of course, any number of suffixes and patterns can be combined into a comma-separated list, and given as an argument to -A. 




-R rejlist
--reject rejlist
reject = rejlist
The --reject option works the same way as --accept, only its logic is the reverse; Wget will download all files except the ones matching the suffixes (or patterns) in the list. 
So, if you want to download a whole page except for the cumbersome mpegs and .au files, you can use wget -R mpg,mpeg,au. Analogously, to download all files except the ones beginning with bjork, use wget -R "bjork*". The quotes are to prevent expansion by the shell. 

The -A and -R options may be combined to achieve even better fine-tuning of which files to retrieve. E.g. wget -A "*zelazny*" -R .ps will download all the files having zelazny as a part of their name, but not the PostScript files. 

Note that these two options do not affect the downloading of html files; Wget must load all the htmls to know where to go at all recursive retrieval would make no sense otherwise. 



--------------------------------------------------------------------------------
Next: Relative Links, Previous: Types of Files, Up: Following Links 

4.3 Directory-Based Limits
Regardless of other link-following facilities, it is often useful to place the restriction of what files to retrieve based on the directories those files are placed in. There can be many reasons for this the home pages may be organized in a reasonable directory structure; or some directories may contain useless information, e.g. /cgi-bin or /dev directories. 

Wget offers three different options to deal with this requirement. Each option description lists a short name, a long name, and the equivalent command in .wgetrc. 


-I list
--include list
include_directories = list
-I option accepts a comma-separated list of directories included in the retrieval. Any other directories will simply be ignored. The directories are absolute paths. 
So, if you wish to download from http://host/people/bozo/ following only links to bozo's colleagues in the /people directory and the bogus scripts in /cgi-bin, you can specify: 

          wget -I /people,/cgi-bin http://host/people/bozo/
     



-X list
--exclude list
exclude_directories = list
-X option is exactly the reverse of -I this is a list of directories excluded from the download. E.g. if you do not want Wget to download things from /cgi-bin directory, specify -X /cgi-bin on the command line. 
The same as with -A/-R, these two options can be combined to get a better fine-tuning of downloading subdirectories. E.g. if you want to load all the files from /pub hierarchy except for /pub/worthless, specify -I/pub -X/pub/worthless. 




-np
--no-parent
no_parent = on
The simplest, and often very useful way of limiting directories is disallowing retrieval of the links that refer to the hierarchy above than the beginning directory, i.e. disallowing ascent to the parent directory/directories. 
The --no-parent option (short -np) is useful in this case. Using it guarantees that you will never leave the existing hierarchy. Supposing you issue Wget with: 

          wget -r --no-parent http://somehost/~luzer/my-archive/
     
You may rest assured that none of the references to /~his-girls-homepage/ or /~luzer/all-my-mpegs/ will be followed. Only the archive you are interested in will be downloaded. Essentially, --no-parent is similar to -I/~luzer/my-archive, only it handles redirections in a more intelligent fashion. 



--------------------------------------------------------------------------------
Next: FTP Links, Previous: Directory-Based Limits, Up: Following Links 

4.4 Relative Links
When -L is turned on, only the relative links are ever followed. Relative links are here defined those that do not refer to the web server root. For example, these links are relative: 

     <a href="foo.gif">
     <a href="foo/bar.gif">
     <a href="../foo/bar.gif">

These links are not relative: 

     <a href="/foo.gif">
     <a href="/foo/bar.gif">
     <a href="http://www.server.com/foo/bar.gif">

Using this option guarantees that recursive retrieval will not span hosts, even without -H. In simple cases it also allows downloads to  just work  without having to convert links. 

This option is probably not very useful and might be removed in a future release. 



--------------------------------------------------------------------------------
Previous: Relative Links, Up: Following Links 

4.5 Following FTP Links
The rules for ftp are somewhat specific, as it is necessary for them to be. ftp links in html documents are often included for purposes of reference, and it is often inconvenient to download them by default. 

To have ftp links followed from html documents, you need to specify the --follow-ftp option. Having done that, ftp links will span hosts regardless of -H setting. This is logical, as ftp links rarely point to the same host where the http server resides. For similar reasons, the -L options has no effect on such downloads. On the other hand, domain acceptance (-D) and suffix rules (-A and -R) apply normally. 

Also note that followed links to ftp directories will not be retrieved recursively further. 



--------------------------------------------------------------------------------
Next: Startup File, Previous: Following Links, Up: Top 

5 Time-Stamping
One of the most important aspects of mirroring information from the Internet is updating your archives. 

Downloading the whole archive again and again, just to replace a few changed files is expensive, both in terms of wasted bandwidth and money, and the time to do the update. This is why all the mirroring tools offer the option of incremental updating. 

Such an updating mechanism means that the remote server is scanned in search of new files. Only those new files will be downloaded in the place of the old ones. 

A file is considered new if one of these two conditions are met: 

A file of that name does not already exist locally. 
A file of that name does exist, but the remote file was modified more recently than the local file. 
To implement this, the program needs to be aware of the time of last modification of both local and remote files. We call this information the time-stamp of a file. 

The time-stamping in GNU Wget is turned on using --timestamping (-N) option, or through timestamping = on directive in .wgetrc. With this option, for each file it intends to download, Wget will check whether a local file of the same name exists. If it does, and the remote file is older, Wget will not download it. 

If the local file does not exist, or the sizes of the files do not match, Wget will download the remote file no matter what the time-stamps say. 

Time-Stamping Usage 
HTTP Time-Stamping Internals 
FTP Time-Stamping Internals 


--------------------------------------------------------------------------------
Next: HTTP Time-Stamping Internals, Up: Time-Stamping 

5.1 Time-Stamping Usage
The usage of time-stamping is simple. Say you would like to download a file so that it keeps its date of modification. 

     wget -S http://www.gnu.ai.mit.edu/

A simple ls -l shows that the time stamp on the local file equals the state of the Last-Modified header, as returned by the server. As you can see, the time-stamping info is preserved locally, even without -N (at least for http). 

Several days later, you would like Wget to check if the remote file has changed, and download it if it has. 

     wget -N http://www.gnu.ai.mit.edu/

Wget will ask the server for the last-modified date. If the local file has the same timestamp as the server, or a newer one, the remote file will not be re-fetched. However, if the remote file is more recent, Wget will proceed to fetch it. 

The same goes for ftp. For example: 

     wget "ftp://ftp.ifi.uio.no/pub/emacs/gnus/*"

(The quotes around that URL are to prevent the shell from trying to interpret the *.) 

After download, a local directory listing will show that the timestamps match those on the remote server. Reissuing the command with -N will make Wget re-fetch only the files that have been modified since the last download. 

If you wished to mirror the GNU archive every week, you would use a command like the following, weekly: 

     wget --timestamping -r ftp://ftp.gnu.org/pub/gnu/

Note that time-stamping will only work for files for which the server gives a timestamp. For http, this depends on getting a Last-Modified header. For ftp, this depends on getting a directory listing with dates in a format that Wget can parse (see FTP Time-Stamping Internals). 



--------------------------------------------------------------------------------
Next: FTP Time-Stamping Internals, Previous: Time-Stamping Usage, Up: Time-Stamping 

5.2 HTTP Time-Stamping Internals
Time-stamping in http is implemented by checking of the Last-Modified header. If you wish to retrieve the file foo.html through http, Wget will check whether foo.html exists locally. If it doesn't, foo.html will be retrieved unconditionally. 

If the file does exist locally, Wget will first check its local time-stamp (similar to the way ls -l checks it), and then send a HEAD request to the remote server, demanding the information on the remote file. 

The Last-Modified header is examined to find which file was modified more recently (which makes it  newer ). If the remote file is newer, it will be downloaded; if it is older, Wget will give up.2 

When --backup-converted (-K) is specified in conjunction with -N, server file X is compared to local file X.orig, if extant, rather than being compared to local file X, which will always differ if it's been converted by --convert-links (-k). 

Arguably, http time-stamping should be implemented using the If-Modified-Since request. 



--------------------------------------------------------------------------------
Previous: HTTP Time-Stamping Internals, Up: Time-Stamping 

5.3 FTP Time-Stamping Internals
In theory, ftp time-stamping works much the same as http, only ftp has no headers time-stamps must be ferreted out of directory listings. 

If an ftp download is recursive or uses globbing, Wget will use the ftp LIST command to get a file listing for the directory containing the desired file(s). It will try to analyze the listing, treating it like Unix ls -l output, extracting the time-stamps. The rest is exactly the same as for http. Note that when retrieving individual files from an ftp server without using globbing or recursion, listing files will not be downloaded (and thus files will not be time-stamped) unless -N is specified. 

Assumption that every directory listing is a Unix-style listing may sound extremely constraining, but in practice it is not, as many non-Unix ftp servers use the Unixoid listing format because most (all?) of the clients understand it. Bear in mind that rfc959 defines no standard way to get a file list, let alone the time-stamps. We can only hope that a future standard will define this. 

Another non-standard solution includes the use of MDTM command that is supported by some ftp servers (including the popular wu-ftpd), which returns the exact time of the specified file. Wget may support this command in the future. 



--------------------------------------------------------------------------------
Next: Examples, Previous: Time-Stamping, Up: Top 

6 Startup File
Once you know how to change default settings of Wget through command line arguments, you may wish to make some of those settings permanent. You can do that in a convenient way by creating the Wget startup file .wgetrc. 

Besides .wgetrc is the  main  initialization file, it is convenient to have a special facility for storing passwords. Thus Wget reads and interprets the contents of $HOME/.netrc, if it finds it. You can find .netrc format in your system manuals. 

Wget reads .wgetrc upon startup, recognizing a limited set of commands. 

Wgetrc Location: Location of various wgetrc files. 
Wgetrc Syntax: Syntax of wgetrc. 
Wgetrc Commands: List of available commands. 
Sample Wgetrc: A wgetrc example. 


--------------------------------------------------------------------------------
Next: Wgetrc Syntax, Up: Startup File 

6.1 Wgetrc Location
When initializing, Wget will look for a global startup file, /usr/local/etc/wgetrc by default (or some prefix other than /usr/local, if Wget was not installed there) and read commands from there, if it exists. 

Then it will look for the user's file. If the environmental variable WGETRC is set, Wget will try to load that file. Failing that, no further attempts will be made. 

If WGETRC is not set, Wget will try to load $HOME/.wgetrc. 

The fact that user's settings are loaded after the system-wide ones means that in case of collision user's wgetrc overrides the system-wide wgetrc (in /usr/local/etc/wgetrc by default). Fascist admins, away! 



--------------------------------------------------------------------------------
Next: Wgetrc Commands, Previous: Wgetrc Location, Up: Startup File 

6.2 Wgetrc Syntax
The syntax of a wgetrc command is simple: 

     variable = value

The variable will also be called command. Valid values are different for different commands. 

The commands are case-insensitive and underscore-insensitive. Thus DIr__PrefiX is the same as dirprefix. Empty lines, lines beginning with # and lines containing white-space only are discarded. 

Commands that expect a comma-separated list will clear the list on an empty command. So, if you wish to reset the rejection list specified in global wgetrc, you can do it with: 

     reject =



--------------------------------------------------------------------------------
Next: Sample Wgetrc, Previous: Wgetrc Syntax, Up: Startup File 

6.3 Wgetrc Commands
The complete set of commands is listed below. Legal values are listed after the =. Simple Boolean values can be set or unset using on and off or 1 and 0. A fancier kind of Boolean allowed in some cases is the lockable Boolean, which may be set to on, off, always, or never. If an option is set to always or never, that value will be locked in for the duration of the Wget invocation command-line options will not override. 

Some commands take pseudo-arbitrary values. address values can be hostnames or dotted-quad IP addresses. n can be any positive integer, or inf for infinity, where appropriate. string values can be any non-empty string. 

Most of these commands have direct command-line equivalents. Also, any wgetrc command can be specified on the command line using the --execute switch (see Basic Startup Options.) 

accept/reject = string
Same as -A/-R (see Types of Files). 

add_hostdir = on/off
Enable/disable host-prefixed file names. -nH disables it. 

continue = on/off
If set to on, force continuation of preexistent partially retrieved files. See -c before setting it. 

background = on/off
Enable/disable going to background the same as -b (which enables it). 

backup_converted = on/off
Enable/disable saving pre-converted files with the suffix .orig the same as -K (which enables it). 

base = string
Consider relative urls in url input files forced to be interpreted as html as being relative to string the same as --base=string. 

bind_address = address
Bind to address, like the --bind-address=address. 

ca_certificate = file
Set the certificate authority bundle file to file. The same as --ca-certificate=file. 

ca_directory = directory
Set the directory used for certificate authorities. The same as --ca-directory=directory. 

cache = on/off
When set to off, disallow server-caching. See the --no-cache option. 

certificate = file
Set the client certificate file name to file. The same as --certificate=file. 

certificate_type = string
Specify the type of the client certificate, legal values being PEM (the default) and DER (aka ASN1). The same as --certificate-type=string. 

check_certificate = on/off
If this is set to off, the server certificate is not checked against the specified client authorities. The default is  on . The same as --check-certificate. 

convert_links = on/off
Convert non-relative links locally. The same as -k. 

cookies = on/off
When set to off, disallow cookies. See the --cookies option. 

connect_timeout = n
Set the connect timeout the same as --connect-timeout. 

cut_dirs = n
Ignore n remote directory components. Equivalent to --cut-dirs=n. 

debug = on/off
Debug mode, same as -d. 

delete_after = on/off
Delete after download the same as --delete-after. 

dir_prefix = string
Top of directory tree the same as -P string. 

dirstruct = on/off
Turning dirstruct on or off the same as -x or -nd, respectively. 

dns_cache = on/off
Turn DNS caching on/off. Since DNS caching is on by default, this option is normally used to turn it off and is equivalent to --no-dns-cache. 

dns_timeout = n
Set the DNS timeout the same as --dns-timeout. 

domains = string
Same as -D (see Spanning Hosts). 

dot_bytes = n
Specify the number of bytes  contained  in a dot, as seen throughout the retrieval (1024 by default). You can postfix the value with k or m, representing kilobytes and megabytes, respectively. With dot settings you can tailor the dot retrieval to suit your needs, or you can use the predefined styles (see Download Options). 

dots_in_line = n
Specify the number of dots that will be printed in each line throughout the retrieval (50 by default). 

dot_spacing = n
Specify the number of dots in a single cluster (10 by default). 

egd_file = file
Use string as the EGD socket file name. The same as --egd-file=file. 

exclude_directories = string
Specify a comma-separated list of directories you wish to exclude from download the same as -X string (see Directory-Based Limits). 

exclude_domains = string
Same as --exclude-domains=string (see Spanning Hosts). 

follow_ftp = on/off
Follow ftp links from html documents the same as --follow-ftp. 

follow_tags = string
Only follow certain html tags when doing a recursive retrieval, just like --follow-tags=string. 

force_html = on/off
If set to on, force the input filename to be regarded as an html document the same as -F. 

ftp_password = string
Set your ftp password to string. Without this setting, the password defaults to -wget@, which is a useful default for anonymous ftp access. 
This command used to be named passwd prior to Wget 1.10. 


ftp_proxy = string
Use string as ftp proxy, instead of the one specified in environment. 

ftp_user = string
Set ftp user to string. 
This command used to be named login prior to Wget 1.10. 


glob = on/off
Turn globbing on/off the same as --glob and --no-glob. 

header = string
Define a header for HTTP doewnloads, like using --header=string. 

html_extension = on/off
Add a .html extension to text/html or application/xhtml+xml files without it, like -E. 

http_keep_alive = on/off
Turn the keep-alive feature on or off (defaults to on). Turning it off is equivalent to --no-http-keep-alive. 

http_password = string
Set http password, equivalent to --http-password=string. 

http_proxy = string
Use string as http proxy, instead of the one specified in environment. 

http_user = string
Set http user to string, equivalent to --http-user=string. 

ignore_length = on/off
When set to on, ignore Content-Length header; the same as --ignore-length. 

ignore_tags = string
Ignore certain html tags when doing a recursive retrieval, like --ignore-tags=string. 

include_directories = string
Specify a comma-separated list of directories you wish to follow when downloading the same as -I string. 

inet4_only = on/off
Force connecting to IPv4 addresses, off by default. You can put this in the global init file to disable Wget's attempts to resolve and connect to IPv6 hosts. Available only if Wget was compiled with IPv6 support. The same as --inet4-only or -4. 

inet6_only = on/off
Force connecting to IPv6 addresses, off by default. Available only if Wget was compiled with IPv6 support. The same as --inet6-only or -6. 

input = file
Read the urls from string, like -i file. 

kill_longer = on/off
Consider data longer than specified in content-length header as invalid (and retry getting it). The default behavior is to save as much data as there is, provided there is more than or equal to the value in Content-Length. 

limit_rate = rate
Limit the download speed to no more than rate bytes per second. The same as --limit-rate=rate. 

load_cookies = file
Load cookies from file. See --load-cookies file. 

logfile = file
Set logfile to file, the same as -o file. 

mirror = on/off
Turn mirroring on/off. The same as -m. 

netrc = on/off
Turn reading netrc on or off. 

noclobber = on/off
Same as -nc. 

no_parent = on/off
Disallow retrieving outside the directory hierarchy, like --no-parent (see Directory-Based Limits). 

no_proxy = string
Use string as the comma-separated list of domains to avoid in proxy loading, instead of the one specified in environment. 

output_document = file
Set the output filename the same as -O file. 

page_requisites = on/off
Download all ancillary documents necessary for a single html page to display properly the same as -p. 

passive_ftp = on/off/always/never
Change setting of passive ftp, equivalent to the --passive-ftp option. Some scripts and .pm (Perl module) files download files using wget --passive-ftp. If your firewall does not allow this, you can set passive_ftp = never to override the command-line. 
password = string
Specify password string for both ftp and http file retrieval. This command can be overridden using the ftp_password and http_password command for ftp and http respectively. 

post_data = string
Use POST as the method for all HTTP requests and send string in the request body. The same as --post-data=string. 

post_file = file
Use POST as the method for all HTTP requests and send the contents of file in the request body. The same as --post-file=file. 

prefer_family = IPv4/IPv6/none
When given a choice of several addresses, connect to the addresses with specified address family first. IPv4 addresses are preferred by default. The same as --prefer-family, which see for a detailed discussion of why this is useful. 

private_key = file
Set the private key file to file. The same as --private-key=file. 

private_key_type = string
Specify the type of the private key, legal values being PEM (the default) and DER (aka ASN1). The same as --private-type=string. 

progress = string
Set the type of the progress indicator. Legal types are dot and bar. Equivalent to --progress=string. 

protocol_directories = on/off
When set, use the protocol name as a directory component of local file names. The same as --protocol-directories. 

proxy_user = string
Set proxy authentication user name to string, like --proxy-user=string. 

proxy_password = string
Set proxy authentication password to string, like --proxy-password=string. 

quiet = on/off
Quiet mode the same as -q. 

quota = quota
Specify the download quota, which is useful to put in the global wgetrc. When download quota is specified, Wget will stop retrieving after the download sum has become greater than quota. The quota can be specified in bytes (default), kbytes k appended) or mbytes (m appended). Thus quota = 5m will set the quota to 5 megabytes. Note that the user's startup file overrides system settings. 

random_file = file
Use file as a source of randomness on systems lacking /dev/random. 

read_timeout = n
Set the read (and write) timeout the same as --read-timeout=n. 

reclevel = n
Recursion level (depth) the same as -l n. 

recursive = on/off
Recursive on/off the same as -r. 

referer = string
Set HTTP Referer: header just like --referer=string. (Note it was the folks who wrote the http spec who got the spelling of  referrer  wrong.) 

relative_only = on/off
Follow only relative links the same as -L (see Relative Links). 

remove_listing = on/off
If set to on, remove ftp listings downloaded by Wget. Setting it to off is the same as --no-remove-listing. 

restrict_file_names = unix/windows
Restrict the file names generated by Wget from URLs. See --restrict-file-names for a more detailed description. 

retr_symlinks = on/off
When set to on, retrieve symbolic links as if they were plain files; the same as --retr-symlinks. 

retry_connrefused = on/off
When set to on, consider  connection refused  a transient error the same as --retry-connrefused. 

robots = on/off
Specify whether the norobots convention is respected by Wget,  on  by default. This switch controls both the /robots.txt and the nofollow aspect of the spec. See Robot Exclusion, for more details about this. Be sure you know what you are doing before turning this off. 

save_cookies = file
Save cookies to file. The same as --save-cookies file. 

secure_protocol = string
Choose the secure protocol to be used. Legal values are auto (the default), SSLv2, SSLv3, and TLSv1. The same as --secure-protocol=string. 

server_response = on/off
Choose whether or not to print the http and ftp server responses the same as -S. 

span_hosts = on/off
Same as -H. 

strict_comments = on/off
Same as --strict-comments. 

timeout = n
Set all applicable timeout values to n, the same as -T n. 

timestamping = on/off
Turn timestamping on/off. The same as -N (see Time-Stamping). 

tries = n
Set number of retries per url the same as -t n. 

use_proxy = on/off
When set to off, don't use proxy even when proxy-related environment variables are set. In that case it is the same as using --no-proxy. 

user = string
Specify username string for both ftp and http file retrieval. This command can be overridden using the ftp_user and http_user command for ftp and http respectively. 

verbose = on/off
Turn verbose on/off the same as -v/-nv. 

wait = n
Wait n seconds between retrievals the same as -w n. 

waitretry = n
Wait up to n seconds between retries of failed retrievals only the same as --waitretry=n. Note that this is turned on by default in the global wgetrc. 

randomwait = on/off
Turn random between-request wait times on or off. The same as --random-wait. 


--------------------------------------------------------------------------------
Previous: Wgetrc Commands, Up: Startup File 

6.4 Sample Wgetrc
This is the sample initialization file, as given in the distribution. It is divided in two section one for global usage (suitable for global startup file), and one for local usage (suitable for $HOME/.wgetrc). Be careful about the things you change. 

Note that almost all the lines are commented out. For a command to have any effect, you must remove the # character at the beginning of its line. 

     ###
     ### Sample Wget initialization file .wgetrc
     ###
     
     ## You can use this file to change the default behaviour of wget or to
     ## avoid having to type many many command-line options. This file does
     ## not contain a comprehensive list of commands -- look at the manual
     ## to find out what you can put into this file.
     ##
     ## Wget initialization file can reside in /usr/local/etc/wgetrc
     ## (global, for all users) or $HOME/.wgetrc (for a single user).
     ##
     ## To use the settings in this file, you will have to uncomment them,
     ## as well as change them, in most cases, as the values on the
     ## commented-out lines are the default values (e.g. "off").
     
     
     ##
     ## Global settings (useful for setting up in /usr/local/etc/wgetrc).
     ## Think well before you change them, since they may reduce wget's
     ## functionality, and make it behave contrary to the documentation:
     ##
     
     # You can set retrieve quota for beginners by specifying a value
     # optionally followed by 'K' (kilobytes) or 'M' (megabytes).  The
     # default quota is unlimited.
     #quota = inf
     
     # You can lower (or raise) the default number of retries when
     # downloading a file (default is 20).
     #tries = 20
     
     # Lowering the maximum depth of the recursive retrieval is handy to
     # prevent newbies from going too "deep" when they unwittingly start
     # the recursive retrieval.  The default is 5.
     #reclevel = 5
     
     # Many sites are behind firewalls that do not allow initiation of
     # connections from the outside.  On these sites you have to use the
     # `passive' feature of FTP.  If you are behind such a firewall, you
     # can turn this on to make Wget use passive FTP by default.
     #passive_ftp = off
     
     # The "wait" command below makes Wget wait between every connection.
     # If, instead, you want Wget to wait only between retries of failed
     # downloads, set waitretry to maximum number of seconds to wait (Wget
     # will use "linear backoff", waiting 1 second after the first failure
     # on a file, 2 seconds after the second failure, etc. up to this max).
     waitretry = 10
     
     
     ##
     ## Local settings (for a user to set in his $HOME/.wgetrc).  It is
     ## *highly* undesirable to put these settings in the global file, since
     ## they are potentially dangerous to "normal" users.
     ##
     ## Even when setting up your own ~/.wgetrc, you should know what you
     ## are doing before doing so.
     ##
     
     # Set this to on to use timestamping by default:
     #timestamping = off
     
     # It is a good idea to make Wget send your email address in a `From:'
     # header with your request (so that server administrators can contact
     # you in case of errors).  Wget does *not* send `From:' by default.
     #header = From: Your Name <username@site.domain>
     
     # You can set up other headers, like Accept-Language.  Accept-Language
     # is *not* sent by default.
     #header = Accept-Language: en
     
     # You can set the default proxies for Wget to use for http and ftp.
     # They will override the value in the environment.
     #http_proxy = http://proxy.yoyodyne.com:18023/
     #ftp_proxy = http://proxy.yoyodyne.com:18023/
     
     # If you do not want to use proxy at all, set this to off.
     #use_proxy = on
     
     # You can customize the retrieval outlook.  Valid options are default,
     # binary, mega and micro.
     #dot_style = default
     
     # Setting this to off makes Wget not download /robots.txt.  Be sure to
     # know *exactly* what /robots.txt is and how it is used before changing
     # the default!
     #robots = on
     
     # It can be useful to make Wget wait between connections.  Set this to
     # the number of seconds you want Wget to wait.
     #wait = 0
     
     # You can force creating directory structure, even if a single is being
     # retrieved, by setting this to on.
     #dirstruct = off
     
     # You can turn on recursive retrieving by default (don't do this if
     # you are not sure you know what it means) by setting this to on.
     #recursive = off
     
     # To always back up file X as X.orig before converting its links (due
     # to -k / --convert-links / convert_links = on having been specified),
     # set this variable to on:
     #backup_converted = off
     
     # To have Wget follow FTP links from HTML files by default, set this
     # to on:
     #follow_ftp = off



--------------------------------------------------------------------------------
Next: Various, Previous: Startup File, Up: Top 

7 Examples
The examples are divided into three sections loosely based on their complexity. 

Simple Usage: Simple, basic usage of the program. 
Advanced Usage: Advanced tips. 
Very Advanced Usage: The hairy stuff. 


--------------------------------------------------------------------------------
Next: Advanced Usage, Up: Examples 

7.1 Simple Usage
Say you want to download a url. Just type: 
          wget http://fly.srk.fer.hr/
     
But what will happen if the connection is slow, and the file is lengthy? The connection will probably fail before the whole file is retrieved, more than once. In this case, Wget will try getting the file until it either gets the whole of it, or exceeds the default number of retries (this being 20). It is easy to change the number of tries to 45, to insure that the whole file will arrive safely: 
          wget --tries=45 http://fly.srk.fer.hr/jpg/flyweb.jpg
     
Now let's leave Wget to work in the background, and write its progress to log file log. It is tiring to type --tries, so we shall use -t. 
          wget -t 45 -o log http://fly.srk.fer.hr/jpg/flyweb.jpg &
     
The ampersand at the end of the line makes sure that Wget works in the background. To unlimit the number of retries, use -t inf. 

The usage of ftp is as simple. Wget will take care of login and password. 
          wget ftp://gnjilux.srk.fer.hr/welcome.msg
     
If you specify a directory, Wget will retrieve the directory listing, parse it and convert it to html. Try: 
          wget ftp://ftp.gnu.org/pub/gnu/
          links index.html
     


--------------------------------------------------------------------------------
Next: Very Advanced Usage, Previous: Simple Usage, Up: Examples 

7.2 Advanced Usage
You have a file that contains the URLs you want to download? Use the -i switch: 
          wget -i file
     
If you specify - as file name, the urls will be read from standard input. 

Create a five levels deep mirror image of the GNU web site, with the same directory structure the original has, with only one try per document, saving the log of the activities to gnulog: 
          wget -r http://www.gnu.org/ -o gnulog
     
The same as the above, but convert the links in the html files to point to local files, so you can view the documents off-line: 
          wget --convert-links -r http://www.gnu.org/ -o gnulog
     
Retrieve only one html page, but make sure that all the elements needed for the page to be displayed, such as inline images and external style sheets, are also downloaded. Also make sure the downloaded page references the downloaded links. 
          wget -p --convert-links http://www.server.com/dir/page.html
     
The html page will be saved to www.server.com/dir/page.html, and the images, stylesheets, etc., somewhere under www.server.com/, depending on where they were on the remote server. 

The same as the above, but without the www.server.com/ directory. In fact, I don't want to have all those random server directories anyway just save all those files under a download/ subdirectory of the current directory. 
          wget -p --convert-links -nH -nd -Pdownload \
               http://www.server.com/dir/page.html
     
Retrieve the index.html of www.lycos.com, showing the original server headers: 
          wget -S http://www.lycos.com/
     
Save the server headers with the file, perhaps for post-processing. 
          wget --save-headers http://www.lycos.com/
          more index.html
     
Retrieve the first two levels of wuarchive.wustl.edu, saving them to /tmp. 
          wget -r -l2 -P/tmp ftp://wuarchive.wustl.edu/
     
You want to download all the gifs from a directory on an http server. You tried wget http://www.server.com/dir/*.gif, but that didn't work because http retrieval does not support globbing. In that case, use: 
          wget -r -l1 --no-parent -A.gif http://www.server.com/dir/
     
More verbose, but the effect is the same. -r -l1 means to retrieve recursively (see Recursive Download), with maximum depth of 1. --no-parent means that references to the parent directory are ignored (see Directory-Based Limits), and -A.gif means to download only the gif files. -A "*.gif" would have worked too. 

Suppose you were in the middle of downloading, when Wget was interrupted. Now you do not want to clobber the files already present. It would be: 
          wget -nc -r http://www.gnu.org/
     
If you want to encode your own username and password to http or ftp, use the appropriate url syntax (see URL Format). 
          wget ftp://hniksic:mypassword@unix.server.com/.emacs
     
Note, however, that this usage is not advisable on multi-user systems because it reveals your password to anyone who looks at the output of ps. 


You would like the output documents to go to standard output instead of to files? 
          wget -O - http://jagor.srce.hr/ http://www.srce.hr/
     
You can also combine the two options and make pipelines to retrieve the documents from remote hotlists: 

          wget -O - http://cool.list.com/ | wget --force-html -i -
     


--------------------------------------------------------------------------------
Previous: Advanced Usage, Up: Examples 

7.3 Very Advanced Usage

If you wish Wget to keep a mirror of a page (or ftp subdirectories), use --mirror (-m), which is the shorthand for -r -l inf -N. You can put Wget in the crontab file asking it to recheck a site each Sunday: 
          crontab
          0 0 * * 0 wget --mirror http://www.gnu.org/ -o /home/me/weeklog
     
In addition to the above, you want the links to be converted for local viewing. But, after having read this manual, you know that link conversion doesn't play well with timestamping, so you also want Wget to back up the original html files before the conversion. Wget invocation would look like this: 
          wget --mirror --convert-links --backup-converted  \
               http://www.gnu.org/ -o /home/me/weeklog
     
But you've also noticed that local viewing doesn't work all that well when html files are saved under extensions other than .html, perhaps because they were served as index.cgi. So you'd like Wget to rename all the files served with content-type text/html or application/xhtml+xml to name.html. 
          wget --mirror --convert-links --backup-converted \
               --html-extension -o /home/me/weeklog        \
               http://www.gnu.org/
     
Or, with less typing: 

          wget -m -k -K -E http://www.gnu.org/ -o /home/me/weeklog
     


--------------------------------------------------------------------------------
Next: Appendices, Previous: Examples, Up: Top 

8 Various
This chapter contains all the stuff that could not fit anywhere else. 

Proxies: Support for proxy servers 
Distribution: Getting the latest version. 
Mailing List: Wget mailing list for announcements and discussion. 
Reporting Bugs: How and where to report bugs. 
Portability: The systems Wget works on. 
Signals: Signal-handling performed by Wget. 


--------------------------------------------------------------------------------
Next: Distribution, Up: Various 

8.1 Proxies
Proxies are special-purpose http servers designed to transfer data from remote servers to local clients. One typical use of proxies is lightening network load for users behind a slow connection. This is achieved by channeling all http and ftp requests through the proxy which caches the transferred data. When a cached resource is requested again, proxy will return the data from cache. Another use for proxies is for companies that separate (for security reasons) their internal networks from the rest of Internet. In order to obtain information from the Web, their users connect and retrieve remote data using an authorized proxy. 

Wget supports proxies for both http and ftp retrievals. The standard way to specify proxy location, which Wget recognizes, is using the following environment variables: 

http_proxy
This variable should contain the url of the proxy for http connections. 

ftp_proxy
This variable should contain the url of the proxy for ftp connections. It is quite common that http_proxy and ftp_proxy are set to the same url. 

no_proxy
This variable should contain a comma-separated list of domain extensions proxy should not be used for. For instance, if the value of no_proxy is .mit.edu, proxy will not be used to retrieve documents from MIT. 
In addition to the environment variables, proxy location and settings may be specified from within Wget itself. 

--no-proxy
proxy = on/off
This option and the corresponding command may be used to suppress the use of proxy, even if the appropriate environment variables are set. 

http_proxy = URL
ftp_proxy = URL
no_proxy = string
These startup file variables allow you to override the proxy settings specified by the environment. 
Some proxy servers require authorization to enable you to use them. The authorization consists of username and password, which must be sent by Wget. As with http authorization, several authentication schemes exist. For proxy authorization only the Basic authentication scheme is currently implemented. 

You may specify your username and password either through the proxy url or through the command-line options. Assuming that the company's proxy is located at proxy.company.com at port 8001, a proxy url location containing authorization data might look like this: 

     http://hniksic:mypassword@proxy.company.com:8001/

Alternatively, you may use the proxy-user and proxy-password options, and the equivalent .wgetrc settings proxy_user and proxy_password to set the proxy username and password. 



--------------------------------------------------------------------------------
Next: Mailing List, Previous: Proxies, Up: Various 

8.2 Distribution
Like all GNU utilities, the latest version of Wget can be found at the master GNU archive site ftp.gnu.org, and its mirrors. For example, Wget 1.10 can be found at ftp://ftp.gnu.org/pub/gnu/wget/wget-1.10.tar.gz 



--------------------------------------------------------------------------------
Next: Reporting Bugs, Previous: Distribution, Up: Various 

8.3 Mailing List
There are several Wget-related mailing lists, all hosted by SunSITE.dk. The general discussion list is at wget@sunsite.dk. It is the preferred place for bug reports and suggestions, as well as for discussion of development. You are invited to subscribe. 

To subscribe, simply send mail to wget-subscribe@sunsite.dk and follow the instructions. Unsubscribe by mailing to wget-unsubscribe@sunsite.dk. The mailing list is archived at http://www.mail-archive.com/wget%40sunsite.dk/ and at http://news.gmane.org/gmane.comp.web.wget.general. 

The second mailing list is at wget-patches@sunsite.dk, and is used to submit patches for review by Wget developers. A  patch  is a textual representation of change to source code, readable by both humans and programs. The file PATCHES that comes with Wget covers the creation and submitting of patches in detail. Please don't send general suggestions or bug reports to wget-patches; use it only for patch submissions. 

To subscribe, simply send mail to wget-subscribe@sunsite.dk and follow the instructions. Unsubscribe by mailing to wget-unsubscribe@sunsite.dk. The mailing list is archived at http://news.gmane.org/gmane.comp.web.wget.patches. 

Finally, there is a read-only list at wget-cvs@sunsite.dk that tracks commits to the Wget CVS repository. To subscribe to that list, send mail to wget-cvs-subscribe@sunsite.dk. The list is not archived. 



--------------------------------------------------------------------------------
Next: Portability, Previous: Mailing List, Up: Various 

8.4 Reporting Bugs
You are welcome to send bug reports about GNU Wget to bug-wget@gnu.org. 

Before actually submitting a bug report, please try to follow a few simple guidelines. 

Please try to ascertain that the behavior you see really is a bug. If Wget crashes, it's a bug. If Wget does not behave as documented, it's a bug. If things work strange, but you are not sure about the way they are supposed to work, it might well be a bug. 
Try to repeat the bug in as simple circumstances as possible. E.g. if Wget crashes while downloading wget -rl0 -kKE -t5 -Y0 http://yoyodyne.com -o /tmp/log, you should try to see if the crash is repeatable, and if will occur with a simpler set of options. You might even try to start the download at the page where the crash occurred to see if that page somehow triggered the crash. 
Also, while I will probably be interested to know the contents of your .wgetrc file, just dumping it into the debug message is probably a bad idea. Instead, you should first try to see if the bug repeats with .wgetrc moved out of the way. Only if it turns out that .wgetrc settings affect the bug, mail me the relevant parts of the file. 

Please start Wget with -d option and send us the resulting output (or relevant parts thereof). If Wget was compiled without debug support, recompile it it is much easier to trace bugs with debug support on. 
Note: please make sure to remove any potentially sensitive information from the debug log before sending it to the bug address. The -d won't go out of its way to collect sensitive information, but the log will contain a fairly complete transcript of Wget's communication with the server, which may include passwords and pieces of downloaded data. Since the bug address is publically archived, you may assume that all bug reports are visible to the public. 

If Wget has crashed, try to run it in a debugger, e.g. gdb `which wget` core and type where to get the backtrace. This may not work if the system administrator has disabled core files, but it is safe to try. 


--------------------------------------------------------------------------------
Next: Signals, Previous: Reporting Bugs, Up: Various 

8.5 Portability
Like all GNU software, Wget works on the GNU system. However, since it uses GNU Autoconf for building and configuring, and mostly avoids using  special  features of any particular Unix, it should compile (and work) on all common Unix flavors. 

Various Wget versions have been compiled and tested under many kinds of Unix systems, including GNU/Linux, Solaris, SunOS 4.x, OSF (aka Digital Unix or Tru64), Ultrix, *BSD, IRIX, AIX, and others. Some of those systems are no longer in widespread use and may not be able to support recent versions of Wget. If Wget fails to compile on your system, we would like to know about it. 

Thanks to kind contributors, this version of Wget compiles and works on 32-bit Microsoft Windows platforms. It has been compiled successfully using MS Visual C++ 6.0, Watcom, Borland C, and GCC compilers. Naturally, it is crippled of some features available on Unix, but it should work as a substitute for people stuck with Windows. Note that Windows-specific portions of Wget are not guaranteed to be supported in the future, although this has been the case in practice for many years now. All questions and problems in Windows usage should be reported to Wget mailing list at wget@sunsite.dk where the volunteers who maintain the Windows-related features might look at them. 



--------------------------------------------------------------------------------
Previous: Portability, Up: Various 

8.6 Signals
Since the purpose of Wget is background work, it catches the hangup signal (SIGHUP) and ignores it. If the output was on standard output, it will be redirected to a file named wget-log. Otherwise, SIGHUP is ignored. This is convenient when you wish to redirect the output of Wget after having started it. 

     $ wget http://www.gnus.org/dist/gnus.tar.gz &
     ...
     $ kill -HUP %%
     SIGHUP received, redirecting output to `wget-log'.

Other than that, Wget will not try to interfere with signals in any way. C-c, kill -TERM and kill -KILL should kill it alike. 



--------------------------------------------------------------------------------
Next: Copying, Previous: Various, Up: Top 

9 Appendices
This chapter contains some references I consider useful. 

Robot Exclusion: Wget's support for RES. 
Security Considerations: Security with Wget. 
Contributors: People who helped. 


--------------------------------------------------------------------------------
Next: Security Considerations, Up: Appendices 

9.1 Robot Exclusion
It is extremely easy to make Wget wander aimlessly around a web site, sucking all the available data in progress. wget -r site, and you're set. Great? Not for the server admin. 

As long as Wget is only retrieving static pages, and doing it at a reasonable rate (see the --wait option), there's not much of a problem. The trouble is that Wget can't tell the difference between the smallest static page and the most demanding CGI. A site I know has a section handled by a CGI Perl script that converts Info files to html on the fly. The script is slow, but works well enough for human users viewing an occasional Info file. However, when someone's recursive Wget download stumbles upon the index page that links to all the Info files through the script, the system is brought to its knees without providing anything useful to the user (This task of converting Info files could be done locally and access to Info documentation for all installed GNU software on a system is available from the info command). 

To avoid this kind of accident, as well as to preserve privacy for documents that need to be protected from well-behaved robots, the concept of robot exclusion was invented. The idea is that the server administrators and document authors can specify which portions of the site they wish to protect from robots and those they will permit access. 

The most popular mechanism, and the de facto standard supported by all the major robots, is the  Robots Exclusion Standard  (RES) written by Martijn Koster et al. in 1994. It specifies the format of a text file containing directives that instruct the robots which URL paths to avoid. To be found by the robots, the specifications must be placed in /robots.txt in the server root, which the robots are expected to download and parse. 

Although Wget is not a web robot in the strictest sense of the word, it can downloads large parts of the site without the user's intervention to download an individual page. Because of that, Wget honors RES when downloading recursively. For instance, when you issue: 

     wget -r http://www.server.com/

First the index of www.server.com will be downloaded. If Wget finds that it wants to download more documents from that server, it will request http://www.server.com/robots.txt and, if found, use it for further downloads. robots.txt is loaded only once per each server. 

Until version 1.8, Wget supported the first version of the standard, written by Martijn Koster in 1994 and available at http://www.robotstxt.org/wc/norobots.html. As of version 1.8, Wget has supported the additional directives specified in the internet draft <draft-koster-robots-00.txt> titled  A Method for Web Robots Control . The draft, which has as far as I know never made to an rfc, is available at http://www.robotstxt.org/wc/norobots-rfc.txt. 

This manual no longer includes the text of the Robot Exclusion Standard. 

The second, less known mechanism, enables the author of an individual document to specify whether they want the links from the file to be followed by a robot. This is achieved using the META tag, like this: 

     <meta name="robots" content="nofollow">

This is explained in some detail at http://www.robotstxt.org/wc/meta-user.html. Wget supports this method of robot exclusion in addition to the usual /robots.txt exclusion. 

If you know what you are doing and really really wish to turn off the robot exclusion, set the robots variable to off in your .wgetrc. You can achieve the same effect from the command line using the -e switch, e.g. wget -e robots=off url.... 



--------------------------------------------------------------------------------
Next: Contributors, Previous: Robot Exclusion, Up: Appendices 

9.2 Security Considerations
When using Wget, you must be aware that it sends unencrypted passwords through the network, which may present a security problem. Here are the main issues, and some solutions. 

The passwords on the command line are visible using ps. The best way around it is to use wget -i - and feed the urls to Wget's standard input, each on a separate line, terminated by C-d. Another workaround is to use .netrc to store passwords; however, storing unencrypted passwords is also considered a security risk. 
Using the insecure basic authentication scheme, unencrypted passwords are transmitted through the network routers and gateways. 
The ftp passwords are also in no way encrypted. There is no good solution for this at the moment. 
Although the  normal  output of Wget tries to hide the passwords, debugging logs show them, in all forms. This problem is avoided by being careful when you send debug logs (yes, even when you send them to me). 


--------------------------------------------------------------------------------
Previous: Security Considerations, Up: Appendices 

9.3 Contributors
GNU Wget was written by Hrvoje Niksic hniksic@xemacs.org. However, its development could never have gone as far as it has, were it not for the help of many people, either with bug reports, feature proposals, patches, or letters saying  Thanks! . 

Special thanks goes to the following people (no particular order): 

Karsten Thygesen donated system resources such as the mailing list, web space, and ftp space, along with a lot of time to make these actually work. 
Shawn McHorse bug reports and patches. 
Kaveh R. Ghazi on-the-fly ansi2knr-ization. Lots of portability fixes. 
Gordon Matzigkeit .netrc support. 
Zlatko Calusic, Tomislav Vujec and Drazen Kacar feature suggestions and  philosophical  discussions. 
Darko Budor initial port to Windows. 
Antonio Rosella help and suggestions, plus the Italian translation. 
Tomislav Petrovic, Mario Mikocevic many bug reports and suggestions. 
Francois Pinard many thorough bug reports and discussions. 
Karl Eichwalder lots of help with internationalization and other things. 
Junio Hamano donated support for Opie and http Digest authentication. 
The people who provided donations for development, including Brian Gough. 
The following people have provided patches, bug/build reports, useful suggestions, beta testing services, fan mail and all the other things that make maintenance so much fun: 

Ian Abbott Tim Adam, Adrian Aichner, Martin Baehr, Dieter Baron, Roger Beeman, Dan Berger, T. Bharath, Christian Biere, Paul Bludov, Daniel Bodea, Mark Boyns, John Burden, Wanderlei Cavassin, Gilles Cedoc, Tim Charron, Noel Cragg, Kristijan Conkas, John Daily, Andreas Damm, Ahmon Dancy, Andrew Davison, Bertrand Demiddelaer, Andrew Deryabin, Ulrich Drepper, Marc Duponcheel, Damir Dzeko, Alan Eldridge, Hans-Andreas Engel, Aleksandar Erkalovic, Andy Eskilsson, Christian Fraenkel, David Fritz, Charles C. Fu, FUJISHIMA Satsuki, Masashi Fujita, Howard Gayle, Marcel Gerrits, Lemble Gregory, Hans Grobler, Mathieu Guillaume, Dan Harkless, Aaron Hawley, Herold Heiko, Jochen Hein, Karl Heuer, HIROSE Masaaki, Ulf Harnhammar, Gregor Hoffleit, Erik Magnus Hulthen, Richard Huveneers, Jonas Jensen, Larry Jones, Simon Josefsson, Mario Juric, Hack Kampbjorn, Const Kaplinsky, Goran Kezunovic, Igor Khristophorov, Robert Kleine, KOJIMA Haime, Fila Kolodny, Alexander Kourakos, Martin Kraemer, Sami Krank, Simos KSenitellis, Christian Lackas, Hrvoje Lacko, Daniel S. Lewart, Nicolas Lichtmeier, Dave Love, Alexander V. Lukyanov, Thomas Lussnig, Andre Majorel, Aurelien Marchand, Matthew J. Mellon, Jordan Mendelson, Lin Zhe Min, Jan Minar, Tim Mooney, Keith Moore, Adam D. Moss, Simon Munton, Charlie Negyesi, R. K. Owen, Leonid Petrov, Simone Piunno, Andrew Pollock, Steve Pothier, Jan Prikryl, Marin Purgar, Csaba Raduly, Keith Refson, Bill Richardson, Tyler Riddle, Tobias Ringstrom, Juan Jose Rodriguez, Maciej W. Rozycki, Edward J. Sabol, Heinz Salzmann, Robert Schmidt, Nicolas Schodet, Andreas Schwab, Chris Seawood, Dennis Smit, Toomas Soome, Tage Stabell-Kulo, Philip Stadermann, Daniel Stenberg, Sven Sternberger, Markus Strasser, John Summerfield, Szakacsits Szabolcs, Mike Thomas, Philipp Thomas, Mauro Tortonesi, Dave Turner, Gisle Vanem, Russell Vincent, Zeljko Vrba, Charles G Waldman, Douglas E. Wegscheid, YAMAZAKI Makoto, Jasmin Zainul, Bojan Zdrnja, Kristijan Zimmer. 

Apologies to all who I accidentally left out, and many thanks to all the subscribers of the Wget mailing list. 



--------------------------------------------------------------------------------
Next: Concept Index, Previous: Appendices, Up: Top 

10 Copying
GNU Wget is licensed under the GNU General Public License (GNU GPL), which makes it free software. Please note that  free  in  free software  refers to liberty, not price. As some people like to point out, it's the  free  of  free speech , not the  free  of  free beer . 

The exact and legally binding distribution terms are spelled out below. The GPL guarantees that you have the right (freedom) to run and change GNU Wget and distribute it to others, and even if you want charge money for doing any of those things. With these rights comes the obligation to distribute the source code along with the software and to grant your recipients the same rights and impose the same restrictions. 

This licensing model is also known as open source because it, among other things, makes sure that all recipients will receive the source code along with the program, and be able to improve it. The GNU project prefers the term  free software  for reasons outlined at http://www.gnu.org/philosophy/free-software-for-freedom.html. 

The exact license terms are defined by this paragraph and the GNU General Public License it refers to: 

GNU Wget is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version. 
GNU Wget is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. 

A copy of the GNU General Public License is included as part of this manual; if you did not receive it, write to the Free Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA. 

In addition to this, this manual is free in the same sense: 

Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with the Invariant Sections being  GNU General Public License  and  GNU Free Documentation License , with no Front-Cover Texts, and with no Back-Cover Texts. A copy of the license is included in the section entitled  GNU Free Documentation License . 
The full texts of the GNU General Public License and of the GNU Free Documentation License are available below. 

GNU General Public License 
GNU Free Documentation License 


--------------------------------------------------------------------------------
Next: GNU Free Documentation License, Up: Copying 

10.1 GNU General Public License
Version 2, June 1991
     Copyright   1989, 1991 Free Software Foundation, Inc.
     675 Mass Ave, Cambridge, MA 02139, USA
     
     Everyone is permitted to copy and distribute verbatim copies
     of this license document, but changing it is not allowed.

Preamble
The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Library General Public License instead.) You can apply it to your programs, too. 

When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things. 

To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it. 

For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. 

We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software. 

Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations. 

Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all. 

The precise terms and conditions for copying, distribution and modification follow. 

TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The  Program , below, refers to any such program or work, and a  work based on the Program  means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term  modification .) Each licensee is addressed as  you . 
Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does. 

You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program. 
You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. 

You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: 
You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change. 
You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License. 
If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.) 
These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. 

Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program. 

In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. 

You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following: 
Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, 
Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, 
Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.) 
The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. 

If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code. 

You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. 
You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it. 
Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License. 
If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program. 
If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances. 

It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. 

This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. 

If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. 
The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. 
Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and  any later version , you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation. 

If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. 
NO WARRANTY

BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM  AS IS  WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 
IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 
END OF TERMS AND CONDITIONS
How to Apply These Terms to Your New Programs
If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. 

To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the  copyright  line and a pointer to where the full notice is found. 

     one line to give the program's name and an idea of what it does.
     Copyright (C) 20yy  name of author
     
     This program is free software; you can redistribute it and/or
     modify it under the terms of the GNU General Public License
     as published by the Free Software Foundation; either version 2
     of the License, or (at your option) any later version.
     
     This program is distributed in the hope that it will be useful,
     but WITHOUT ANY WARRANTY; without even the implied warranty of
     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
     GNU General Public License for more details.
     
     You should have received a copy of the GNU General Public License
     along with this program; if not, write to the Free Software
     Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.

Also add information on how to contact you by electronic and paper mail. 

If the program is interactive, make it output a short notice like this when it starts in an interactive mode: 

     Gnomovision version 69, Copyright (C) 20yy name of author
     Gnomovision comes with ABSOLUTELY NO WARRANTY; for details
     type `show w'.  This is free software, and you are welcome
     to redistribute it under certain conditions; type `show c'
     for details.

The hypothetical commands show w and show c should show the appropriate parts of the General Public License. Of course, the commands you use may be called something other than show w and show c; they could even be mouse-clicks or menu items whatever suits your program. 

You should also get your employer (if you work as a programmer) or your school, if any, to sign a  copyright disclaimer  for the program, if necessary. Here is a sample; alter the names: 

     Yoyodyne, Inc., hereby disclaims all copyright
     interest in the program `Gnomovision'
     (which makes passes at compilers) written
     by James Hacker.
     
     signature of Ty Coon, 1 April 1989
     Ty Coon, President of Vice

This General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Library General Public License instead of this License. 



--------------------------------------------------------------------------------
Previous: GNU General Public License, Up: Copying 

10.2 GNU Free Documentation License

Version 1.2, November 2002
     Copyright   2000,2001,2002 Free Software Foundation, Inc.
     59 Temple Place, Suite 330, Boston, MA  02111-1307, USA
     
     Everyone is permitted to copy and distribute verbatim copies
     of this license document, but changing it is not allowed.

PREAMBLE 
The purpose of this License is to make a manual, textbook, or other functional and useful document free in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others. 

This License is a kind of  copyleft , which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software. 

We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference. 

APPLICABILITY AND DEFINITIONS 
This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The  Document , below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as  you . You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law. 

A  Modified Version  of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language. 

A  Secondary Section  is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them. 

The  Invariant Sections  are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none. 

The  Cover Texts  are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words. 

A  Transparent  copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not  Transparent  is called  Opaque . 

Examples of suitable formats for Transparent copies include plain ascii without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only. 

The  Title Page  means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such,  Title Page  means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text. 

A section  Entitled XYZ  means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as  Acknowledgements ,  Dedications ,  Endorsements , or  History .) To  Preserve the Title  of such a section when you modify the Document means that it remains a section  Entitled XYZ  according to this definition. 

The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License. 

VERBATIM COPYING 
You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3. 

You may also lend copies, under the same conditions stated above, and you may publicly display copies. 

COPYING IN QUANTITY 
If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects. 

If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages. 

If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public. 

It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document. 

MODIFICATIONS 
You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version: 

Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission. 
List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement. 
State on the Title page the name of the publisher of the Modified Version, as the publisher. 
Preserve all the copyright notices of the Document. 
Add an appropriate copyright notice for your modifications adjacent to the other copyright notices. 
Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below. 
Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice. 
Include an unaltered copy of this License. 
Preserve the section Entitled  History , Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled  History  in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence. 
Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the  History  section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission. 
For any section Entitled  Acknowledgements  or  Dedications , Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein. 
Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles. 
Delete any section Entitled  Endorsements . Such a section may not be included in the Modified Version. 
Do not retitle any existing section to be Entitled  Endorsements  or to conflict in title with any Invariant Section. 
Preserve any Warranty Disclaimers. 
If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles. 

You may add a section Entitled  Endorsements , provided it contains nothing but endorsements of your Modified Version by various parties for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard. 

You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one. 

The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version. 

COMBINING DOCUMENTS 
You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers. 

The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work. 

In the combination, you must combine any sections Entitled  History  in the various original documents, forming one section Entitled  History ; likewise combine any sections Entitled  Acknowledgements , and any sections Entitled  Dedications . You must delete all sections Entitled  Endorsements.  

COLLECTIONS OF DOCUMENTS 
You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects. 

You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document. 

AGGREGATION WITH INDEPENDENT WORKS 
A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an  aggregate  if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document. 

If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate. 

TRANSLATION 
Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail. 

If a section in the Document is Entitled  Acknowledgements ,  Dedications , or  History , the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title. 

TERMINATION 
You may not copy, modify, sublicense, or distribute the Document except as expressly provided for under this License. Any other attempt to copy, modify, sublicense or distribute the Document is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. 

FUTURE REVISIONS OF THIS LICENSE 
The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See http://www.gnu.org/copyleft/. 

Each version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License  or any later version  applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. 

10.2.1 ADDENDUM: How to use this License for your documents
To use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page: 

       Copyright (C)  year  your name.
       Permission is granted to copy, distribute and/or modify this document
       under the terms of the GNU Free Documentation License, Version 1.2
       or any later version published by the Free Software Foundation;
       with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
       Texts.  A copy of the license is included in the section entitled ``GNU
       Free Documentation License''.

If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts, replace the  with...Texts.  line with this: 

         with the Invariant Sections being list their titles, with
         the Front-Cover Texts being list, and with the Back-Cover Texts
         being list.

If you have Invariant Sections without Cover Texts, or some other combination of the three, merge those two alternatives to suit the situation. 

If your document contains nontrivial examples of program code, we recommend releasing these examples in parallel under your choice of free software license, such as the GNU General Public License, to permit their use in free software. 

Bunjil was not satisfied until he had created sentient human beings. It was a harder task than any he had attempted. The making of other forms of animal life had been comparatively simple. The marking of a man was a challenge to the Great Spirit, for within the framework of flesh there was need for powers of thought, reasoning, and other human characteristics that would seperate man from the animal creation. 

He pondered long before attempting the supreme masterpiece. When at last he was ready he prepared two sheets of bark, cutting them to the shape he envisaged as suited to such a noble purpose. Mobility and dexterity were important, and these he incorporated into his design. Next he took soft clay, moulding it to the shape of the bark, smoothing it with his hands. 

When the work was finished he danced round the two inert figures, implanting seeds of knowledge and the capacity to reason and learn. 

The time had come for his skill to be put to the test. He gave them names--Berrook-boorn and Kookin-berrook. This was the first and most important step, for without names they would have lacked personality and spirit. Bunjil was well aware that if these beings were to fulfill their purpose, they must share his spirit as well as the characteristics of animals. 

Although without breath they were now named and ready for the infilling of the life force. Again Bunjil danced round them and then lay on their bodies, one after the other, breathing breath and life into their mouths, nostrils, and navels. 

For the third time Bunjil danced round them. As his feet wove intricate patterns in the dust, Berrook-boorn and Kookin-berrook rose slowly to their feet. They linked hands with Bunjil and with each other, joining the All-Father in the dance of life singing with him the first song that ever came from the lips of man. 
The aboriginal stories of creation, myths and legends about moral and natural issues, and fables are a remarkable group of tales--full of evocative, sometimes even surreal, imagery and deep observations on life. While no doubt these stories have been tainted by a Western viewpoint, they still represent a remarkable chance to understand even a little about cultures that lived for tens of thousands of years. What follows here is the beginning of an on-line collection of stories, taken from as many sources as possible and from as many different Aboriginal cultures as possible. 

Bunjil's work was nearing an end. The land was fair, adorned with vegetation ranging from moss and tiny blades of grass to the tall trees that stood stiff and unyielding in the still air. Animal life was abundant and infinite in variety, flying, scurrying across the ground, and burrowing through the soil. Only the trees and plants remained motionless, as though Bunjil had forgotten to give them life. 

"There must be movement, for life is a pulsating state of ceaseless activity," he murmured. "There must be moving air to carry the clouds on its back, strong winds to bend the trees, and fitful breezes to enable birds to fight against them and make them strong." 

He looked round him. Bellin-bellin the Crow was behind him, with an airtight bag suspended from his neck. 

"Have you kept the winds I gave you to mind safe in your bag?" he asked. 

"Yes, Great Father Bunjil, they are all there. Not one has escaped." 

"Good! Now you may open it and release some of the winds." 

Bellin-bellin cautiously opened one corner of the bag. A gentle breeze sped across the western lands, another to the east, another to the south, and a fiercer, colder wind to the north. 

The trees waved their branches, the birds lifted their voices as they felt the fresh air caressing their bodies, and even the insects and lizards joined in praise of Bunjil, the Great Provider. 

"That is good," Bunjil told him. "One last wind, please, a stronger one, a colder one, that will challenge my children to be brave and stand up to raging storms, and prepare them for the evil years that may lie ahead." 

Bellin-bellin opened the neck of the bag wider still, and out roared a screaming wind with snow and the chill of high mountain pools, cold and bracing. 

"Enough! Enough!" cried Bunjil. "No one can withstand the power of the south wind." 

So strong was the wind that it bent the tall trees double and denuded them of their leaves, while he and his family were blown right out of the world, together with all their possessions. It did not stop blowing until Bunjil and all his relatives and followers were blown back to their permanent home in the sky. 

Some of our earlier success stories are listed below. We hope you enjoy reading them. (There's a lot of pictures, so it may take a few minutes to load!)  If you have a picture of your adopted lab that you would like to add to our list of success stories, let us know and we'll be more than happy to post it!
To view our 2003 success stories, click here.
To view our 2002 success stories, click here. 
To view our 2001 success stories, click here. 
 
 
  
  Stoli
Stoli was about 10 years old when she came to live with me almost a year ago. She was tense and worried at first, but after a couple of weeks she began to relax and get used to her new routine. After a few months she finally started to let her cheerful personality come out, as well as some questionable habits like surfing the garbage pail. We have the house rules straightened out now, and she is a real pleasure. She still has plenty of playfulness and a moderate energy level that matches my own, but sheis always calm on the leash, well-behaved in the house, and as pleasant and sensible as a mature dog should be. 

She is a wonderful companion, and a perfect fit for my life. Thanks to everyone at L.E.A.R.N. for matching me up with such a terrific dog! 

Amy Ronner 
 
  
  Dudley 
Now that I found my new family I figured I'd give LEARN the inside scoop on my new life. I was through multiple homes in my short life due to no fault of my own. I needed owners who could understand me and help me learn what I needed to do to become a great dog. My family tells me that I brighten their every day. They do the same for me. 

I have been through basic obedience which was a great learning experience for me. My mom brags and tells everyone I was a very quick learner. I have been to the dog parks in my area and it is great fun to sniff the other dogs and run and fetch the ball. In my previous life I wasn't socialized with other dogs so learning how to interact and play has helped me realize that meeting other dogs is fun!  

I have been camping, hiking and fishing with my new family. I have learned to swim and I love the water. I am my Dad's fishing buddy except when I decide to jump in after his catch of the day. I can't help it! The flopping fishies make me want to retrieve them and bring them back to him!

My best friend is my sister Bella, the yellow dog in the picture. We are inseparable, wherever she goes I follow. I love to spend time with her playing, swimming, running or snuggling. My brother Cooper is a senior lab so he's glad to have us youngsters out of his hair.  

Did I forget to tell you that I love the hose? I'd play with the squirting water all day long if I could, even when I'm getting a bath.

Well that's about all for now. I just wanted to make sure I thanked LEARN for all that they have done for me and for finding my forever family.

Dan, Robin, Bella, Cooper and Dudley Kassees 
 
  
  Guinness (formerly Bert)
I wanted to share with you my adoption story. I am a black lab that was adopted Nov. 30, 2003 from Madison, WI. My name was Bert, but my new family renamed me Guinness, after my dad's favorite beer (THANK GOD). 

When I arrived at my new house, I was pretty calm due to the fact that I just drove two hours in the car. The next day my new mom took me, I mean I took her for a walk. When we got home from that walk I knew I was in trouble because she called the DOG TRAINER. So, for the next ten weeks I had to go to training. In class I was very obedient because if I wasn't that mean old teacher called me to the front, and I was very afraid of the teacher. But, I passed the class and got a blue ribbon for the best student.

In July my dad, his brothers, and I took a very long trip in the car to our new house in California. At first I really didn't like California because it was 100 degrees everyday and that is too hot for me. But, I did like to go outside to swim in the swimming pool, which I love. My favorite thing to do, is to dive in the pool after a long run with my dad. Just as I was getting use to California, my mom goes and has a baby. Now I have to start all over again. They moved me across the country and replace me with a baby all in the same year.

It has been three months since my sister was born and I really don't pay any attention to her. I do try to get as much attention from my mom and dad as possible.

My new favorite thing to do, is to go to the Sequoia National Park. It's so much fun. I love running up and down the mountains. My mom sent a picture of me running in the mountains for you to see.
 
  
  Chloe
Wow 8 months have sure flown by! I am very well adjusted at home now. I graduated beginners obedience training and can now sit, stay and heel. I also amazed my parents by learning how to give paw and by guessing which hand has a treat.

I was introduced to the dog park this summer and found lots of joy in running through the fields and the woods. I apparently still need to learn when to come when my name is called. Hey, it's not my fault there are so many cool things to sniff and chase.

I am still slightly skittish of new things and do not make much of a watch dog. I would rather lick the person coming in then bark at them. I guess I have shown my parents what a good dog I am because now I get to sleep outside of my crate on a doggie bed in the front room. I have only nibbled on two things that weren't my toys since I have joined my new home. One time it was to get my parents attention when they were too busy doing other things. Needless to say there were not pleased to find a paint brush chewed up on the floor.

I have a new addition to my collar. It's this thing that my parents turn on when I go outside. It blinks a light that I think might be red but I'm not sure since I'm color blind. Mom says it's because when it's dark outside it was very hard to find me in the backyard. Now they get a kick out of seeing me dart through the night with a flashing light.

Mom and Dad also think it's really funny when I take my raw hide and try and bury it in my doggie bed. (They caught me in the act! See the picture) I start by trying to flip the cushion over the bone. Eventually I give up and hide it in various places such as on the step, under the DVD holder, under the table or even on Mom or Dad. They especially laugh when I do that. Sometimes I don't understand why they laugh so hard at it. I am just trying to save my treat for later.

I sure do feel a part of the family. They even made me a custom stocking with what is supposed to look like me. They just can't seem to capture my cuteness on the stocking.

Thanks LEARN for finding my forever family!

Always,
Chloe (Jim and Kate too)
 
  
  Cocoa
Well, today is Cocoa's 1st birthday and we are so glad she's part of our family. Since we adopted her in February, we've met a lot of new "dog" people, which has been a great experience. She has grown to about 60 pounds and is a beautiful, active dog. She has a ton of new dog friends, and continues to meet more. Cocoa loves to swim in Lake Michigan at the Dog Beach (with all the other labs!), she's caught a few birds and squirrels (yeech) and has been adopted by at least 3 10-year-old boys (besides our own 10-year-old boy).

She's really, really fast, and as soon as I get time, I'm going to enroll her in an agility training class. She's wonderful exercise, because I walk her about 2 miles in the morning and another 1 in the evening. Cocoa loves to cuddle and if anyone is getting a hug, she jumps up to get in on the hugging. She passed her obedience class with flying colors, and continues to be a well-behaved dog, although she has been acting a little like a teenager lately, by chewing on things if she doesn't get a run/walk. I recommend the Lab Rescue group all the time when people admire her, so I just wanted to say thank you again for my lovely dog.

Jeanne Cuff & family
 
  
  Beast (formerly Yogi)
Well, it's been an exciting couple of months for me since I was adopted. I got to go on a long car ride home and man, do I love the car. My new family has two kids and an older dog and I love to play with all of them. I have a new name - Beast. They tell me I was named after the dog in the movie "The Sandlot," since they think I like balls as much as the dog in the movie.

The girl in the family loves to have me sleep on her bed and throw me the ball. The boy in the family was scared of me at first, but I have won him over with lots of affection. My new mom and I have become great friends. I don't beg and I love to follow her around looking for attention and make sure she always has company around the house. She thinks I am beautiful, except after I dive into the water dish. My new dad loves me to death, and I make sure I give him lots of kisses to let him know how much I like it at my new home.

Did you see the picture of me and my new buddy Clancy? He is an older guy (14) and we get along great. We don't go after each other's food and he is teaching me how to fetch the newspaper. He loves balls as much as I do. We play tug of war with the big rope and I am very gentle with him (sometimes I let him win). We go to a dog park together a couple of times each week. It's our favorite place. We get to run free in a big fenced park and play with many other dogs. We get great exercise and get to explore. Lot's of people tell me what a handsome guy I am and are very impressed at fast I am. I have met many other rescue dogs (labs, Great Danes, Boxers, Mastiffs, Greyhounds) and have a great time showing them how athletic I am.

Did I tell you how much I love to chase squirrels and chipmunks in our yard? I get to look out the windows into the yard and if I see one of those little guys, I whine and bark and cry until I am let out. And then I show them who is in charge of the yard. I haven't caught one yet, but they know who is the boss.

Please let everyone at LEARN know what a great new home I have and tell them thanks for all of their help!

Woof!
Beast
 
  
  Tony (formerly Smokey)
He's a wonderful dog and we love him to pieces. What a wild man, though! You may recall that we wanted a dog who would play with Jake. Well, we got one beyond our wildest dreams. The two of them run and wrestle like the couple of teenage brothers they are. It's a hoot to see one of them start a play date by coming up to the other with a sock or rope or ball in his mouth and challenge the other guy to try to take it away. 

Tony's true personality really comes out up at our cabin. He will swim for tennis balls till he drops - but come to think of it, he never has dropped yet - we just have to cut him off at some point to rest him. Then he returns to the water to entertain himself. He does what we call "shore patrol," digging things up from the bottom of the lake with his front paws, then sticking his entire head underwater, or even diving in deeper water, to retrieve his prize, which can be anything from a very large rock to his grand champion prize, a six-foot long piece of driftwood. He then brings these items proudly onto the dock and presents them to us. 

The other amazing water trick he does we call "harbor master." We have a two-person paddle boat which is docked with a loop of nylon rope attached to the center front. We couldn't believe it the first time he swam out to a returning paddleboat, seized the rope in his teeth and "swam" the two people in the boat back to shore. When he got into shallow water, he dropped the rope, turned around, picked the rope up again and backed the boat into the dock. 

He is a total lover, though. He loves to put his head on our laps for lots of pets and he puts up with kisses, too, now.  

Thanks for a great dog!
Jennifer
 
  
  Lulu (formerly Licorice)
I am a LEARN volunteer who had volunteered to dog-sit for a weekend because fostering made me nervous that I would adopt the first dog that I fostered. Lulu was found as a stray and had been at a shelter for quite sometime before LEARN saved her and put her in a foster home. Her foster home was going on vacation and need someone to dog-sit Lulu. Well, I baby-sat Lulu for 3 days, and I ended up adopting her - the exact reason that I was afraid of fostering. 

We have had Lulu for 4 month now and - we LOVE Lulu. She is a stinker and a trouble maker and sometimes I think she is mixed with Tazmanian-devil, but when she gives you a sweet little lick, while staring at you with her big, floppy, wobbly ears dangling from her little head, everything seems to be OK. Her improvements vastly outnumber her problems - although we still have problems to work on. Lulu excels when we are in our daily routine. She likes to know that everything is going to be the same day in and day out, it keeps her comfortable. It's days that we change something that she acts out, like destroys a couch cushion. 

When we first got Lulu, if you took her to the dog park and let her off leash - she would be gone and it would take an hour or two for you to find her AND be able to catch her!! Last Saturday, we went to the dog park. Lulu stayed within sight of us at all times, only leaving us once for a quick romp thru the woods, when she promptly returned - running towards me - not away from me - came right to me to sit and let me put her leash on. A small miracle if you met Lulu previously. 

Lulu has gained a little weight so you can't see her ribs anymore. Her coat went from a dull blackish-gray, that was shedding like crazy, to the most beautiful shiny black coat. She has more energy than anyone knows what to do with. We've found that the best and only way to REALLY tire her out is to tie her leash to my husband's bike and let her run - I just can't jog fast enough or long enough for her. Lulu is so fast that she has been clocked on my husband's bike at 19 miles per hour, and she was still pulling him!!! 

And is Lulu ever smart!!  She is too smart for her own good some days - with a little stubbornness mixed in. She graduated from obedience class, making a huge difference - even though she still likes to choose when she is going to listen to you. 

Lulu is a total mama's girl. I can't believe that we have lived without her! Every night that she curls up on my feet in bed, it warms my heart (and my feet) to know that we could make a difference in someone's life. And to see every day how much she loves us back is it's own reward. I hope that everyone who adopts gets to know that feeling. 

Thanks so much for taking the time and effort to save Lulu's life and to bring her into ours. 

Carmen, Shane and Roscoe Conant and of course, Lulu!!! 
 
  
  Harley
I was the recipient of one of your dogs in May of 2003 - Harley - the guy who had an eye problem and had been tied up in someone's backyard, who always barked and broke his chain and ran away. I am very pleased to report that Harley is 90 pounds - on a diet - likes to jump in my lap during thunderstorms, has learned all the basic commands, does very good off leash and sticks to me like my shadow. He is with me for at least 18 hours per day, actually more like 24 hours if you count sleeping. I couldn't get him to run away if I tried. He rarely barks and when he does it is usually when someone comes to the door, and not even then all the time. He is very sociable, likes small dogs (my daughter had a 8 lb. rescue something or another), cats (we have two of them and they can walk over him and he just rolls his eyeballs).

In summary, what I have for a dog is not the same dog that the previous owner said that he had. He was easy to train and all he wanted was some lovin'. He has gone with me to Texas, Arkansas and Colorado. As I did with my previous Lab, I have fixed up the back seat of my pickup into a bed with a foam mattress, blankets, quilts and pillows. I recently bought a new pickup and the window in the back seat are power operated so that I don't have to share my window with him.

Thank you for the runaway, barking dog that doesn't. Sorry I got so carried away telling you about him but he is a great find.
Warren Sommerfeld
 
  
  Brandy (formerly Jordan)
Brandy has added so much to our lives. It is hard to imagine life prior to Brandy. We adopted her in December 2000, and had challenges with her aggressive behavior. Fortunately, being a very quick learner, dog training classes helped us and her gain a mutual understanding of living together. Four years later, we are continuing to fine-tune training, such as learning to weave through our legs or waiting for the proper signal to fetch a toy. She is so happy, so affectionate, and makes a terrific companion. 

Brandy's family
 
  
  Molly 
Hi everyone at LEARN! This is Molly, and I was adopted in June 2003. I live in Illinois now, and have a nice big backyard to play in. My parents have found out a lot about me this past year. They bought me all these toys, but I don't play with anything but my tennis balls. They take me on walks almost every morning - I love to stop and sniff everything. They joke that my name should be Shadow because I follow them everywhere. I'm very loyal and sweet and they love me so much. 

I am a special needs dog - I have epilepsy. My family makes sure I go to the vet regularly to get my blood levels checked so I have as few seizures as possible. When I do have a seizure, they hold onto me and pet me, trying to calm me down and make me feel better. I think the seizures are harder for them than they are for me sometimes!

I just wanted to thank everyone at LEARN for finding me such a good home. My family loves me so much! 

Molly's family
 
  
  Kobie 
Hello everyone! It has been a little over a year since our house was made a home with the addition of Kobie. He has done well with the two cats that he lives with, he learned quickly which one he could be by and which one to avoid. He has made friends with the two labs that live next door, Winnie and Roxie.

He loves to chase balls and take sticks and play keep-away so that you have to chase him around the tree. He also loves to take your shoes and hide them around the house, he of course shows you that he has your shoes then takes off to hide them. I believe he thinks he is a little dog because he climbs up on your lap and crawls underneath the coffee table to get from one side of the living room to the other. He has also helped to motivate me into doing some home remodeling. I now know what people mean when they say Labs like to chew.

He completed his good canine citizenship last November after he showed the other dogs in the class that he was the teacher's favorite. The first day proved to be a little hard for him he became very excited around the other 7 labs that were in the class. 

I want to thank LEARN for helping us find Kobie. There is something about seeing those Labby eyes first thing in the morning that melts your heart. 

The Sanchez Family
 
  
  Lincoln
We adopted Lincoln and got him home on July 5th and we consider ourselves the lucky ones! We kept his name Lincoln and he is such a sweet boy! He has won over the hearts of all in our family and really all who meet him. He now loves the water, where before he did not go near it! He loves his "sister" Wrigley, our black Lab we had previously, you cannot separate them or they cry for each other, even when they are just getting their nails trimmed. 

Lincoln is a lover; he loves to be laid on and just loves people. Our neighbors 6-year-old son has been very afraid of dogs and he is now over his fear, thanks to Lincoln. He loves him! His parents are amazed. Lincoln is excellent with our kids and has taken to our trampoline, not a behavior we like, but he cannot help himself when the kids are on it. 

We are still training him to be good a leash, all though he just isn't on one enough. He loves the dog park at the Forest Preserve in Wauconda. He goes often with Dad. He is a great listener and is house trained and crate trained. He rarely barks, but if he does, we listen. We plan on fostering dogs for LEARN with our children are older. 

Lincoln's family
 
  
  Lambeau "Bo" (formerly Sammy)
Hi all! 

My name is Lambeau and I was adopted back in April of '04. In my foster home I was known as Sammy, but my new mom and dad like the Packers so much they changed my name to Lambeau - Bo for short. 

I've adjusted to my new name and life as a house dog very well. I've taken over the recliner and I love to sleep on it and snore so loud my mom and dad have to turn up the volume on the TV. Louie, the cat, and I have become fast friends. We play the game, I bark at you and you bat your paw at me. I push my limits with the garbage can, but I am learning. Who can blame me, dinning on all the food and paper towel I can get. 

In July I tore my ACL so I had to get surgery, but I'm doing great now. I will have a scar, but I'm so cute, no one will notice. When people come to the house I get so excited I do a happy hop in excitement. 

I want to thank everyone who helped me get a new home with a big doggy kiss and smile.

Tad, Kim & Bo
 
 

  
  Gabbie (formerly Abbie)
Hi there!
It's me Gabbie, formerly known as Abbie. I'm doing great in my new home. My Dad and Mom give me too much love and attention. I don't know if I'm going to like it too much when they have to go back to teaching! I even have a sister, Gracie, who has become my best friend. I'm getting bigger by the day, pretty soon Mom won't be able to pick me up and hold me like a baby! I'm the best puppy ever, at least that's what I'm told. 

I get to go on lots of walks and car rides, play on the tennis courts with Mom and Dad, visit with my doggie cousins, and swim in the water at the beach. Dad and Mom are having an in ground pool installed; Gracie and I can't wait to jump in! 

Thank you to L.E.A.R.N., Mary Beck, and the Spatz Family for your patience and making me the happiest puppy ever! 

Love, 
Gabbie (and Jim, Sarah and Gracie Roscoe) 
 
 

  
  Abby

Here are some pictures of Abby on our houseboat vacation in Voyageur's National Park (the boundary waters between the US and Canada). It was Abby's first experience in water. She's not a swimmer yet, but she overcame her fear of water and really got to where she loved splashing along the shore. 

At home, Abby loves going on walks and to the off-leash dog park where she can run full speed. That is a beautiful sight. My husband and I are so impressed with how calm Abby is around our elderly parents. She fell asleep at my mother-in-law's feet and my parents are always glad to see Abby come along to visit. So it is unanimous: everybody loves Abby. 

We are so grateful to LEARN and Nicole Scherer, her foster mother, for this wonderful member of our family, Abby. 

Warmly,
The Schuh Family
 
 

  
  Lucky

Lucky is doing GREAT. He is alive and well and living in northern Illinois. He has put on about 7-8 needed lbs and has helped his owner shed about 7-8 un-needed lbs. He walks about 3 miles a day and has learned to walk quite well on a leash. He goes to the local dog park almost every day where he is a legendary instigator, tricking other dogs into chasing him where only the quickest can even keep up with his speed and stamina. He enjoys swimming in the lake where he puts on a show for the local kids with his belly flops off of the pier and into the water. He has a talent for chasing down and catching a Frisbee, he will play fetch with a tennis ball 24 hours a day, any time/any place. He attended obedience school where the results were less than hoped for - probably due to the fact that he just wanted to play with the other canines in the class. He has learned many tricks but his specialty is bringing in the morning newspaper.

Although his name is Lucky, we are the fortunate ones as he has brought a great deal of renewed spirit into our lives.

Thanks to the LEARN organization for their fine work, special thanks to Brenda who is truly a life saver by rescuing Lucky and being his foster mom.

Best Regards,
Bruce & Debbie
 
 

  
  Lacey

Just wanted to update you on our recent adoptee, Lacey. Back in April our family fell in love with Lacey from your web site and our visits with her and her foster mom, Penny. Now, over three months later, we can't imagine life without her! 

After recovering well from having her puppies, she has thrived here and her coat is just gleaming. Lacey and our first dog, Rosie, are inseparable and love playing together at home and while at doggie daycare. 

We still are working on teaching Lacey the yard boundaries, but she is doing great in obedience class and learns fast. Lacey is so affectionate and full of personality! She always makes us laugh out loud with her antics and playfulness. 

We can't thank your organization or Penny, her foster mom, enough for giving her AND us this chance to come together. She is a special, wonderful dog and we are so lucky!! Thanks again! 

Todd and Kathy (& Rosie!!)
 
 

  
  Lady

Hello! I wanted to write a small letter to let everyone know how Lady is doing (AKA Tara).

We are in Washington state and have been here for a year now. Greg is deployed right now and will not be back until October. The boys are great. They're 4 and 6 now.

Lady is wonderful. She looooooooooves to play ball now. She could spend all day outside playing with a ball. Those Chuckit's come in real handy. She loves to swim - we take her to the lake whenever we have a chance too. She spends a lot of time with us over at grandmas and playing with the boys. She is still very calm and loving and all around wonderful. We are so glad that we have her and that L.E.A.R.N. gave us the opportunity to adopt her! I hope all is well with everyone.

The website looks great!! Keep up the good work! 

Lysa, Greg, Ian, Patrick, Lady Glaeser
 
 

  
  Copper

My husband and I adopted Copper from L.E.A.R.N. on May 6, 2004. He had just had his last dose of heartworm medicine on that day. We have since been back to the vet for a checkup and he is now heartworm free! 

Copper is such a great dog, we couldn't have made a better choice. He is so loving and such a joy. He seems to love his back yard, he loves to run. He must be a bird dog because he loves to chase birds, whether they are in the yard or flying over head. He is very good. He never counter surfs and never begs at the dinner table. He loves to be brushed and does a happy dance when he goes for a ride in the car. He loves to just sit and watch out our front window at what's going on outside.

The only bad habit he has is chasing flies in the house. He's like a bull in a china shop. But you have to give him credit - he does catch them! My two year old granddaughter absolutely loves him and he is so gentle with her.

We are attending obedience class on Monday nights. He wants so bad to play with his classmates but when class starts he does great.

Thank you so much for helping us to find Copper.

Kathy
 
 

  
  Chloe

Hey there! Remember me, Chloe, the lab mix with beautiful socks?!? Wanted to let you know that I have been at my new home for the last two months and I am having way too much fun. Thanks so much to my foster mom, Pat, for taking care of me and my pups. You have helped me developed into the wonderful dog that I am and placed me with my Mom and Dad. My new family takes me for walks twice a day! Sometimes I get to run around in a baseball diamond and other times I like to show off by stalking these weird things with wings. 

This thing called a family is great. I get two meals a day and treats for sitting at corners, for giving my paw and for lying down. I started classes with other dogs. I am so much better then they are. Ha! I will do anything for food. I just don't understand why a dog won't sit for a treat. Come on...free food! Sometimes I think that I am on my own again and I try to eat rocks, bugs and other things. My family looks at me weird but they don't understand how good it is! I am such a daddy's girl. My mom is cool but my daddy does all these fun things like letting me chase him around the house. Sometimes I get him to throw the ball and I will retrieve. Mom thinks I am funny. She gave me a bunch of these things she calls "squeaky toys." They look like stuffed bears or geese. She doesn't seem to understand why I chew them up in about two minutes. Needless to say she stopped buying them for me. I have tons of other toys but they are all red and rubbery. I call them all Kong. I can't seem to find out how to destroy them. That seems to make Mom and Dad happy. I also have a best friend call "Buddy Ball." You can see "Buddy Ball" in my pictures. It is my favorite toy! 

I don't have any brothers and sisters but I have two cousins and a friend. Jessie is a cocker spaniel that Mom grew up with. Jessie doesn't seem to like me and I am not sure why she wouldn't want me to jump on her and steal her toys? I am just trying to play! Archie is my other cousin, a Rat Terrier, and he likes to play but only his way. But my favorite is my friend Raven and Malcom. Raven is a huge black lab and Malcom is a mix like me. We like to wrestle and play tug of war. I enjoy my time with my family and friends. Well thats it for now! 

Chloe (Jim and Kate too)
 
 

  
  Lucie

Just wanted to update you on Lucie who we adopted 6 weeks ago. Lucie had knee surgery and has been somewhat confined for the six weeks that she has been with us so that her leg could heal properly. Being that she is only 2 and 1/2 years old you can imagine how frustrating it has been for not only us but for her also due to the fact that she wanted to play and run but she was not allowed to. She also wants to be with us as much as possible, so living in a two story home made it difficult when we would go to bed for the night, but we all made it through. As of today the vet has given Lucie her complete recovery. 

We would like to thank L.E.A.R.N. for letting us adopt Lucie. She is not only pretty, but she is very smart. She has fit into our lifestyle quite well. So well in fact she has become EXTREMELY comfortable in her new home. Again, thanks to both you and L.E.A.R.N. for putting us together.

Mark and Linda Kobylanski
 
 

  
  Hallie

Hi everyone. I'm Hallie, the mother of seven puppies, who was rescued by LEARN back in October. I've been with my family since the beginning of February. I love it here in my new home. I have a Mom, Dad, a brother (Mike) and a sister (Colleen) and two feline pals, Mercedes and Tigre. Tigre has actually become my best friend. Mom and Dad say we are partners in crime as we tend to get into mischief together. 

I get to go on lots of walks and car rides and I have a great backyard to play in. I sometimes even get to go to work with Mom! Now that the weather is getting warmer, Mom is going to take me to the beach to see if I will like swimming. My family hopes that I do, since they plan to take me to their lake house in the Michigan UP this summer.

I'm still hesitant about meeting new people and dogs, but Mom is working on this with me and my confidence level is increasing. Also, now that I've been with my family for a while, Mom has started taking me to obedience classes. The trainer says I'm a very smart dog and will do well. Mom read me the success stories about 2 of my puppies, Jake (formerly Frosty) and Jake (formerly Albert). I was happy to hear that they are doing well. I hope the other 5 are also happy in their new homes. Thank you to Pat (my foster Mom) and LEARN for taking a chance on me, for taking care of me and my puppies and for matching me with my forever family. My family and I are so grateful. Thanks again.

Love,
Hallie Leider and her family: Mike, Caroline, Mike (brother) and Colleen (sister)
 
 

  
  Jake (formerly Frosty)

I was one of Hallie's puppies. My name was Frosty - my adopted family thought that I looked to much like a bear to have that name. They voted on Jake and that is now my new name. I will be six months old soon and I have been very happy. I hardly have any more accidents and I just love my Mom. (She calls me her "Jakie puppy"). When she is home I follow her everywhere. She takes me for walks everyday and get to play with "my kids" Jesse, Andy, & Jenny. I have even gone to visit my Nana's dogs (two wire-haired terriers) three times. We get along great and have fun running around her house. I have gotten bigger than them but we still play well together. My family is great and I am very happy. I have gotten used to the routine (this house if very busy) and have even visited Andy at his baseball practice. All the kids in Jenny's kindergarten class know who I am and always ask her about me. I am a bit of a klepto and will steal anything that I can get. All the kids have learned to keep doors closed to keep me out of their stuff - but the second that door is open - Look Out! I have also developed a thing for stealing Kleenex - which my family is trying to break me of. I just love to steal it and shred it up to make a mess. I have made friends with one of the cats that live next door to me but I can not seem to understand why he can't bark - but I sure whine and cry when I know he is outside without me. I LOVE to be outside in the yard - especially when the whole neighborhood is out - I have tons to watch and guard. I have attached a picture of myself from about a month ago. I just want to thank LEARN for rescuing my mom, Hallie, and finding a perfect family to love me. 

Love, Jake Chorley (and Jeff, Allison, Jesse, Andy, & Jenny too)
 
 

  
  Spencer (formerly Heath)

HI GUYS!
It's me, wonder dog, better known as Spencer. I thought it would be a good idea to let you know I have exceeded all my expectations of being the new super hero "wonder dog." Well, according to my parents, I have! They are very happy with me, as they say, but I don't think they are kidding, I am sure they love me like a son. I think I have made them very happy, me too! As I send this e-mail, my daddy is wondering how I could hit such small keys with my big paws. The wonders of nature shall never be known to the parents of such brilliant dogs. Thanks for everything.
 woof, woof,
 Spencer
 
 

  
 Jake

The newest member of our family is Jake; he is a healthy, happy 4 month old puppy who came to live with us 2 months ago. When we adopted Jake, his name was Albert; Pat and her kids called him Albie, which was really cute. Our daughter's name is Allie, however; the names were too close, so we changed Albie's name to Jake. 

We got a little lover when we took Jake home! He loves to cuddle and lick and ask for pets - and we give the love right back. He walks with me and my children to school every day; he doesn't want to leave the kids! He knows when it's time to pick them up, too - he's ready for the walk back to school to see all the kids again! And all the kids coming out of school look for him, too - he's so great with kids and other dogs. And cats - he doesn't get that the cat next door doesn't like him...he keeps trying to win her over! 

We keep trying to figure out what Jake's mix is - his mother was a dainty yellow Lab, so we know there's Lab in him, but with his little bat-wing ears, and his small size, we're just stumped! All we know is that he's the perfect dog for us, and we wouldn't trade him for anything. Thank you for helping us find the puppy that fits our family to a "T"!
 
From the Stahmer family - Jeanne, Warren, Kevin and Allie 
 
 

  
 Kasey 

I just wanted to give you an update on a dog we adopted back in November of 2002. Her name was formally Lucy, and now we have changed it to Kasey.

She is an adorable lab and loves to cuddle with anyone and everyone. She gets really excited whenever anyone new comes over and runs around the house as fast as she can. She makes us laugh so much with all the crazy things she does. She doesn't bark unless you have her kong. At first she would not even go into the water and now we can't keep her away from it. As soon as she is near a lake, puddle, or river she is in the water exploring. She is an excellent retriever and just needs a little help to bring her toy back to you. We have used an e-collar to keep her on track. 

She also loves snow. She jumps and plays in it whenever we get it. She also loves eating snow. She will bury her head in the snow just to get a mouthful and she will also drag her jaw to scoop up a little snow for refreshment on our walks. 

Her favorite toy is a black kong which is the only toy she gets because everything else is torn apart. Rawhides last about 10 minutes, even with the big ones. A black kong usually lasts about 2 months because she will chew on it for hours at a time. My wife and I are now immune to the squeaking and sucking noise she makes with them.

However I would like to mention all the things she has eaten in the past year we have had her. I figure this will provide a little humor to anyone who has labs. They include: two pillows, a cable remote, a cordless telephone, two text books from college, two stuffed teddy bears, numerous plastic cups, a night light, a cross-stitching my wife was making, a pair of sandals, a whole bag of Oreo cookies, a labrador calendar, numerous envelopes, Kleenex and she also dispenses her own toilet paper. All of these things we never gave her or intentionally left out she just would grab them when we were doing something else. She doesn't have separation anxiety, we just think she is just a dog with lots of attitude.

Besides all she has eaten, (well at least me, I can't always speak for my wife) we still love her very dearly and she keeps us very busy. We are just glad to be able to give Kasey a good home.

Nick and Debbie Bednarek
 
 

  
  Cedar

Hello to all from Cedar. It is approaching a year since we were able to bring Cedar home. He has adapted well to being indoors and loves playing with his new sister Snickers. He has no lack of energy and continually wants to play ball, sock or just crawl in your lap. His personality is always on and he is always looking to go have fun. He has filled in wonderfully and is at a perfect weight. Carolyn and I would like to thank the volunteers at LEARN for rescuing this wonderful animal. For if it were not for their combined efforts we would not have such a wonderful addition to our family.
We have a couple of pictures to share. One is of his first Christmas enjoying life. Thanks again.

Sincerely,
Mike, Carolyn, Snickers and Cedar Agee
 
 

  
  Dakota

In April of 2002, we were fortunate to become the new family for "Decorah," now known as Dakota. We had had labs for many years and had just recently lost our previous lab, Carson, to cancer. After deciding that the house was far too quiet without a dog, we began the search for a new family member. We learned of LEARN through a local newspaper ad. After viewing the website, we fell in love with Decorah/Dakota and began the adoption process. We appreciated the thorough job that LEARN did in assessing our suitability as Decorah's new family, and were thrilled when she became ours. (Or did we become hers?) She started out well in life but, due to divorce, went to another family member, then to LEARN, then to us. A lot of change for a then 10 month old puppy, but she adapted wonderfully to everything, including her name change. 
This has become Dakota's house. She quickly determined, as so many self-respecting labs do, that the bed is a much better place to sleep than the floor. She adores her human brother and sister, and has mom and dad wrapped around her paw. She has also come to love horses and looks forward to going to horse shows and to visiting the barn. (She did run the other way the first time she saw a horse, however. It was quite funny.) While seemingly a tom boy, more like a bull in a china shop at times, she thoroughly amazed us last summer when trying to "protect" our young nephew while he was swimming in the pool. Completely frantic, she kept putting herself between our nephew and the pool when he was on the deck, and actually jumped into the pool 3 times in order to try to grab him and pull him to safety! She'd never shown any interest in the pool before this, nor since. 

She's such a good girl, and we're very lucky to have her. Thank you, LEARN, for taking her into foster care (Joanne, we love her as much as you did), and for allowing us to give her a wonderful forever home. If we ever decide to add to the family again, we'll definitely be in touch. 

Sincerely, 

 
 Now available on DVD and VHS from Kino 

"Robot Stories and More Screenplays" paperback now available at Amazon.com 

"...I'd take a chance on Greg Pak's micro-budget indie Robot Stories, a valentine from the future. Broken into a quartet of subtle science-fiction fables, each is more Ray Bradbury than Keanu Reeves -- and romantic in its own strange way: A sculptor becomes obsessed with the digitized memories of his wife; a worried couple tries to love an android baby; a mother reconstructs her dying son's vintage robot-toy collection; and an android office-drone (played by Pak) falls in love. Somehow the cast (largely composed of underutilized Asian-American actors like Tamlyn Tomita) underplays the gizmos and hits the emotional beats dead-on, heating up a genre that so often looks stylish, but feels dead-cold."
- Logan Hill, Nerve.com

"Enthralling."
- Cincinnati CityBeat

"Extremely great."
- Harry Knowles, AintItCoolNews.com

"A tremendously powerful set of vignettes. Writer/director Greg Pak uses four stories about robots to create wonderfully human moments. The stories are quick and to the point. The visually compelling and well-acted stories do not exist in the same world; tied together by theme, each is its own stirring narrative. Robots as toys, robots as children, robots as office tools, and robots as deceased loved ones teach the humans around them various lessons without hammering the audience. Quiet subtlety abounds throughout these terrific shorts. Even the opening credits animation is a wonderfully self-contained story. Another must-see. "
Bobby Kirk, Playback St. Louis

NPR.org | NPR's RSS PageRSS Help
RSS Feed
NPR Columns: Movie Reviews
Movie Reviews

How do I subscribe to this RSS feed?
If you use an online service like My Yahoo!, Google Fusion, My MSN, Bloglines or Newsgator, please click on a button below to subscribe to this feed.

................
Or, copy the URL in the box below into your preferred RSS reader. New content will be delivered as it's published. A list of common RSS readers is available at NPR's RSS page.

Feed Contents:
'United 93': Recent Painful History on Film
United 93 takes an unflinching look at the events surrounding the hijacking of United Airlines Flight 93 on Sept. 11, 2001. The plane's passengers rushed the terrorists, leading to a fatal crash in a Pennsylvania field.
Fri, 28 Apr 2006 22:09:00 EDT

Slate's Summary Judgment: 'United 93,' 'Akeelah and the Bee,' 'RV'
Slate contributor Mark Jordan Legan reviews what critics are saying about this weekend's movie releases -- United 93, Akeelah and the Bee and RV.
Fri, 28 Apr 2006 13:00:00 EDT

Greengrass Tells Story of 'United 93'
Film critic David Edelstein reviews United 93, the dramatized version of events on the plane that crashed into a field in Pennsylvania after being hijacked Sept. 11. Learning that other planes had been flown into the World Trade Center and the Pentagon, passengers fought back for control of the plane.
Fri, 28 Apr 2006 12:40:00 EDT

Events of 'United 93' Terrifying on Screen
United 93, a movie about passengers who fought for control of a hijacked plane on Sept. 11, is hard to get out of your mind, no matter how hard you try, according to Los Angeles Times and Morning Edition film critic Kenneth Turan.
Fri, 28 Apr 2006 11:37:00 EDT

Breathtaking Eco-Thriller: 'Kekexili'
High in the mountains of Tibet, a life-and-death struggle has been raging nearly unnoticed for decades. It involves roving groups of poachers, a small band of volunteers, and antelope that once numbered in the millions. The story inspired a film, Kekexili.
Tue, 25 Apr 2006 16:20:00 EDT

'Look Both Ways'
Film critic David Edelstein reviews the new Australian film Look Both Ways, which was written and directed by Sarah Watt.
Fri, 21 Apr 2006 13:08:00 EDT

Slate's Summary Judgment: 'Silent Hill,' 'The Sentinel,' 'American Dreamz'
Slate contributor Mark Jordan Legan wraps up what critics are saying about the weekend's movie releases: Silent Hill, The Sentinel and American Dreamz.
Fri, 21 Apr 2006 13:00:00 EDT

'American Dreamz' is Amusing, Not Bitter, Satire
Los Angeles Times and Morning Edition film critic Kenneth Turan reviews American Dreamz, directed by Paul Weitz. It stars Hugh Grant as a Simon Cowell-like host of an American Idol knockoff. It's is satire that takes on pop culture and politics.
Fri, 21 Apr 2006 10:32:00 EDT

'Hard Candy' Indeed: A Dark Thriller
Hard Candy is a thriller that wraps issues of pedophilia, torture, and vengeance into a slick -- but sick -- little package.
Fri, 14 Apr 2006 19:27:00 EDT

'The Notorious Bettie Page': Too Wholesome?
The Notorious Bettie Page explores the life of the 1950s pin-up queen, who posed for under-the-counter men?s fetish magazines, and for photos and films for private collectors into the bondage scene.
Fri, 14 Apr 2006 13:39:00 EDT

'The Notorious Bettie Page' Lacks Substance
The Notorious Bettie Page is a movie based on the life of the pinup girl whose legendary poses transformed her into an icon. Los Angeles Times and Morning Edition film critic Kenneth Turan calls it an "empty film."
Fri, 14 Apr 2006 13:36:00 EDT

Studios Catch the Growing Animation Wave
As the summer movie season nears, so does an impending tidal wave of computer-animated movies. But NPR's Bob Mondello wonders whether it will swamp what's fast becoming Hollywood's most lucrative genre. Ice Age 2: The Meltdown brought in more than $70 million in three days.
Fri, 07 Apr 2006 18:31:00 EDT

'Friends with Money'
Film critic David Edelstein reviews Friends with Money, the new film by writer/director Nicole Holofcener who wrote and directed Lovely & Amazing and Walking and Talking.
Fri, 07 Apr 2006 13:10:00 EDT

Acerbic and Sweet: 'Friends with Money'
Nicole Holofcener's Friends with Money is a "modern comedy of manners," according to Morning Edition and Los Angeles Times film critic Kenneth Turan. The film features Catherine Keener, Jennifer Aniston, Frances McDormand and Joan Cusack as four Los Angeles women who have issues with love and money.
Fri, 07 Apr 2006 13:08:00 EDT

'Brick' Takes a Clever Look at Suburban Adolescence
Actor Joseph Gordon-Levitt plays a young outcast trying to find out what happened to his ex-girlfriend in Brick, a hard-boiled noir set in a suburban high school. 

Terms of Use
Updated August 31, 2005

Introduction; Consent to Terms
Please read these Terms of Use before using NPR.org, nprjazz.org or freshair.npr.org or any features of these sites, including, but not limited to, the NPR Shop, NPR RSS feeds and NPR Podcasts (collectively, the "NPR Web sites" or the "Service"). By using the Service, you agree to be bound by these terms of use. If you do not agree to these terms of use, please exit the Service. We reserve the right, at our discretion, to modify, add or delete portions of these terms at any time by posting updated terms of use on the NPR Web sites. Please check these terms frequently for updates. Any modifications, additions or deletions to these terms shall be effective immediately upon posting of updated terms of use. Your continued use of the Service following the posting of updated terms of use will mean that you agree to those changes.

Copyrights
The contents of NPR Web sites are protected by U.S. and international copyright laws. You may not reproduce, distribute, transmit, display, prepare derivative works, or perform any copyrighted material on the NPR Web sites without the prior written consent of NPR, except as provided below. You may copy and print a limited amount of content, and you may download NPR Podcast content, for your personal, non-commercial use only, provided that you include all copyright and other notices contained in the content and that you do not modify the content. Any other use of NPR content requires prior written permission from NPR. For permission requests, visit our permissions page. For information regarding the purchase of transcripts or tapes/CDs, visit our transcripts page.

Trademarks
National Public Radio , NPR  and NPR program names are service marks of either National Public Radio, Inc. or the producers of the programs All rights reserved. You may not use any NPR service marks, logos or graphics without NPR's prior written consent, except that you may use the NPR-provided logo contained in an NPR RSS Feed in connection with the personal, noncommercial use of NPR's RSS Feeds, subject to the requirements set forth below in NPR's RSS Feed policy.

Framing
NPR does not allow framing of its Web sites. 

Links to NPR Web Sites
NPR encourages and permits links to content on NPR Web sites. However, NPR is an organization committed to the highest journalistic ethics and standards and to independent, noncommercial journalism, both in fact and appearance. Therefore, the linking should not (a) suggest that NPR promotes or endorses any third party's causes, ideas, Web sites, products or services, or (b) use NPR content for inappropriate commercial purposes. We reserve the right to withdraw permission for any link.

Links to Third Party Sites
NPR has provided links to Internet sites maintained by third parties, over which NPR has no control. NPR does not endorse the content, operators, products or services of such sites, and NPR is not responsible or liable for the content, operators, availability, accuracy, quality, advertising, products, services or other materials on or available from such sites. NPR shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with use of or reliance on any such content, products or services available on or through such sites.

RSS Feeds
NPR's RSS feeds, provided by NPR at www.npr.org/rss/, consist of a selection of headlines, summaries and links to full news stories, which can be delivered to your desktop using a technology called XML (extensible Markup Language) (individually, an "RSS Feed" and collectively, the "RSS Feeds"). The RSS Feeds are protected by U.S. and international copyright laws. All rights in and to the RSS Feeds are reserved to NPR. The RSS Feeds are available for personal, noncommercial use only. You may display, excerpt from, and link to the RSS Feeds on your personal web site, weblog, or similar application for personal, noncommercial purposes, provided that (a) the links redirect the user to the NPR Web sites when the user clicks on them, (b) the use or display does not suggest that NPR promotes or endorses any third party causes, ideas, Web sites, products or services, (c) the fundamental meaning of the content contained in the RSS Feeds, including the headlines and summaries, is not changed or distorted, and (d) you do not modify the news stories or other content that are linked to by the RSS Feeds. If you display an entire RSS Feed on your personal web site, weblog or similar application, (a) you may not redistribute the RSS Feed, and (b) you must provide attribution to NPR adjacent to the RSS Feed, by including "NPR News Headlines" in text or by displaying the NPR logo contained in the RSS feed, without modification. Any other use of NPR's trademarks, or service marks, or of the RSS Feeds requires the prior written permission of NPR. For permission requests, please visit our permissions page. For use of the news stories or other content that are linked to by the RSS Feeds, please see the Copyright section of these Terms of Use. NPR reserves the right to discontinue providing RSS Feeds and to require that you cease accessing or using the RSS Feeds, any content contained in the RSS Feeds, and/or NPR logo at any time for any reason.

NPR Podcasts
NPR provides podcasts (the "NPR Podcasts") consisting of selected audio content from NPR and other content providers, available at www.npr.org/podcasts/ that is provided over the Internet using an XML feed and an associated audio file so that the audio file may be downloaded and played from a user's computer or transferred to a portable listening device. NPR Podcasts are protected by U.S. and international copyright laws. All rights in and to the NPR Podcasts are reserved to NPR or the content provider. NPR Podcasts are available for personal, noncommercial use only. You may download, copy and/or transfer to a portable listening device the NPR Podcasts for your personal, non-commercial use only, provided that you do not modify the content. You also may link to NPR Podcasts from your web site, weblog or similar application, as long as the linking does not (a) suggest that NPR promotes or endorses any third party's causes, ideas, Web sites, products or services, or (b) use NPR content for inappropriate commercial purposes NPR reserves the right to discontinue providing NPR Podcasts and to require that you cease accessing or using the NPR Podcasts, or any content contained in the NPR Podcasts, at any time for any reason.

Use of the Service
If you use the Service, including by participation in the NPR Discussions or Chat Areas of the NPR Web sites or by submission of essays, e-mails or other information via NPR.org, you must follow a few simple rules: 
1. You may not post, upload or transmit any messages or materials anonymously or under a false name or a false email address. 
2. While we do not seek to limit expression of ideas, certain materials and language will not be tolerated. You may not post, upload or transmit any material or links to material that is libelous, defamatory, false, obscene, indecent, lewd, pornographic, violent, abusive, threatening, harassing, discriminatory, in violation of the law or that constitutes hate speech. 
3. You may only post, upload or transmit materials for which you have the copyright or other permission to distribute electronically. You may not violate, plagiarize, or infringe on the rights of third parties, including copyright, trademark, trade secret, privacy, personal, publicity, moral or proprietary rights. You promise and represent that any posted, uploaded, included or transmitted materials will be owned by you or in the public domain. 
4. The Service can only be used for non-commercial purposes. You cannot distribute or otherwise publish any material containing any solicitation of funds, advertising or solicitation for goods and services. 
5. You may not post, upload or transmit any software or other material which contains a virus or other harmful code or device. You may not use the Service to distribute chain letters, mass mailings or "spam" or to gather e-mail addresses for the purpose of sending "spam" to other users of the Service. 
6. By posting, uploading or transmitting any material on or through the Service, you hereby grant NPR a non-exclusive, perpetual, royalty-free, worldwide license to use, copy, sublicense, modify, transmit, publicly perform or display that material, and your name and location (city, state and/or country), if you provide that information to us, in connection with that material, in all media, whether or not now known. 
7. Persons under the age of eighteen (18) MUST obtain parental permission before registering for the NPR Discussion Boards or Chat Forums or posting, uploading or transmitting material on or through the Service. Children under the age of thirteen (13) are not eligible to participate in the NPR Discussions or chats or otherwise post, upload or transmit material on or through the Service. 
8. You are solely responsible for any content posted, uploaded or transmitted using your account. NPR does not and cannot review every message posted by you and is not responsible for any materials posted by others in the NPR Discussions or Chat Forums or on or through the Service. However, NPR has the right (but not the obligation) to delete, edit or move any material that it deems, in its sole discretion, to be in violation of the Rules of the NPR Discussion Boards or Chat Forums or these Terms of Use, or which it otherwise, in its sole discretion, considers to be objectionable. 
9. NPR is not responsible or liable for any materials posted in the NPR Discussions or Chat Forums or posted, uploaded or transmitted on or through the Service. NPR cannot verify the accuracy of statements that users place on the NPR Discussions or Chat Forums or on or through the Service, and does not guarantee that any material has been posted, uploaded or transmitted with the permission of the copyright or proprietary owner or is otherwise in compliance with the Rules of the NPR Discussion Boards or Chat Forums or these Terms of Use. 
10. By using the Service, you agree to abide by these rules. You acknowledge that NPR has the right to delete, edit or move material that it deems, in its sole discretion, to be in violation of the Rules of the NPR Discussion Boards or Chat Forums or these Terms of Use, or which it otherwise, in its sole discretion, considers to be objectionable. You further acknowledge that NPR has the right to limit or terminate access to the NPR Discussions or Chat Forums for any reason, including violation of the Rules of the NPR Discussion Boards or Chat Forums or these Terms of Use. 

Representations; Indemnity
You represent and warrant that (a) any material you post, upload or transmit on the NPR Discussions or Chat Forums or on or through the Service does not and will not violate any Rules of the NPR Discussion Boards or Chat Forums or these Terms of Use, and (b) your use of the Service complies with these Terms of Use. You agree that you will indemnify and hold harmless NPR and its directors, officers, employees, members and licensees from any and all claims, liabilities, costs or expenses, including reasonable attorneys' fees, arising from (1) your breach of any of the above representations and warranties, (2) your use of the Service, (3) the materials you have posted, uploaded or transmitted in the NPR Discussions or Chat Forums or on or through the Service, or (4) NPR's publication, distribution or use of such materials.

Disclaimer
THE SERVICE AND ANY PRODUCTS SOLD ON OR THROUGH THE SERVICE ARE AVAILABLE ON AN "AS IS" AND "AS AVAILABLE" BASIS, WITHOUT ANY WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OF TITLE OR IMPLIED WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. NPR DOES NOT WARRANT THAT THE SERVICE WILL BE UNINTERRUPTED OR ERROR-FREE. NOR DOES NPR MAKE ANY WARRANTIES AS TO THE RESULTS THAT MAY BE OBTAINED FROM USE OF THE SERVICE OR AS TO THE ACCURACY, RELIABILITY OR CONTENT OF ANY INFORMATION, SERVICE, PRODUCTS, OPINIONS, OR STATEMENTS AVAILABLE ON OR THROUGH THE SERVICE OR THROUGH LINKS ON THE SERVICE. USE OF THIS SERVICE IS ENTIRELY AT YOUR OWN RISK. 

LIMITATION OF LIABILITY
IN NO EVENT WILL NPR, ITS DIRECTORS, OFFICERS, EMPLOYEES OR MEMBERS BE LIABLE FOR ANY INDIRECT, CONSEQUENTIAL, SPECIAL, INCIDENTAL OR PUNITIVE DAMAGES, ARISING OUT OF THE USE OR INABILITY TO USE THE SERVICE OR RELATING TO ANY PRODUCTS SOLD ON OR THROUGH THE SERVICE. IN NO EVENT WILL NPR, ITS DIRECTORS, OFFICERS, EMPLOYEES OR MEMBERS BE LIABLE FOR ANY AMOUNT IN EXCESS OF $100. 

Online Shopping
In the event a product available on or through the NPR Shop is listed at an incorrect price or with incorrect information, NPR will have the right to refuse or cancel any orders placed for product listed at the incorrect price. Your receipt of an electronic or other form of order confirmation does not signify NPR's acceptance of your order, nor does it constitute confirmation of NPR's offer to sell. NPR reserves the right at any time after receipt of your order to accept or decline your order for any reason. NPR may charge and withhold the applicable sales tax for orders. Otherwise, you are solely responsible for all sales taxes, or other taxes, on orders shipped to you. 

Copyright Agent
NPR respects the intellectual property rights of others. If you believe that your work has been copied in a way that constitutes copyright infringement, please provide the following information in writing to NPR's Copyright Agent: 
1. a physical or electronic signature of a person authorized to act on behalf of the owner of the copyright. 
2. a description of the copyrighted work that you claim has been infringed. 
3. a description of the material that you claim to be infringing or to be the subject of infringing activity and that is to be removed or access to which is to be disabled, and information sufficient to permit us to locate the material. 
4. information so that we can contact you, such as address, telephone number and e-mail address. 
5. a statement that you have a good faith belief that use of the material in the manner complained of is not authorized by the copyright owner, its agent, or the law. 
6. a statement that the information in the notification is accurate and, under penalty of perjury, that you are the copyright owner or are authorized to act on behalf of the owner of a copyright that is allegedly infringed. 

NPR's Copyright Agent can be reached as follows:
Michelle Shanahan
Copyright Agent
National Public Radio
635 Massachusetts Avenue, NW
Washington, DC 20001
Phone: (202) 513-2040
Fax: (202) 513-3021
Email: ogcstaff@npr.org


International Users
The Service is controlled and operated within the United States. NPR makes no representation that content, materials or products available on or through the Service are appropriate or available for use outside of the United States. If you access the Service from a location outside the United States, you are responsible for compliance with applicable laws, including U.S. export laws and regulations. 

Miscellaneous
These terms of use, together with the Privacy Policy, represent the entire understanding of the parties regarding the use of the NPR Web Sites and supersede any previous documents, correspondence, conversations, or other oral or written understanding related to these terms of use. These terms of use shall be governed by and construed under District of Columbia law without regard to its choice of law rules, and, where applicable, the laws of the United States. To the extent permissible by law, any disputes under these terms of use or relating to the Service shall be litigated in the local or Federal courts located, and you hereby consent to personal jurisdiction and venue, in the District of Columbia. A modification or waiver of a part of these terms of use shall not constitute a waiver or modification of any other portion of the terms of use. If for any reason any provision of these terms of use is found unenforceable, that provision will be enforced to the maximum extent permissible, and the remainder of the terms of use will continue in full force and effect. 

Questions
If you have any questions about the terms of use, please contact NPR. 
Click here to download high resolution stills from the film for your publication or website. 
 

"...I'd take a chance on Greg Pak's micro-budget indie Robot Stories, a valentine from the future. Broken into a quartet of subtle science-fiction fables, each is more Ray Bradbury than Keanu Reeves -- and romantic in its own strange way: A sculptor becomes obsessed with the digitized memories of his wife; a worried couple tries to love an android baby; a mother reconstructs her dying son's vintage robot-toy collection; and an android office-drone (played by Pak) falls in love. Somehow the cast (largely composed of underutilized Asian-American actors like Tamlyn Tomita) underplays the gizmos and hits the emotional beats dead-on, heating up a genre that so often looks stylish, but feels dead-cold."
- Logan Hill, Nerve.com

"Enthralling."
- Cincinnati CityBeat

"Extremely great."
- Harry Knowles, AintItCoolNews.com

"Pak sets himself up as a filmmaker to watch, a creator of intelligent, thoughtful stories that refuse to be contained by traditional definition. Which is exactly what we need in our movies."
- HollywoodBitchslap.com

"Robot Stories is a rare breed of anthology that managed to combine the ideas of science fiction, the down to earth nature of the indie film, and more humanity than most of what you ll find on shelves today."
- DVDTalk.com

"[The film's] four loosely interwoven stories about robots, parents, children and toxic workplaces contain enough intriguing notions about artificial intelligence and human relationships to fuel at least an hour's worth of happy post-movie geeking out."
- Now Toronto

"... these four segments cumulatively offer something much more satisfying than most sci-fi blockbusters. George Lucas could learn something from Pak and his predominantly Asian-American cast about creating credible characters."
- Anthony Allison, Las Vegas Mercury

"Written and directed by talented novice Greg Pak, Robot Stories is composed of four separate, exemplary tales about the nebulous boundary between man and machine. An independent production devoid of expensive, and unnecessary, special effects, it offers crisp glimpses into a near future in which the intelligence is artificial but the emotion is real."
- Steven G. Kellman, San Antonio Current

"Director Greg Pak's 'Robot Stories' is the most entertainingly humanistic robot film since 'Metropolis,' and anyone who tells you otherwise is probably a replicant and not to be trusted around small animals and electrical outlets."
- Marc Savlov, Austin Chronicle

"One of the most human and humane science fiction films I've seen in a long time."
- Kathi Maio, Magazine of Fantasy & Science Fiction

"A smart, low budget meditation (or meditations) on love, loss, family, and community, writer director Greg Pak's anthology borrows elements from Ray Bradbury, Phillip K. Dick, and other sci-fi visionaries, but places them in a recognizable, down-to-earth context... 'Robot Stories' isn't slick, isn't gimmicky. These are tales from the heart - pulsing to a high-tech beat."
- Steven Rea, Philadelphia Inquirer

"By turns funny, melancholy and incredibly moving."
- Katie Haegele, Philadelphia Weekly

"It will be interesting to see what [director Greg] Pak can do with a film that gets a little more funding and wider release. Then again, a big studio and a big budget wouldn't have improved 'Robot Stories.' It's fairly close to perfect already.'"
- Peter Hartlaub, San Francisco Chronicle

"Built around the themes of love, death, family, and of course robots, Korean director Greg Pak's 'Robot Stories' beautifully styles four tales. Through narratives both hilarious and touching, humans are forced to interact with robots in a way that eerily reflects the growing influence technology has on our lives... Each story is stunningly executed and moving in its own right."
- Melissa McCartney, San Francisco Bay Guardian

"Forget 'Hellboy.' 'Robot Stories' is the real deal -- a science-fiction with a brain and a heart."
- Ed Blank, Pittsburgh Tribune-Review

"Quiet subtlety abounds throughout these terrific shorts... For fans of independent films, or people that just like good movies, Robot Stories is a must-see."
- Bobby Kirk, Playback St. Louis

"Each of the stories, impeccably staged and acted, has just the right length... Never allowing preciousness or ponderousness to infuse the material, filmmaker Pak demonstrates a real talent for concise storytelling marked by poignancy and humor."
- Frank Scheck, Hollywood Reporter

"... this is a heartfelt endeavor, given weight by [Sab] Shimono's extraordinary performance, in which the actor uses the subtlest flicks of his weary brow to call forth torrents of sorrow and minefields of regret."
- Scott Foundas, LA Weekly

"A new candidate for the Robotic Hall of Fame."
- Gerald Peary, Boston Phoenix

"Director-writer Greg Pak's quartet of short films about the intersection of mechanized men with human nature is sensitively directed, beautifully acted and - unusual for most science-fiction movies - gracefully rendered."
- Gene Seymour, Newsday

"Each fable concludes with a lovely, enduring final image that resonates like the last sentence of a powerful short story."
- Jordan Reed, Box Office

"Ostensibly about artificial life forms, each of these four short, expertly crafted stories offers a poignant perspective on what it means to be human... Following in the footsteps of Ray Bradbury, Rod Serling and Philip K. Dick rather than George Lucas, Pak returns to the tradition of intelligent, humanistic sci-fi and reminds us of the value of good genre fiction."
- Ken Fox, TV Guide

"Mr. Pak's stylized stoicism is discernible in 'Machine Love,' the segment in which he can be seen as Archie, an android office worker who gamely endures the ridicule of the other drones around him because of his single-mindedness. With a clever economy of means, 'Machine Love' conveys the blooming of feelings inside the android, who is slowly developing an attachment to a woman who works nearby. The director is using the material to joke about the coldness of humanity and the intimidating power of loneliness."
- Elvis Mitchell, New York Times

"The title of Greg Pak's "Robot Stories" is a little misleading. This superb four-part anthology features one robot, two androids, a slew of microbot toys, a full-figured lady hologram and several very interesting humans."
- Jack Matthews, New York Daily News

"For all the melodrama lurking at the edges of Robot Stories, Pak never resorts to preachiness or pathos. He's an uncannily assured visual storyteller, and his crew - particularly cinematographer Peter Olsen and editor Stephanie Sterner - matches his creative fervor. The result is a quietly impassioned, genuinely stirring indie rarity. As a character in 'The Robot Fixer' puts it, 'A little care goes a long way.'"
- Mark Holcomb, Village Voice

"'Machine Love' is about mankind s repulsive yet apparently timeless desire to hold other creatures as slaves. The sight of Archie standing upright and shirtless against a picture window at night filled with longing, yet powerless to do anything to halt the evil occurring next door - is one of the most disturbing images in modern sci-fi cinema."
- Matt Zoller Seitz, New York Press

"Movies often use fanciful technology as a mere source of plot twists, but Robot Stories's final chapter reminds us that such devices are a lot more effective when used to explore the human heart." 
- Andrew Johnston, Time Out New York

"Greg Pak s lovely, low-key science-fiction film has more in common with the short stories of Ray Bradbury than the pyrotechnics of George Lucas. Composed as a quartet of expertly acted chapters, the film s a smart evocation of love in the near-future, told through a widower s grief, a mother s anxiety, a family s tragedy, and a robot s confusion." 
- Bilge Ebiri and Logan Hill, New York Magazine

"In less than 90 minutes, "Robot Stories" says more about humanity's relationship to machines than the entire "Matrix" trilogy. At its best, this quartet of vignettes could also be favorably compared to "Minority Report" (without the budget) and "A.I." (without the bluster)."
- St. Louis Post-Dispatch

"If this well-done collection of four shorts was on paper instead of film, you'd find it in the pages of The New Yorker or Atlantic Monthly. Writer-director Greg Pak focuses on our contemporary computerized lives -- occasionally delving into the future -- in a weighty and relevant anthology."
- Fort Lauderdale Sun Sentinel

"If only Steven Spielberg had had the quirkiness and the funny bone of New York filmmaker Greg Pak when he set out to make A.I.: Artificial Intelligence a couple of years ago. We might all have been the richer for it."
- Michael Janusonis, Providence Journal

"Don t be fooled by the title - 'Robot Stories' isn't just for science-fiction fans. Each of the four segments that makes up the film tells a heartbreaking story that has as much to do with human interactions as it has to do with robots... Even the most tragic circumstances, however, are juxtaposed in the film with moments of pure, innocent joy."
- Audrey Magazine

"The stories span the life cycle from birth to death and take you on an emotional roller-coaster ride from hilarious highs to emotion-grabbing moments that will have you reaching for your hankie."
- The Movie Chicks

"One of the most moving pieces I've seen all year."
- John Petrakis, Chicago Tribune

"Extremely powerful. The writing has a real simplicity that I thought was all the more remarkable for how powerful the films really are. There might just be one simple line or image which carries tremendous emotional impact."
- Jean Oppenheimer, FilmWeek, 89.3 KPCC LA 
 
"'Pak loves the high concept, but keeps it simple; we're sucked in not by the conceit, the gimmick, but the emotion behind each tale, and it's a remarkable bit of work."
- Robert Wilonsky, Dallas Observer 
 
"'Robot Stories' is an exquisitely moving and original examination of the humanity of machines and the artificiality of humans, directed with grace and humor by Pak."
- Natalie Kim Burns, FunFactor 
 
"This is postmodern anthropology, a strange and bittersweet little movie that knows its sci-fi ancestry but dares to take it into new and personal directions. You'll want to follow along."
- Steve Schneider, Orlando Weekly 
 
"'The Robot Fixer' is truly transcendent. A mother, visiting her comatose son's apartment, uncovers a cache of his old toys, a huge but crumbling collection of robot action figures. She becomes obsessed with restoring them, convinced that in doing so, she can somehow repair her damaged son. A poignant tale of love and loss.
- Pam Grady, Contra Costa Times 
 
"One of the better screenplays of independent film in 2002."
- Chris Wehner, Screenwriters Utopia 
 
"A technological treasure - the kind of science fiction that sophisticated audiences crave and deserve."
- Jonathan Hickman, Entertainment Insiders 
 
"Strong thesping by a largely Asian American cast and clever sci-fi concepts... Helmer Greg Pak understands the short form well, mercifully avoiding blatant O. Henry twists while pulling off neat reversals of expertly set-up genre expectations." 
- Ronnie Scheib, Variety 
 
"'My Robot Baby,' which stars Tamlyn Tomita as the ambivalent mother to a ridiculously ovoid mechanical infant, gets the feeling of new motherhood exactly right. Its corollary, 'The Robot Fixer,' features a pitch-perfect, beautifully understated performance by Wai Ching Ho as a mother determined to complete her comatose son's collection of toy robots."
- Marritt Ingman, Austin Chronicle 
 
"Creepy and intriguing."
- San Francisco Examiner 
 
"Pak deftly balances thought-provoking sci-fi story-lines with palpable human drama."
- Brita Brundage, Fairfield County Weekly 
 
"Pak's digital feature is infused with clever and perceptive interpretations of character motivation and fresh perspectives on the role of technology in everyday life."
- Justin Lowe, AsianWeek 
 
"'Robot Stories' explores people's humanity by setting it agains a mechanical backdrop, without once resorting to cliche... In the mad rush of Hawaii International Film Festival movies this season, 'Robot Stories' is one film that should not be left behind.'"
- Robb Bonnell, Honolulu Weekly 
 
"Funny, clever and emotionally visceral... Artfully realized, 'Robot Stories' pulsates with humanity." 
- Hersch Doby, Hamptons Int'l Film Festival 
 
"This is a moving, thoughtful film with good performances by actors Sab Shimono and Tamlyn Tomita which deserves theatrical distribution and/or broadcast on cable or PBS." 
- Albert Lanier, Honolulu News 
 
"An exhilarating ride by a masterful filmmaker, four utterly engrossing tales which speak with the wisdom of parables to our technologically-obsessed age. Greg Pak infuses each moment of this beautiful film with an infectious wonder at the joys and complexities of existence." 
- David Henry Hwang ("M. Butterfly," "Flower Drum Song") 
 
"By turns quirky, thoughtful, and laugh-out-loud funny, Greg Pak's feature film debut is an off-kilter look at a world in which machines think and feel, and humans often don't. In short, 'Robot Stories' is Steven Spielberg's 'A.I.' done right." 
- Jeff Yang, founder, aMagazine: Inside Asian America 
 

